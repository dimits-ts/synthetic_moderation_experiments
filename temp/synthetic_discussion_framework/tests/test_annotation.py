import unittest.mock
import tempfile
import json
import os
from unittest.mock import MagicMock

from ..src.sdl.backend.cpp_model import LlamaModel
from ..src.sdl.serialization.annotation_io import LlmAnnotationData, LLMAnnotationGenerator
from ..src.sdl.generation.annotation import AnnotationConv


class TestLLMAnnotatorData(unittest.TestCase):
    # mostly generated by LLM
    # TODO: add more edge cases

    def test_constructor(self):
        # Test valid creation
        data = LlmAnnotationData(
            attributes=["Thoughtful", "Detail-oriented"],
            instructions="Annotate with a focus on clarity and completeness.",
            history_ctx_len=5,
            include_moderator_comments=True,
        )
        self.assertEqual(data.history_ctx_len, 5)

    def test_from_json_file(self):
        tmp = tempfile.NamedTemporaryFile()
        # Create a sample JSON file
        sample_data = {
            "attributes": ["Analytical", "Concise"],
            "instructions": "Annotate the text focusing on key insights.",
            "history_ctx_len": 4,
            "include_moderator_comments": False,
        }
        with open(tmp.name, "w") as f:
            json.dump(sample_data, f)

        # Load the data and verify the contents
        data = LlmAnnotationData.from_json_file(tmp.name)
        self.assertEqual(data.attributes, sample_data["attributes"])
        self.assertEqual(data.instructions, sample_data["instructions"])
        self.assertEqual(data.history_ctx_len, sample_data["history_ctx_len"])

    def test_to_json_file(self):
        tmp = tempfile.NamedTemporaryFile()
        # Test serialization to JSON
        data = LlmAnnotationData(
            attributes=["Observant", "Critical"],
            instructions="Provide feedback on argument strength and clarity.",
            history_ctx_len=3,
            include_moderator_comments=True,
        )
        data.to_json_file(tmp.name)

        # Verify the content
        with open(tmp.name, "r") as f:
            loaded_data = json.load(f)
        self.assertEqual(loaded_data["attributes"], data.attributes)
        self.assertEqual(loaded_data["instructions"], data.instructions)
        self.assertEqual(loaded_data["history_ctx_len"], data.history_ctx_len)

    def test_missing_required_fields_in_json(self):
        tmp = tempfile.NamedTemporaryFile()
        # Test with missing 'attributes' field in JSON
        incomplete_data = {
            "instructions": "This is a sample instruction."
            # Missing 'attributes' and 'history_ctx_len'
        }
        with open(tmp.name, "w") as f:
            json.dump(incomplete_data, f)

        with self.assertRaises(TypeError):
            LlmAnnotationData.from_json_file(tmp.name)

    def test_invalid_json_structure(self):
        tmp = tempfile.NamedTemporaryFile()
        # Create an invalid JSON file to test error handling
        with open(tmp.name, "w") as f:
            f.write("{invalid_json: true}")

        with self.assertRaises(json.JSONDecodeError):
            LlmAnnotationData.from_json_file(tmp.name)


class TestLLMAnnotationGenerator(unittest.TestCase):

    def setUp(self):
        # Mock LlamaModel to avoid dependencies on actual model logic
        self.mock_llm = MagicMock(spec=LlamaModel)

        # Create sample LLMAnnotatorData for tests
        self.data = LlmAnnotationData(
            attributes=["Insightful", "Direct"],
            instructions="Annotate with focus on logical structure.",
            history_ctx_len=4,
            include_moderator_comments=True,
        )
        self.conv_logs_path = "output/conv_logs"

    def test_initialization(self):
        # Initialize generator and verify attributes
        generator = LLMAnnotationGenerator(
            self.data, self.mock_llm, self.conv_logs_path
        )
        self.assertEqual(generator.data, self.data)
        self.assertEqual(generator.llm, self.mock_llm)
        self.assertEqual(generator.conv_logs_path, self.conv_logs_path)

    def test_produce_conversation_with_invalid_data(self):
        # Test if initialization fails with an invalid model
        with self.assertRaises(Exception):
            LLMAnnotationGenerator(None, None, self.conv_logs_path)  # type: ignore


class TestAnnotationConv(unittest.TestCase):
    # test generated by LLM, manually checked and modified

    def setUp(self):
        # Sample conversation log data
        self.sample_conv_logs = {
            "id": "123",
            "logs": [
                ("user1", "Hello, how are you?"),
                ("moderator", "Please keep the discussion respectful."),
                ("user2", "I'm fine, thank you."),
                ("moderator", "Let's move on to the next topic."),
                ("user1", "I agree with that point."),
            ],
        }

        # Mock annotator
        self.mock_annotator = MagicMock()
        self.mock_annotator.speak = MagicMock(
            side_effect=lambda ctx: f"Annotated: {ctx[-1]}"
        )
        self.mock_annotator.describe = MagicMock(return_value="MockAnnotator")

    def test_annotation_with_moderator_comments(self):
        tmp = tempfile.NamedTemporaryFile()
        with open(tmp.name, "w", encoding="utf8") as f:
            json.dump(self.sample_conv_logs, f)

        annotation_conv = AnnotationConv(
            annotator=self.mock_annotator,
            conv_logs_path=tmp.name,
            include_moderator_comments=True,
            history_ctx_len=4,
        )

        annotation_conv.begin_annotation(verbose=False)

        # Check that moderator messages are included in annotation logs
        annotated_messages = [entry[0] for entry in annotation_conv.annotation_logs]
        print(annotation_conv.annotation_logs)
        print(annotated_messages)
        self.assertEqual(len(annotated_messages), 5)
        self.assertIn("Please keep the discussion respectful.", annotated_messages)
        self.assertIn("Let's move on to the next topic.", annotated_messages)

    def test_annotation_without_moderator_comments(self):
        tmp = tempfile.NamedTemporaryFile()
        with open(tmp.name, "w", encoding="utf8") as f:
            json.dump(self.sample_conv_logs, f)

        annotation_conv = AnnotationConv(
            annotator=self.mock_annotator,
            conv_logs_path=tmp.name,
            include_moderator_comments=False,
            history_ctx_len=4,
        )

        annotation_conv.begin_annotation(verbose=False)

        # Check that moderator messages are excluded from annotation logs
        annotated_messages = [entry[0] for entry in annotation_conv.annotation_logs]
        self.assertEqual(len(annotated_messages), 3)
        self.assertNotIn("Please keep the discussion respectful.", annotated_messages)
        self.assertNotIn("Let's move on to the next topic.", annotated_messages)

    def test_annotation_context_with_history_limit(self):
        tmp = tempfile.NamedTemporaryFile()
        with open(tmp.name, "w", encoding="utf8") as f:
            json.dump(self.sample_conv_logs, f)

        annotation_conv = AnnotationConv(
            annotator=self.mock_annotator,
            conv_logs_path=tmp.name,
            include_moderator_comments=True,
            history_ctx_len=2,
        )

        annotation_conv.begin_annotation(verbose=False)

        # Ensure context respects history context length
        for _, annotation in annotation_conv.annotation_logs:
            ctx_length = len(
                annotation.split(": ")[1].split(", ")
            )  # count items in context
            self.assertLessEqual(ctx_length, 2)

    def test_to_dict_output_format(self):
        tmp = tempfile.NamedTemporaryFile()
        with open(tmp.name, "w", encoding="utf8") as f:
            json.dump(self.sample_conv_logs, f)

        annotation_conv = AnnotationConv(
            annotator=self.mock_annotator,
            conv_logs_path=tmp.name,
            include_moderator_comments=True,
            history_ctx_len=4,
        )
        annotation_conv.begin_annotation(verbose=False)
        conv_dict = annotation_conv.to_dict()

        # Check that the dictionary format matches expected keys and values
        self.assertIn("conv_id", conv_dict)
        self.assertEqual(conv_dict["conv_id"], "123")
        self.assertIn("logs", conv_dict)
        self.assertEqual(len(conv_dict["logs"]), len(annotation_conv.annotation_logs))

    def test_to_json_file(self):
        tmp = tempfile.NamedTemporaryFile()
        with open(tmp.name, "w", encoding="utf8") as f:
            json.dump(self.sample_conv_logs, f)

        # Initialize AnnotationConv and generate annotations
        annotation_conv = AnnotationConv(
            annotator=self.mock_annotator,
            conv_logs_path=tmp.name,
            include_moderator_comments=True,
            history_ctx_len=4,
        )
        annotation_conv.begin_annotation(verbose=False)

        tmp2 = tempfile.NamedTemporaryFile()
        # Write to JSON file
        annotation_conv.to_json_file(tmp2.name)

        # Verify the output file exists and contains expected data
        self.assertTrue(os.path.exists(tmp2.name))
        with open(tmp2.name, "r", encoding="utf8") as f:
            data = json.load(f)
            self.assertEqual(data["conv_id"], "123")
            self.assertEqual(len(data["logs"]), len(annotation_conv.annotation_logs))


if __name__ == "__main__":
    unittest.main()
