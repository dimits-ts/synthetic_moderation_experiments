% !TEX root = ../main.tex
%

\begin{abstract}
   Despite the ever-growing importance of online moderation, there has been no large-scale study evaluating the effectiveness of alternative moderation strategies. This is largely due to the lack of appropriate datasets, and the difficulty of getting human discussants, moderators, and evaluators involved in multiple experiments. In this paper, we propose a methodology for leveraging synthetic experiments performed exclusively by \acp{LLM} to initially bypass the need for human participation in experiments involving online moderation. We evaluate six \ac{LLM} moderation configurations; two currently used real-life moderation strategies (guidelines issued for human moderators for online moderation and real-life facilitation), two baseline strategies (guidelines elicited for \ac{LLM} alignment work, and \ac{LLM} moderation with minimal prompting) a baseline with no moderator at all, as well as our own proposed strategy inspired by a \ac{RL} formulation of the problem. We find that our own moderation strategy significantly outperforms established moderation guidelines, as well as out-of-the-box \ac{LLM} moderation. We also find that smaller \acp{LLM}, with less intensive instruction-tuning, can create more varied discussions than larger models. In order to run these experiments, we create and release an efficient, purpose-built, open-source Python framework, dubbed  “SynDisco”\projectlink, to easily simulate hundreds of discussions using \ac{LLM} user-agents and moderators. Additionally, we release the “\ac{VMD}”, a large dataset of \ac{LLM}-generated and \ac{LLM}-annotated discussions, generated by three families of open-source \acp{LLM} \datasetlink, accompanied by an exploratory analysis of the dataset\analysislink.
\end{abstract}