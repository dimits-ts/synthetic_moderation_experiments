% !TEX root = ../main.tex
%
\section{Background and Related Work}

\subsection{Synthetic Discussions}
\label{ssec:related:discussions}

While studies exist for simulating user interactions in social media \cite{park_simulacra, mou_2024, tornberg_2023, y_social, balog_2024}, and for using LLM facilitators \cite{kim_et_al_chatbot, cho-etal-2024-language}, none so far have combined the two approaches. 


\citet{balog_2024} propose a methodology for generating synthetic discussions by extracting topics and comments from real online ones and prompting an LLM to continue them. However, they do not use LLM-based user agents to simulate conversational dynamics, nor do they include facilitators in their setup. Their method also struggles with malformed metadata (e.g., missing usernames) generated by the LLM, for which they only suggest error detection as a solution. Additionally, their approach depends on the availability of appropriate human discussion datasets.

\citet{ulmer2024} create synthetic discussions between two roles: an agent controlling a fictional environment and a client interacting with it. These discussions are filtered and used to finetune the agent LLM for a specific task. Our methodology generalizes their framework: an agent (facilitator) interacts with multiple clients (non-facilitator users).

Finally, \citet{abdelnabi_negotiations} generate synthetic negotiations involving multiple agents with different agendas and responsibilities. Our work can be seen as a domain shift of their approach --- from negotiation to discussion facilitation --- where various user types (e.g., normal users, trolls, community veterans) engage in discussion moderated by a facilitator with veto power.


\subsection{LLM Facilitation}

Unlike classification models traditionally used in online platforms, LLMs can actively facilitate discussions \cite{korre2025evaluation}. They can warn users for rule violations \cite{Kumar_AbuHashem_Durumeric_2024}, monitor engagement \cite{schroeder-etal-2024-fora}, aggregate diverse opinions \cite{small-polis-llm}, and provide translations and writing tips, which is especially useful for marginalized groups \cite{Tsai2024Generative}. These capabilities suggest that LLMs may be able to assist or even replace human facilitators in many tasks \cite{small-polis-llm, seering_self_moderation}.

Moderator chatbots have shown promise; \citet{kim_et_al_chatbot} demonstrated that simple rule-based models can enhance discussions, although their approach was largely confined to organizing the discussion based on the ``think-pair-share'' framework \cite{ahmad_2010_supporting, Navajas2018}, and balancing user activity. \citet{cho-etal-2024-language} use LLM facilitators in human discussions, with facilitation strategies based on Cognitive Behavioral Therapy and the work of \citet{rosenberg2015nonviolent}. They show that LLM facilitators can provide “specific and fair feedback” to users, although they struggle to make users more respectful and cooperative.  In contrast to both works, our work uses exclusively LLM participants and LLM facilitators, and tests the latter in an explicitly toxic and challenging environment.


\subsection{Discussion Quality}
\label{ssec:related:quality}

In this paper we need to evaluate two different quality dimensions. One is \emph{discussion quality as seen by humans}, which is difficult to measure, both because of the breadth of the possible goals of a discussion, and because of the lack of established computational metrics in Social Science literature \cite{korre2025evaluation}.

The second quality dimension is measuring “high-quality” or “useful” data. This is essential in LLM-based discussion frameworks, as such discussions tend to deteriorate quickly without human involvement, often becoming repetitive and low-quality \citep{ulmer2024}. Despite this importance, methods for quantifying the quality of synthetic data remain limited.

 \citet{balog_2024} use a mix of graph-based, methodology-specific, and lexical similarity metrics, many of which depend on human discussion datasets. Their most generalizable measure is a loosely defined “coherence” score, which is LLM-annotated without theoretical grounding. \citet{kim_et_al_chatbot} assess quality through post-discussion surveys and by measuring lexical diversity to approximate the variety of opinions expressed. \citet{ulmer2024}  introduce a metric called \emph{``Diversity''}, which penalizes repeated text sequences between comments using ROUGE-L \citep{lin-2004-rouge} scores.
