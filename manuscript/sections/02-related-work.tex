% !TEX root = ../main.tex
%
\section{Related Work}

\subsection{LLMs as human subjects}
\label{ssec:related:human-llm}

Recent advancements in \acp{LLM} have sparked considerable debate among researchers, particularly within the field of Social Science. As argued by \citet{grossman_2023}, synthetic agents have the potential to not only generate synthetic data for social experiments, but also eventually replace human participants, a perspective shared by other researchers \cite{tornberg_2023, argyle2023}. \acp{LLM} have demonstrated emergent behaviors such as information diffusion \cite{Park2023GenerativeAI}, scale-free networks \cite{demarzo_2023}, social behavior (to an extent) \cite{leng_2024}, social strategies \cite{abdelnabi_negotiations}, and certain psychological patterns \cite{abramski_2023}, while also being capable of predicting human survey responses given extensive personal data \cite{park2024generativeagentsimulations1000}. If realized, this development could revolutionize Social Sciences by alleviating significant costs and challenges associated with human participation in research \cite{rossi_2024, shapiro2019polling}.

However, it is important to acknowledge the limitations of \acp{LLM}. There are issues such as dataset contamination, undetectable behavioral hallucinations \cite{rossi_2024}, sociodemographic, statistical, and political biases \cite{anthis_2025,hewitt2024predicting,rossi_2024}, which can be amplified during discussions \cite{Taubenfeld2024SystematicBI}, unreliable survey responses \cite{jansen_2023,bisbee_2023,neumann_2025} and annotations \cite{Gligoric2024CanUL}. Furthermore, model outputs are non-deterministic \cite{atil_2025}, particularly in closed-source models\cite{bisbee_2023}. This lack of consistency raises significant concerns, especially given the broader replication crisis within Social Science research. Furthermore, agents tend to be “too agreeable”, likely due to alignment procedures \cite{Park2023GenerativeAI, anthis_2025, rossi_2024}.

Despite the existence of objectively measurable emergent behaviors like information diffusion \cite{Park2023GenerativeAI}, researchers often anthropomorphize \ac{LLM} behaviors \cite{rossi_2024}. It is crucial to acknowledge that \acp{LLM} operate on fundamentally different principles from humans, and their outputs should not be attributed with human-like traits or intentions, since anthropomorphization may introduce researcher bias and obscure the true nature of \ac{LLM} behaviors \cite{anthis_2025,zhou-etal-2024-real}. Moreover, we add that crafting instruction prompts for synthetic experiments can encode researcher bias and expectations, despite often being necessary for getting around model alignment.


\subsection{Synthetic discussions}
\label{ssec:related:discussions}


Researchers have explored \ac{LLM} self-talk (a term inspired by \ac{RL}'s self-play\citep{cheng-self-play}) for jailbreaking mitigation \cite{liu2024largelanguagemodelsagents, cheng-self-play}, alignment \cite{Bai2022ConstitutionalAH, collective_constitution}, and self-refinement \cite{Madaan2023SelfRefineIR, lambert2024}. \citet{ulmer2024} employ distinct LLM characters in fictional scenarios to facilitate high-quality discussions for further finetuning. However, this approach remains underexplored in complex social situations \cite{zhou-etal-2024-real}. Meanwhile, \citet{balog_2024} introduce a thread-based method with summarized past comments but face challenges when \acp{LLM} generate malformed data, for which they offer no solution.

Synthetic discussions are often studied in the context of “digital twins” of social media sites, which aim to replicate their environment and study their operation using synthetic users. These range from synthetic clones of Reddit \cite{park_simulacra}, Twitter \cite{mou_2024} and social media in general \cite{tornberg_2023, y_social}. Digital twins are not limited to social media; \citet{Park2023GenerativeAI} create an interactive in-game world with \ac{LLM}-controlled \acp{NPC}, while \citet{zhou_2024_sotopia} create virtual scenarios to evaluate social abilities of \ac{LLM} actors.  


\subsection{Synthetic Data Quality}

Synthetic discussions often degrade rapidly without human interaction, exhibiting repetitive, low-quality content \citep{ulmer2024}. To address this, we require robust “synthetic quality” metrics that capture internal characteristics, rather than realism. However, research on quantifying data quality is currently limited.

\citet{balog_2024} introduce metrics utilizing comparisons with human data, but this approach depends on datasets with the same topics and lacks scientific grounding due to unestablished links between human-like text and behavior (see Section \ref{ssec:related:human-llm}). Their most generalizable metric—a vague coherence score—is \ac{LLM}-annotated without theoretical support. Alternatively, \citet{ulmer2024} propose metrics like N-gram-based \textit{“Diversity”} (Section \ref{ssec:methodology:diversity}), which is topic-agnostic, methodology-independent, and correlates with effective fine-tuning data.


\subsection{LLM moderation}

\citet{korre2025evaluation} identified moderation functions that \acp{LLM} can replace. \acp{LLM} have proven capable of detecting toxicity \cite{kang-qian-2024-implanting, Wang2022ToxicityDW}, hate-speech \cite{Nirmal2024TowardsIH, shi-2024-hatespeech}, and misinformation \cite{Liu2024DetectIJ, Xu2024ACS}. Unlike traditional \ac{ML} models, \acp{LLM} can actively moderate through conversational abilities. They can warn users for rule violations \cite{Kumar_AbuHashem_Durumeric_2024}, monitor engagement \cite{schroeder-etal-2024-fora}, and aggregate diverse opinions \cite{small-polis-llm}. These capabilities suggest that \acp{LLM} can replace human moderators in many tasks \cite{small-polis-llm, seering_self_moderation}. \citet{small-polis-llm} suggest that \acp{LLM} can start discussions by generating initial opinions, although traditional Information Retrieval methods outperform \acp{LLM} in selecting appropriate starting points for discussions \cite{karadzhov2023delidata}, and some guidelines explicitly prohibit moderators from performing these tasks \cite{dimitra-book}. \acp{LLM} can also aid users, particularly minority or ethnic groups, by providing translations and improving grammar \cite{Tsai2024Generative}. 

Moderator chatbots have shown promise; \citet{kim_et_al_chatbot} demonstrated that simple rule-based models can enhance discussions. \citet{cho-etal-2024-language} use \ac{LLM} moderators in human discussions, with moderation strategies based on Cognitive Behavioral Therapy and the work of \citet{rosenberg2015nonviolent}. This is in contrast to our work, which uses exclusively \ac{LLM} participants and focuses specifically on conversational moderation literature and current practices. They show that \ac{LLM} moderators can provide “specific and fair feedback” to users, although they struggle to make users more respectful and cooperative. Finally, \citet{dtsirmpas_thesis} use \ac{LLM} moderators in synthetic discussions and investigate the use of \ac{LLM} annotator-agents for discussion evaluation.