% !TEX root = ../main.tex
%

\section{Related Work}


\subsection{LLMs as human subjects}
\label{ssec:related:human-llm}

Recent advancements in \acp{LLM} have sparked considerable debate among researchers, particularly within the field of Social Science. As argued by \citet{grossman_2023}, synthetic agents have the potential to not only generate synthetic data for social experiments, but also eventually replace human participants entirely, a perspective largely shared by other researchers such as \citet{tornberg_2024, argyle2023}. \acp{LLM} have demonstrated emergent behaviors such as information diffusion \cite{Park2023GenerativeAI}, scale-free networks \cite{demarzo_2023}, social behavior (to an extent) \cite{leng_2024}, social strategies \cite{abdelnabi_negotiations}, and certain psychological patterns \cite{abramski_2023}. Additionally, \citet{park2024generativeagentsimulations1000} reveal that substituting \acp{SDB} with interview-based information enables \acp{LLM} to reliably predict survey responses from interviewed humans, although their approach relies on extensive human interviews and computational resources, and although other studies have demonstrated that \ac{LLM} survey responses \cite{jansen_2023, bisbee_2023, neumann_2025}, as well as \ac{LLM} annotation \cite{Gligoric2024CanUL} are generally unreliable and surface-level. 

If realized, this development could revolutionize Social Sciences by alleviating significant costs and challenges associated with human participation in research \cite{rossi_2024, shapiro2019polling}. However, \citet{rossi_2024} raise valid concerns regarding the limitations of assuming \ac{LLM} responses reflect human behavior accurately. These include issues such as dataset contamination (where models may inadvertently draw from previous studies' data), potential hallucinations at the behavioral level that are inherently undetectable, the unsupported belief equating “believable” outputs with behavioral authenticity, and model biases \cite{anthis_2025, hewitt2024predicting}, which can be reinforced during the course of a discussion \cite{Taubenfeld2024SystematicBI}. Furthermore, \ac{LLM} outputs are often non-deterministic \cite{rossi_2024}, particularly in closed-source models \cite{bisbee_2023}, which is an important concern in the context of the broader replication crisis in the field of Social Science.


\subsection{Synthetic discussions}
\label{ssec:related:discussions}

\ac{LLM} agents talking with each other has been an active area of research outside the context of discussion simulation. The term “\ac{LLM} self-talk” is sometimes used in this case, a term adapted from “self-play” in \ac{RL} \citep{cheng-self-play, ulmer2024}. Researchers have experimented with using self-talk for reinforcing \acp{LLM} against jailbreaking \cite{liu2024largelanguagemodelsagents, cheng-self-play, zheng2024optimalllmalignmentsusing}, for \ac{LLM} alignment \cite{Bai2022ConstitutionalAH, collective_constitution}, and self-refinement in general \cite{Madaan2023SelfRefineIR, lambert2024} by pitting the model against itself. Self-talk is not necessarily tied to \ac{RL} methods. \citet{ulmer2024} create fictional scenarios where the participating \ac{LLM} agents have distinct characters. They demonstrate that \ac{LLM} self-talk can occasionally produce high-quality discussions, which can then be used to further finetune the model. \citet{balog_2024} showcase a new methodology where posts are generated using a thread structure, and where past comments are summarized. This methodology is vulnerable to the \ac{LLM} breaking the structure of the discussion by generating malformed data, which is not addressed.

Synthetic discussions are often studied in the context of “digital twins” of social media sites, which aim to replicate their environment and study their operation using synthetic users. These range from synthetic clones of Reddit \cite{park_simulacra}, Twitter \cite{mou_2024}, and social media in general \cite{tornberg_2023, y_social}. Digital twins are not limited to social media; \citet{Park2023GenerativeAI} create an interactive in-game world where \ac{LLM}-controlled \acp{NPC} engage with players, the environment, and each other. While socially capable, these agents tend to be “too agreeable”, likely due to alignment artifacts, which is a general issue stemming from \acp{LLM} being explicitly designed as assistants \cite{anthis_2025}.

Synthetic discussions devoid of human interaction often degrade rapidly, exhibiting repetitive and low-quality content \citep{ulmer2024}. To address this, we need robust “synthetic quality” metrics that capture the internal characteristics of generated discussions, rather than aiming for realism. Realism is an inadequate goal, as \acp{LLM} are not human-like in their reasoning processes \citep{rossi_2024}, often produce outputs designed to be believable rather than replicating human behaviors \citep{rossi_2024, anthis_2025}, and can mislead humans even when based on outdated models \citep{rossi_2024}. \citet{balog_2024} introduce metrics that compare generated and human-authored text, but this approach is limited by its dependence on human datasets with identical topics and questionable scientific grounding, given the lack of established links between human-like text and behavior \citep{rossi_2024}. Their most generalizable metric—a vague coherence score—is annotated via \acp{LLM} and lacks theoretical support. In contrast, \citet{ulmer2024} propose topic-agnostic synthetic quality metrics, such as N-gram-based Diversity, which are robust to perturbations, independent of generation methodology, and correlate with effective fine-tuning data.


\subsection{LLM moderation}

\citet{korre2025evaluation} identified moderation functions that \acp{LLM} can replace. These include detecting toxicity \cite{kang-qian-2024-implanting, Wang2022ToxicityDW}, hate-speech \cite{Nirmal2024TowardsIH, shi-2024-hatespeech}, and misinformation \cite{Liu2024DetectIJ, Xu2024ACS}. Unlike traditional \ac{ML} models, \acp{LLM} can actively moderate through conversational abilities. They can warn users for rule violations \cite{Kumar_AbuHashem_Durumeric_2024}, monitor engagement \cite{schroeder-etal-2024-fora}, and aggregate diverse opinions \cite{small-polis-llm}. These capabilities suggest that \acp{LLM} can replace human moderators in many tasks \cite{small-polis-llm, seering_self_moderation}.  \citet{small-polis-llm} highlight the potential of \acp{LLM} to use discussion facilitation techniques and starting discussions by generating initial opinions. However, they note that traditional \ac{IR} methods outperform \acp{LLM} in selecting appropriate starting points for discussions \cite{karadzhov2023delidata}, while some facilitation guidelines \cite{dimitra-book} explicitly prohibit moderators from performing these tasks. \acp{LLM} can also aid users, particularly minority or ethnic groups, by providing translations and improving grammar \cite{Tsai2024Generative}. 

Moderator chatbots have shown promise; \citet{kim_et_al_chatbot} demonstrated that simple rule-based models can enhance discussions. \citet{cho-etal-2024-language} use \ac{LLM} moderators in human discussions, and their strategies are obtained from general conversational Social Science theory. This is in contrast to our work, which uses \ac{LLM} user-agents and focuses specifically on literature and current practices on moderation. They show that \ac{LLM} moderators can provide “specific and fair feedback” to users, although the \acp{LLM} struggle to make users more respectful and cooperative. Finally, \citet{dtsirmpas_thesis} use \ac{LLM} moderators in synthetic discussions and investigate the use of \ac{LLM} annotator-agents for discussion evaluation.