% !TEX root = ../main.tex
%

\section{Related Work}


\subsection{Replicating human behavior with LLMs}

Studies show that while \acp{LLM} can not fully replicate human behavior, they can approximate it to some extent. \textcite{hewitt2024predicting} suggest using \acp{LLM} as substitutes in social science experiments, even for minority groups, particularly for pilot studies. However, they highlight limitations such as low variance, model bias, and the challenge of encoding all relevant participant details in prompts. \textcite{park2024generativeagentsimulations1000} demonstrate that replacing \acp{SDB} with interview-based information can significantly improve behavioral replication. However, their method requires extensive human interviews and substantial computational resources (since it requires inserting an enormous amount of information at the start of every prompt), making it impractical for the pilot experiments we present here. Moreover, \textcite{abdelnabi2024cooperationcompetitionmaliciousnessllmstakeholders} show that \acp{LLM} can execute (to some extent) social strategies in negotiations amongst each other in order to achieve long-term goals. For example, an environmentalist \ac{LLM} agent may be stoking disagreement between other agents, in order to eventually sabotage the negotiations for the creation of a new factory.

Moreover, \ac{LLM} agents talking with each other have been an active area of research outside the context of discussion simulation. The term “\ac{LLM} self-talk” is used in this case, a term adapted from “self-play” in \ac{RL} \parencite{cheng2024selfplayingadversariallanguagegame}). For instance, researchers have experimented with using self-talk for reinforcing \acp{LLM} against jailbreaking \cite{liu2024largelanguagemodelsagents, cheng2024selfplayingadversariallanguagegame, zheng2024optimalllmalignmentsusing}, for \ac{LLM} alignment \cite{Bai2022ConstitutionalAH, collective_constitution}, and self-refinement in general \cite{Madaan2023SelfRefineIR, lambert2024selfdirectedsyntheticdialoguesrevisions} by pitting the model against itself. \textcite{ulmer2024bootstrappingllmbasedtaskorienteddialogue} give their \ac{LLM} agents distinct roles in a conversational setting. Their research shows that \ac{LLM} self-talk can occasionally produce high-quality, convincing discussions, and that these discussions can be used to further finetune the model.  

\acp{LLM} can also simulate human behavior within communities. \textcite{park2022socialsimulacracreatingpopulated} use a single \ac{LLM} with \ac{SDB} prompting to generate entire Reddit communities, producing synthetic discussions indistinguishable from real ones. \textcite{Park2023GenerativeAI} create an interactive in-game world where \ac{LLM}-controlled \acp{NPC} engage with players, the environment, and each other, displaying complex social behavior like information sharing and event planning. However, they note that these agents tend to be “too agreeable”, likely due to alignment artifacts.

Finally, \textcite{Gligoric2024CanUL} show that while \acp{LLM} can be used for annotating data by simulating humans of different backgrounds, statistical estimates based on wholly-synthetic annotations may not be reliable. Our previous work \cite{dtsirmpas_thesis} also demonstrated the potential for \ac{LLM} synthetic simulations and annotations in the context of moderation experiments for online discussions.


\subsection{Human Moderation}

There currently exists limited research on moderation strategies \cite{korre2025evaluation, make_reddit_great}. \textcite{cheung-et-al-2011} and \textcite{park_et_al_2012_facilitation} both propose taxonomies for moderation tactics, the second of which aligns with other research on the topic \cite{seering_self_moderation}. While these taxonomies are useful for analyzing moderator actions after-the-fact, they are not by themselves applicable to the creation of moderation strategies. This is due to them being descriptors of moderation actions, instead of informing what actions should be taken in various situations.

Typically, human moderators are not provided with any instructions other than the rules they must uphold \cite{seering_self_moderation, schaffner_community_guidelines}. We find two exceptions, the Cornell eRulemaking moderator manual \cite{Cornell_eRulemaking2017}, and the MIT Constructive Communications Facilitation guidelines \cite{dimitra-guide}. While the latter is unreleased at the time of writing, most of its information can be accessed via the accompanying book \cite{dimitra-book}.

Thus, most of our understanding of human moderation is empirical, gained through observation. We can observe, for instance, that moderators typically use an “escalation ladder” when threatening or applying disciplinary action \cite{seering_self_moderation}. Knowing when to intervene is an important part of successful moderation, but current related work 
% remains simplistic in nature (such 
is still in early stages (e.g., trying to predict escalation points in discussions \cite{korre2025evaluation}). Likewise, how moderators intervene is an equally important question, which does not find a clear answer in current literature \cite{korre2025evaluation}. Additionally, the lack of datasets addressing alternative moderation strategies in particular \cite{korre2025evaluation} means that simply observing human moderators in-vitro, or through existing datasets, can only give us limited insights.


\subsection{LLM Moderation}

Our previous work \cite{korre2025evaluation} compiled a list of moderation functions that \acp{LLM} can replace. For instance, much of the current \ac{ML}-based moderation systems used for detection of toxicity \cite{kang-qian-2024-implanting, Wang2022ToxicityDW}, hate-speech \cite{Nirmal2024TowardsIH, shi-2024-hatespeech} and misinformation \cite{Liu2024DetectIJ, Xu2024ACS} can be readily substituted by \acp{LLM}. Unlike traditional \ac{ML} models, \ac{LLM} agents can also actively moderate due to their conversational abilities. \acp{LLM} can warn users for violating the rules of a platform \cite{Kumar_AbuHashem_Durumeric_2024}, monitor engagement \cite{schroeder-etal-2024-fora}, and aggregate diverse opinions \cite{small-polis-llm}. Thus, there is optimism that \ac{LLM} moderators can replace human moderators in most of their duties \cite{small-polis-llm, seering_self_moderation}. \textcite{small-polis-llm} indicate that \acp{LLM} have the opportunity to help moderators in various tasks; such as iterative summarization, where a moderator attempts to summarize a group's arguments until the group agrees with the \ac{LLM}'s interpretation. Another task suited for \acp{LLM} would be generating opinions to kick-start the discussion. However, traditional Information Retrieval approaches still outperform \acp{LLM} in selecting appropriate starting points for the discussion \cite{karadzhov2023delidata}, while some facilitation manuals explicitly forbid moderators from doing so \cite{dimitra-guide}). Finally, \acp{LLM} can help users, and especially minority or other ethnic groups, by providing translations and helping in grammar and syntax \cite{Tsai2024Generative}.

Moderator chatbots have already been the subject of experimentation. \textcite{kim_et_al_chatbot} show that a simple rule-based model can improve discussions. The work of \textcite{cho-etal-2024-language} 
% demonstrate the 
is the closest to our own; they compare \ac{LLM} moderation strategies using discussions performed by human users and annotators, and their strategies are obtained from general conversational Social Science theory. This is in contrast to our work, which uses \ac{LLM} agents and focuses specifically on Social Science theory for ideas on  \emph{moderation}, often also called `facilitation'. 
They show that \ac{LLM} moderators can provide “specific and fair feedback” to users, although the \acp{LLM} struggle to make users more respectful and cooperative.

