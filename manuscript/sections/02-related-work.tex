% !TEX root = ../main.tex
%
\section{Background and Related Work}

\subsection{LLMs as Human Subjects}
\label{ssec:related:human-llm}

When conducting social experiments with \acp{LLM} instead of human subjects, it is imperative to know how representative results can be. \citet{grossman_2023} argue that synthetic agents have the potential to eventually replace human participants, a perspective shared by other researchers \cite{tornberg_2023, argyle2023}. Indeed, \acp{LLM} have demonstrated emergent complex social behaviors\cite{Park2023GenerativeAI, demarzo_2023, leng_2024, abdelnabi_negotiations, abramski_2023}. They are also capable of predicting human survey responses in aggregate \cite{hewitt2024predicting} and in the level of individual people, given extensive personal data \cite{park2024generativeagentsimulations1000}. %If realized, this development could revolutionize Social Sciences by alleviating significant costs and challenges associated with human participation in research \cite{rossi_2024, shapiro2019polling}.

However, significant limitations of \acp{LLM} remain in the context of Social Science experiments. Issues include dataset contamination; undetectable behavioral hallucinations \cite{rossi_2024}; sociodemographic, statistical and political biases \cite{anthis_2025,hewitt2024predicting,rossi_2024}, often amplified during discussions \cite{Taubenfeld2024SystematicBI};unreliable survey responses \cite{jansen_2023,bisbee_2023,neumann_2025}; inconsistent annotations \cite{Gligoric2024CanUL}; non-deterministic outputs \cite{atil_2025}, especially in closed-source models \cite{bisbee_2023}; and excessive agreeableness due to alignment procedures \cite{Park2023GenerativeAI, anthis_2025, rossi_2024}. Despite these shortcomings, researchers frequently anthropomorphize \ac{LLM} agents \cite{rossi_2024}, leading to biased interpretations and obscuring the true nature of their behavior \cite{anthis_2025,zhou-etal-2024-real}. Our study must thus be conservative towards the generalizability of our results to discussions with human participants.

We stress that we propose and investigate synthetic discussions as a means of preliminarly “debugging” and exploring artificial facilitators (e.g., with different facilitation strategies) in silico, before testing them in much more costly experiments with human participants. For example, synthetic experiments may help illustrate a clear risk or disadvantage of a particular facilitation strategy, that may be otherwise difficult to foresee. Experiments with real participants, however, are ultimately needed, and we leave them for future work.


\subsection{Synthetic Data Quality}
\label{ssec:related:quality}

Synthetic discussions often degrade rapidly without human interaction, exhibiting repetitive, low-quality content \citep{ulmer2024}. 
%To address this, we require robust “synthetic quality” metrics that detect such phenomena, rather than realism (Section~\ref{ssec:related:human-llm}). 
However, research on quantifying data quality is currently limited.

\citet{balog_2024} introduce metrics utilizing comparisons with human data, but this approach depends on datasets with the same topics, and lacks scientific grounding since believable \ac{LLM} outputs do not necessarily lead to behavior simulation \cite{rossi_2024}. Their most generalizable metric—a vague “coherence” score—is \ac{LLM}-annotated without theoretical support. Alternatively, \citet{ulmer2024} propose metrics like N-gram-based \textit{“Diversity”} (Section \ref{ssec:methodology:diversity}), which is topic-agnostic, methodology-independent, and correlates with effective fine-tuning data.


\subsection{Synthetic Discussions}
\label{ssec:related:discussions}

%Researchers have explored \ac{LLM} “self-talk” (a term inspired by \acf{RL}'s “self-play” \citep{cheng-self-play}) for jailbreaking mitigation \cite{liu2024largelanguagemodelsagents, cheng-self-play}, alignment \cite{Bai2022ConstitutionalAH, collective_constitution}, and self-refinement \cite{Madaan2023SelfRefineIR, lambert2024}. 

Synthetic discussion systems 
%are often employed in the context of “digital twins”, which aim to replicate an environment and study its operation using synthetic components (in our case, synthetic participants). These range from 
include synthetic clones of Reddit \cite{park_simulacra}, Twitter/X \cite{mou_2024} and social media in general \cite{tornberg_2023, y_social} as well as games \cite{Park2023GenerativeAI} and social experiments \cite{zhou_2024_sotopia}.

Similarly to our work, \citet{balog_2024} introduce a methodology to produce synthetic discussions, where they extract topics and comments from real-world online discussions, and prompt an \ac{LLM} to continue the discussion. Notably, unlike our approach, they do not use \ac{LLM} user-agents to model conversational dynamics nor do they model the presence of facilitators. However, their methodology faces challenges when \acp{LLM} generate malformed metadata, for which they offer no solution, and relies on the existence of suitable human discussion datasets.

\citet{ulmer2024} create synthetic discussions between two participants; an agent (who controls the environment) and a client (who interacts with the agent). They then filter the generated discussions and use them as training data to further finetune the agent \ac{LLM} for a specific task. Their approach however does not model the existence of multiple clients (users), nor is it applied on online discussion facilitation. Our proposed methodology can be modelled as a generalization of their paradigm; an agent (moderator) converses with multiple clients (non-moderator users).

Finally, \citet{abdelnabi_negotiations} create synthetic negotiations with multiple agents having various agendas and responsibilities. Our work can be modelled as a domain shift of their methodology from negotiations, to online discussion facilitation; participants with different motivations (i.e., normal users, trolls, long-standing community members), interact between themselves and with a stakeholder holding veto power (facilitator) and presiding over the discussion.


\subsection{LLM Facilitation}

%\acp{LLM} have proven capable of detecting toxicity \cite{kang-qian-2024-implanting, Wang2022ToxicityDW}, hate-speech \cite{Nirmal2024TowardsIH, shi-2024-hatespeech}, and misinformation \cite{Liu2024DetectIJ, Xu2024ACS}. 
Unlike traditional \ac{ML} models, \acp{LLM} can actively facilitate discussions \cite{korre2025evaluation}. They can warn users for rule violations \cite{Kumar_AbuHashem_Durumeric_2024}, monitor engagement \cite{schroeder-etal-2024-fora}, aggregate diverse opinions \cite{small-polis-llm}, provide translations and writing tips, which is especially useful for marginalized groups \cite{Tsai2024Generative}. These capabilities suggest that \acp{LLM} may be able to assist or even replace human facilitators in many tasks \cite{seering_self_moderation}.
%\citet{small-polis-llm} suggest that \acp{LLM} can start discussions by generating initial opinions, although traditional Information Retrieval methods outperform \acp{LLM}. \cite{karadzhov2023delidata}, and some guidelines explicitly prohibit facilitators from performing these tasks \cite{dimitra-book}. 

%TODO: kim approach
Moderator chatbots have shown promise; \citet{kim_et_al_chatbot} demonstrated that simple rule-based models can enhance discussions, although their approach was largely confined to monitoring and balancing user activity. \citet{cho-etal-2024-language} use \ac{LLM} facilitators in human discussions, with moderation strategies based on Cognitive Behavioral Therapy and the work of \citet{rosenberg2015nonviolent}. They show that \ac{LLM} facilitators can provide “specific and fair feedback” to users, although they struggle to make users more respectful and cooperative.  In contrast to both works, our work uses exclusively \ac{LLM} participants (and \ac{LLM} facilitators), tests them in an explicitly toxic and challenging environment. %,and focuses specifically on current facilitation literature.
%Finally, \citet{dtsirmpas_thesis} use \ac{LLM} facilitators in synthetic discussions and investigate the use of \ac{LLM} annotator-agents for discussion evaluation.