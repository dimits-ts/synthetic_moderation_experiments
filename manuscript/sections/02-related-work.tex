% !TEX root = ../main.tex
%
\section{Background and Related Work}

\subsection{Synthetic Discussions}
\label{ssec:related:discussions}

Synthetic discussion systems include synthetic clones of Reddit \cite{park_simulacra}, Twitter/X \cite{mou_2024}, generic social media \cite{tornberg_2023, y_social}, games \cite{Park2023GenerativeAI}, and social experiments \cite{zhou_2024_sotopia}.

\citet{balog_2024} introduce their own methodology to produce synthetic discussions; they extract topics and comments from real-world online discussions, and prompt an \ac{LLM} to continue them. Unlike our approach, they do not use \ac{LLM} user-agents to model conversational dynamics, nor do they model the presence of facilitators. Their methodology faces challenges when \acp{LLM} generate malformed metadata (such as missing usernames), for which they offer no solution besides detecting the errors. It also relies on the existence of suitable human discussion datasets.

\citet{ulmer2024} create synthetic discussions between two participants; an agent (who controls a fictional environment) and a client (who interacts with the agent). They then filter the generated discussions and use them as training data to further finetune the agent \ac{LLM} for a specific task. Their approach, however, does not model the existence of multiple clients (users), nor is it applied to online discussion facilitation. Our proposed methodology can be modelled as a generalization of their paradigm; an agent (facilitator) converses with multiple clients (non-facilitator users).

Finally, \citet{abdelnabi_negotiations} create synthetic negotiations with multiple agents having various agendas and responsibilities. Our work can be modelled as a domain shift of their methodology, from negotiations to discussion facilitation; participants with different motivations (i.e., normal users, trolls, long-standing community members) interact with one another, while a stakeholder holding veto power (facilitator) presides over the discussion.


\subsection{LLM Facilitation}

Unlike \ac{ML} classification models traditionally used in online platforms, \acp{LLM} can actively facilitate discussions \cite{korre2025evaluation}. They can warn users for rule violations \cite{Kumar_AbuHashem_Durumeric_2024}, monitor engagement \cite{schroeder-etal-2024-fora}, aggregate diverse opinions \cite{small-polis-llm}, and provide translations and writing tips, which is especially useful for marginalized groups \cite{Tsai2024Generative}. These capabilities suggest that \acp{LLM} may be able to assist or even replace human facilitators in many tasks \cite{small-polis-llm, seering_self_moderation}.

Moderator chatbots have shown promise; \citet{kim_et_al_chatbot} demonstrated that simple rule-based models can enhance discussions, although their approach was largely confined to organizing the discussion based on the ``think-pair-share'' framework \cite{ahmad_2010_supporting, Navajas2018}, and balancing user activity. \citet{cho-etal-2024-language} use \ac{LLM} facilitators in human discussions, with facilitation strategies based on Cognitive Behavioral Therapy and the work of \citet{rosenberg2015nonviolent}. They show that \ac{LLM} facilitators can provide “specific and fair feedback” to users, although they struggle to make users more respectful and cooperative.  In contrast to both works, our work uses exclusively \ac{LLM} participants and \ac{LLM} facilitators, and tests the latter in an explicitly toxic and challenging environment.


\subsection{Evaluating Discussion Quality}
\label{ssec:related:quality}

In this paper we need to evaluate two different quality dimensions. One is discussion quality as seen by humans; which we measure using toxicity as annotated by \acp{LLM}  (\S\ref{ssec:experimental:evaluation}). \acp{LLM} are reliable for toxicity detection \citep{kang-qian-2024-implanting, Wang2022ToxicityDW, anjum2024hate}, and toxicity can inhibit online and deliberative discussions \citep{dekock2022disagree, XiaToxicity}\footnote{We note that this is not always true \citep{Avalle2024PersistentIP}.}. 

The second quality dimension is unrelated to human discussions, but crucial to ones where all the actors are \acp{LLM}, since they often degrade rapidly without human interaction, exhibiting repetitive, low-quality content \citep{ulmer2024}. However, research on quantifying synthetic data quality is currently limited. \citet{balog_2024} utilize a collection of graph-based, methodology-dependent, and lexical similarity metrics, most of which utilize human discussion datasets. Their most generalizable metric—a vague ``coherence'' score—is \ac{LLM}-annotated without theoretical support. \citet{kim_et_al_chatbot} rely on post-discussion surveys and lexical diversity to estimate the number of diverse opinions. 

\citet{ulmer2024} propose \emph{``Diversity''}, a metric which penalizes repeated sequences between comments in a discussion. Low diversity points to pathological problems (e.g., \acp{LLM} repeating previous comments) \cite{ulmer2024}. On the other hand, extremely high diversity may point to a lack of interaction between participants; a discussion in which participants engage with each other will feature some lexical overlap (e.g., common terms, paraphrasing points of other participants). Diversity is defined as:

\small
\begin{equation}
	\label{eq:variety}
	\textit{div}(d) = 1 - \frac{2}{N_d(N_d-1)}
	\sum_{i=1}^{N_d-1} \sum_{\substack{j=i+1}}^{N_d} R(c(i,d), c(j,d))
\end{equation}
\normalsize
\noindent where \textit{R} is the ROUGE-L F1 score\footnote{We use the \href{https://pypi.org/project/rouge-score}{rouge-score} package in our analysis.} \cite{lin-2004-rouge}, and $N_d$ the length (in comments) of discussion $d$.

 We will be using \emph{diversity} in this paper, although we note that more robust synthetic quality metrics are needed.


\subsection{LLMs as Human Subjects}
\label{ssec:related:human-llm}

\citet{grossman_2023} argue that synthetic agents have the potential to eventually replace human participants, a perspective shared by other researchers \cite{tornberg_2023, argyle2023}. Indeed, \acp{LLM} have demonstrated complex, emergent social behaviors \cite{Park2023GenerativeAI, demarzo_2023, leng_2024, abdelnabi_negotiations, abramski_2023, hewitt2024predicting, park2024generativeagentsimulations1000}.

However, significant limitations of \acp{LLM} remain in the context of Social Science experiments. Issues include undetectable behavioral hallucinations \cite{rossi_2024}; socio-demographic, statistical and political biases \cite{anthis_2025,hewitt2024predicting,rossi_2024, Taubenfeld2024SystematicBI}; unreliable annotations \cite{jansen_2023,bisbee_2023,neumann_2025, Gligoric2024CanUL}; non-deterministic outputs \cite{atil_2025, bisbee_2023}; and excessive agreeableness \cite{Park2023GenerativeAI, anthis_2025, rossi_2024}.

Our study must thus be conservative towards the generalizability of our results to discussions with humans. We stress that our methodology is designed for “debugging” and exploring \ac{LLM} facilitators in-silico, before testing them in much more costly experiments with human participants. Reproduction studies with humans are ultimately needed, and we leave them for future work.