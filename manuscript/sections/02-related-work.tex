% !TEX root = ../main.tex
%

\section{Related Work}


\subsection{Synthetic discussions}

\ac{LLM} agents talking with each other has been an active area of research outside the context of discussion simulation. The term “\ac{LLM} self-talk” is usually used in this case, a term adapted from “self-play” in \ac{RL} \citep{cheng-self-play}). For instance, researchers have experimented with using self-talk for reinforcing \acp{LLM} against jailbreaking \cite{liu2024largelanguagemodelsagents, cheng-self-play, zheng2024optimalllmalignmentsusing}, for \ac{LLM} alignment \cite{Bai2022ConstitutionalAH, collective_constitution}, and self-refinement in general \cite{Madaan2023SelfRefineIR, lambert2024} by pitting the model against itself. \citet{ulmer2024} give their \ac{LLM} agents distinct roles in a conversational setting and demonstrate that \ac{LLM} self-talk can occasionally produce high-quality, convincing discussions. These discussions can be used to further finetune the model.  \citet{abdelnabi_negotiations} show that \acp{LLM} can execute (to some extent) social strategies in discussions in order to achieve long-term goals. An example is a negotiation where an environmentalist \ac{LLM} agent may be stoking disagreement between other agents, in order to eventually sabotage the negotiations for the creation of a new factory.


\subsection{Replicating human behavior with LLMs}

Studies demonstrate that while \acp{LLM} can not fully replicate human behavior, they approximate it to a limited extent. \citet{hewitt2024predicting} recommend employing \acp{LLM}s in social science experiments for minority groups, particularly pilot studies, but highlight limitations including low variance, model bias, and challenges in encoding all participant details into prompts. \citet{park2024generativeagentsimulations1000} reveal that substituting \acp{SDB} with interview-based information enhances their authenticity. However, their approach relies on extensive human interviews and computational resources.

\acp{LLM} can also simulate human behavior within communities. \citet{park_simulacra} use a single \ac{LLM} with \ac{SDB} prompting to generate entire Reddit communities, producing synthetic discussions indistinguishable from real ones. \citet{Park2023GenerativeAI} create an interactive in-game world where \ac{LLM}-controlled \acp{NPC} engage with players, the environment, and each other, displaying complex social behavior like information sharing and event planning. However, they note that these agents tend to be “too agreeable”, likely due to alignment artifacts.


\subsection{LLM moderation}

\citet{korre2025evaluation} identified moderation functions that \acp{LLM} can replace. These include detecting toxicity \cite{kang-qian-2024-implanting, Wang2022ToxicityDW}, hate-speech \cite{Nirmal2024TowardsIH, shi-2024-hatespeech}, and misinformation \cite{Liu2024DetectIJ, Xu2024ACS}. Unlike traditional \ac{ML} models, \acp{LLM} can actively moderate through conversational abilities. They can warn users for rule violations \cite{Kumar_AbuHashem_Durumeric_2024}, monitor engagement \cite{schroeder-etal-2024-fora}, and aggregate diverse opinions \cite{small-polis-llm}. These capabilities suggest that \acp{LLM} can replace human moderators in many tasks \cite{small-polis-llm, seering_self_moderation}.  \citet{small-polis-llm} highlight the potential of \acp{LLM} to use discussion facilitation techniques and starting discussions by generating initial opinions. However, they note that traditional \ac{IR} methods outperform \acp{LLM} in selecting appropriate starting points for discussions \cite{karadzhov2023delidata}, while some facilitation guidelines \cite{dimitra-book} explicitly prohibit moderators from performing these tasks. \acp{LLM} can also aid users, particularly minority or ethnic groups, by providing translations and improving grammar \cite{Tsai2024Generative}. 

Moderator chatbots have shown promise; \citet{kim_et_al_chatbot} demonstrated that simple rule-based models can enhance discussions. The work of \citet{cho-etal-2024-language} 
is the closest to our own; they use \ac{LLM} moderators in human discussions, and their strategies are obtained from general conversational Social Science theory. This is in contrast to our work, which uses \ac{LLM} user-agents and focuses specifically on literature and current practices on moderation. They show that \ac{LLM} moderators can provide “specific and fair feedback” to users, although the \acp{LLM} struggle to make users more respectful and cooperative.