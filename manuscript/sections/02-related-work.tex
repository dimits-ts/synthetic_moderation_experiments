% !TEX root = ../main.tex
%
\section{Background and Related Work}

\subsection{LLMs as Human Subjects}
\label{ssec:related:human-llm}

When conducting social experiments with \acp{LLM} instead of human subjects, it is imperative to know how representative results can be. \citet{grossman_2023} argue that synthetic agents have the potential to eventually replace human participants, a perspective shared by other researchers \cite{tornberg_2023, argyle2023}. Indeed, \acp{LLM} have demonstrated emergent complex social behaviors \cite{Park2023GenerativeAI, demarzo_2023, leng_2024, abdelnabi_negotiations, abramski_2023}, and are able to infer survey responses from \acp{SDB} \cite{hewitt2024predicting} and personalized interviews \cite{park2024generativeagentsimulations1000}.

However, significant limitations of \acp{LLM} remain in the context of Social Science experiments. Issues include dataset contamination; undetectable behavioral hallucinations \cite{rossi_2024}; sociodemographic, statistical and political biases \cite{anthis_2025,hewitt2024predicting,rossi_2024}, often amplified during discussions \cite{Taubenfeld2024SystematicBI}; unreliable survey responses \cite{jansen_2023,bisbee_2023,neumann_2025}; inconsistent annotations \cite{Gligoric2024CanUL}; non-deterministic outputs \cite{atil_2025}, especially in closed-source models \cite{bisbee_2023}; and excessive agreeableness due to alignment procedures \cite{Park2023GenerativeAI, anthis_2025, rossi_2024}. Despite these shortcomings, researchers frequently anthropomorphize \ac{LLM} agents \cite{rossi_2024}, obscuring the true causes of their behavior \cite{anthis_2025,zhou-etal-2024-real}. 

Our study must thus be conservative towards the generalizability of our results to discussions with human participants. We stress that our methodology is designed for “debugging” and exploring artificial facilitators in silico, before testing them in much more costly experiments with human participants. Experiments with real participants, however, are ultimately needed, and we leave them for future work.


\subsection{Evaluating Discussion Quality}
\label{ssec:related:quality}

Synthetic discussions often degrade rapidly without human interaction, exhibiting repetitive, low-quality content \citep{ulmer2024}. However, research on quantifying synthetic data quality is currently limited.

\citet{balog_2024} introduce metrics utilizing comparisons with human data, but this approach depends on datasets with the same topics, and lacks scientific grounding since believable \ac{LLM} outputs do not necessarily lead to behavior simulation \cite{rossi_2024}. Their most generalizable metric—a vague “coherence” score—is \ac{LLM}-annotated without theoretical support. \citet{kim_et_al_chatbot} rely on post-discussion surveys and lexical diversity to estimate the number of diverse opinions.

Alternatively, \citet{ulmer2024} propose \textit{“Diversity”}, which penalizes repeated sequences between comments in a discussion:

\small
\begin{equation}
\label{eq:variety}
    \textit{div}(d) = 1 - \frac{2}{N_d(N_d-1)}
\sum_{i=1}^{N_d} \sum_{\substack{j=i+1}}^{N_d} R(c(i,d), c(j,d))
\end{equation}
\normalsize

\noindent where \textit{R} is the ROUGE-L F1 score\footnote{We use the \href{https://pypi.org/project/rouge-score}{rouge-score} package in our analysis.} \cite{lin-2004-rouge}, and $N_d$ the length (in comments) of discussion $d$.

Low diversity points to pathological problems (e.g., \acp{LLM} repeating previous comments). Extremely high diversity scores, on the other hand, may point to a lack of interaction between participants; a discussion in which participants engage with each other will feature some lexical overlap (e.g., common terms, paraphrasing points of other participants). We can instead compare the distribution of \textit{diversity} scores for synthetic discussions with that measured on sampled human discussions. This allows us to estimate the extent to which synthetic discussions approximate real-world content variety and participant interaction.%, or at the very least, points to pathological problems in our generated data. 

Besides metrics for the quality of synthetic data, we also need metrics that can quantify how ``well'' a discussion is going from a human standpoint. We choose Toxicity for two reasons: Prompting \acp{LLM} for toxicity detection is reliable \citep{kang-qian-2024-implanting, Wang2022ToxicityDW, anjum2024hate}, and toxicity can inhibit online and deliberative discussions \citep{dekock2022disagree, XiaToxicity}.\footnote{We note that this is not always true \citep{Avalle2024PersistentIP}.}


\subsection{Synthetic Discussions}
\label{ssec:related:discussions}

Synthetic discussion systems include synthetic clones of Reddit \cite{park_simulacra}, Twitter/X \cite{mou_2024}, social media in general \cite{tornberg_2023, y_social} as well as games \cite{Park2023GenerativeAI} and social experiments \cite{zhou_2024_sotopia}.

\citet{balog_2024} introduce their own methodology to produce synthetic discussions, where they extract topics and comments from real-world online discussions, and prompt an \ac{LLM} to continue them. Unlike our approach, they do not use \ac{LLM} user-agents to model conversational dynamics, nor do they model the presence of facilitators. Their methodology faces challenges when \acp{LLM} generate malformed metadata, for which they offer no solution, and relies on the existence of suitable human discussion datasets.

\citet{ulmer2024} create synthetic discussions between two participants; an agent (who controls the environment) and a client (who interacts with the agent). They then filter the generated discussions and use them as training data to further finetune the agent \ac{LLM} for a specific task. Their approach however does not model the existence of multiple clients (users), nor is it applied on online discussion facilitation. Our proposed methodology can be modelled as a generalization of their paradigm; an agent (moderator) converses with multiple clients (non-moderator users).

Finally, \citet{abdelnabi_negotiations} create synthetic negotiations with multiple agents having various agendas and responsibilities. Our work can be modelled as a domain shift of their methodology from negotiations, to discussion facilitation; participants with different motivations (i.e., normal users, trolls, long-standing community members), interact with themselves and a stakeholder holding veto power (facilitator) who presides over the discussion.


\subsection{LLM Facilitation}

Unlike traditional \ac{ML} models, \acp{LLM} can actively facilitate discussions \cite{korre2025evaluation}. They can warn users for rule violations \cite{Kumar_AbuHashem_Durumeric_2024}, monitor engagement \cite{schroeder-etal-2024-fora}, aggregate diverse opinions \cite{small-polis-llm}, provide translations and writing tips, which is especially useful for marginalized groups \cite{Tsai2024Generative}. These capabilities suggest that \acp{LLM} may be able to assist or even replace human facilitators in many tasks \cite{seering_self_moderation}.

Moderator chatbots have shown promise; \citet{kim_et_al_chatbot} demonstrated that simple rule-based models can enhance discussions, although their approach was largely confined to organizing the discussion based on the``think-pair-share'' framework \cite{ahmad_2010_supporting, Navajas2018}, and balancing user activity. \citet{cho-etal-2024-language} use \ac{LLM} facilitators in human discussions, with moderation strategies based on Cognitive Behavioral Therapy and the work of \citet{rosenberg2015nonviolent}. They show that \ac{LLM} facilitators can provide “specific and fair feedback” to users, although they struggle to make users more respectful and cooperative.  In contrast to both works, our work uses exclusively \ac{LLM} participants (and \ac{LLM} facilitators), and tests them in an explicitly toxic and challenging environment.