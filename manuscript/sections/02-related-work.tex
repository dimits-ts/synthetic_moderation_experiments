% !TEX root = ../main.tex
%
\section{Related Work}

\subsection{LLMs as human subjects}
\label{ssec:related:human-llm}

Recent advancements in \acp{LLM} have sparked considerable debate among researchers, particularly within the field of Social Science. As argued by \citet{grossman_2023}, synthetic agents have the potential to not only generate synthetic data for social experiments, but also eventually replace human participants, a perspective shared by other researchers \cite{tornberg_2023, argyle2023}. \acp{LLM} have demonstrated emergent behaviors such as information diffusion \cite{Park2023GenerativeAI}, scale-free networks \cite{demarzo_2023}, social behavior (to an extent) \cite{leng_2024}, social strategies \cite{abdelnabi_negotiations}, and certain psychological patterns \cite{abramski_2023}, while also being capable of predicting human survey responses given extensive personal data \cite{park2024generativeagentsimulations1000}. If realized, this development could revolutionize Social Sciences by alleviating significant costs and challenges associated with human participation in research \cite{rossi_2024, shapiro2019polling}.

However, \citet{rossi_2024} raise valid concerns regarding the limitations of assuming \ac{LLM} responses reflect human behavior accurately. These include issues such as dataset contamination (where models may inadvertently draw from previous studies' data), potential hallucinations at the behavioral level (which are inherently undetectable) and the unsupported belief of equating “believable” outputs with replicating human behavior. Numerous studies point to sociodemographic, statistical and political model biases \cite{anthis_2025, hewitt2024predicting, rossi_2024}, which can be reinforced during the course of a discussion in the case of the latter \cite{Taubenfeld2024SystematicBI}. Furthermore, other studies have demonstrated that \ac{LLM} survey responses \cite{jansen_2023, bisbee_2023, neumann_2025}, as well as \ac{LLM} annotations \cite{Gligoric2024CanUL} are generally unreliable and surface-level. Additionally, \ac{LLM} outputs are non-deterministic particularly in closed-source models \cite{bisbee_2023}, but also in general \cite{atil_2025}, which is an important concern in the context of the broader replication crisis in the field of Social Science.

Finally, while some emergent behaviors such as information diffusion can be objectively observed, there is a tendency among researchers to anthropomorphize the behaviors of \acp{LLM} in their studies \cite{rossi_2024}. It is essential to recognize that \acp{LLM} operate on fundamentally different principles than humans, and thus, their outputs should not be attributed with human-like traits or intentions. This anthropomorphization can introduce researcher bias and potentially obfuscate the true nature of \ac{LLM} behaviors. Furthermore, \citet{anthis_2025} point out that \acp{LLM} could “fake” behaviors that would align with researcher expectations and \citet{zhou-etal-2024-real} caution against the realism of one \ac{LLM} controlling multiple agents having complete information. We add that the process of crafting instruction prompts and other configurations for synthetic experiments by itself encodes researcher bias and expectations, although it may be essential for getting around model alignment.


\subsection{Synthetic discussions}
\label{ssec:related:discussions}

\ac{LLM} agents talking with each other has been an active area of research outside the context of discussion simulation. The term “\ac{LLM} self-talk” is sometimes used in this case, a term adapted from “self-play” in \ac{RL} \citep{cheng-self-play, ulmer2024}. Researchers have experimented with using self-talk for reinforcing \acp{LLM} against jailbreaking \cite{liu2024largelanguagemodelsagents, cheng-self-play, zheng2024optimalllmalignmentsusing}, for \ac{LLM} alignment \cite{Bai2022ConstitutionalAH, collective_constitution}, and self-refinement in general \cite{Madaan2023SelfRefineIR, lambert2024} by pitting the model against itself. Self-talk is not necessarily tied to \ac{RL} methods; \citet{ulmer2024} create fictional scenarios where the participating \ac{LLM} agents have distinct characters. They demonstrate that \ac{LLM} self-talk can occasionally produce high-quality discussions, which are then used to further finetune the model, an approach that is underexplored in more complex social scenarios \cite{zhou-etal-2024-real}. \citet{balog_2024} showcase a new methodology where posts are generated using a thread structure, and where past comments are summarized. This methodology is vulnerable to the \ac{LLM} breaking the structure of the discussion by generating malformed data, which is not addressed in their work. \citet{zhou_2024_sotopia} present “SOTOPIA” a general framework that creates complex social situations and evaluates synthetic agents on task-specific metrics.

Synthetic discussions are often studied in the context of “digital twins” of social media sites, which aim to replicate their environment and study their operation using synthetic users. These range from synthetic clones of Reddit \cite{park_simulacra}, Twitter \cite{mou_2024}, and social media in general \cite{tornberg_2023, y_social}. Digital twins are not limited to social media; \citet{Park2023GenerativeAI} create an interactive in-game world with \ac{LLM}-controlled \acp{NPC}. While socially capable, these agents tend to be “too agreeable”, likely due to alignment artifacts, which is a general issue stemming from \acp{LLM} being explicitly designed as assistants \cite{anthis_2025}. 

Synthetic discussions often degrade rapidly without human interaction, exhibiting repetitive, low-quality content \citep{ulmer2024}. To address this, we require robust “synthetic quality” metrics that capture internal characteristics, rather than realism. \citet{balog_2024} introduce metrics utilizing comparisons with human data, but this approach depends on human datasets with the same topics and lacks scientific grounding due to unestablished links between human-like text and behavior (see Section \ref{ssec:related:human-llm}). Their most generalizable metric—a vague coherence score—is \ac{LLM}-annotated without theoretical support. Alternatively, \citet{ulmer2024} propose metrics like N-gram-based Diversity, which is topic-agnostic, methodology-independent, and correlates with effective fine-tuning data


\subsection{LLM moderation}

\citet{korre2025evaluation} identified moderation functions that \acp{LLM} can replace. \acp{LLM} have proven capable of detecting toxicity \cite{kang-qian-2024-implanting, Wang2022ToxicityDW}, hate-speech \cite{Nirmal2024TowardsIH, shi-2024-hatespeech}, and misinformation \cite{Liu2024DetectIJ, Xu2024ACS}. Unlike traditional \ac{ML} models, \acp{LLM} can actively moderate through conversational abilities. They can warn users for rule violations \cite{Kumar_AbuHashem_Durumeric_2024}, monitor engagement \cite{schroeder-etal-2024-fora}, and aggregate diverse opinions \cite{small-polis-llm}. These capabilities suggest that \acp{LLM} can replace human moderators in many tasks \cite{small-polis-llm, seering_self_moderation}. \citet{small-polis-llm} suggest that \acp{LLM} can start discussions by generating initial opinions, although traditional Information Retrieval methods outperform \acp{LLM} in selecting appropriate starting points for discussions \cite{karadzhov2023delidata}, and some guidelines explicitly prohibit moderators from performing these tasks \cite{dimitra-book}. \acp{LLM} can also aid users, particularly minority or ethnic groups, by providing translations and improving grammar \cite{Tsai2024Generative}. 

Moderator chatbots have shown promise; \citet{kim_et_al_chatbot} demonstrated that simple rule-based models can enhance discussions. \citet{cho-etal-2024-language} use \ac{LLM} moderators in human discussions, with moderation strategies based on Cognitive Behavioral Therapy and the work of \citet{rosenberg2015nonviolent}. This is in contrast to our work, which uses \ac{LLM} user-agents and focuses specifically on conversational moderation literature and current practices. They show that \ac{LLM} moderators can provide “specific and fair feedback” to users, although the \acp{LLM} struggle to make users more respectful and cooperative. Finally, \citet{dtsirmpas_thesis} use \ac{LLM} moderators in synthetic discussions and investigate the use of \ac{LLM} annotator-agents for discussion evaluation.