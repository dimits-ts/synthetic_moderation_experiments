% !TEX root = ../main.tex
%

\section{Introduction}
\label{sec:introduction}

Research on moderation techniques is instrumental in keeping up with a changing online environment, but it has stayed significantly behind current needs \cite{seering_self_moderation, make_reddit_great}.
% This paper is part of ongoing work we are carrying out in order to answer the following questions: \textit{Which moderation strategy works best for \acf{LLM} moderators?}
One of the largest obstacles encountered in both research and practical applications of moderation is the need for extensive human effort; in research, to conduct experiments and annotation, and in practical applications, to moderate the discussions. This leads to a slow pace of development and exorbitant costs for researchers and platforms alike. Indeed, many platforms can only keep up by “outsourcing” moderation to volunteers or the users themselves \cite{Matias2019TheCL, schaffner_community_guidelines, seering_self_moderation}. Limited automation, usually via traditional \ac{ML} models, somewhat alleviates manual, human moderation \cite{horta_automated_moderation}, but not to a significant enough scale in practice \cite{schaffner_community_guidelines}. Research is also hampered by a critical lack of datasets pertaining to online moderation \cite{korre2025evaluation}. \acfp{LLM} have shown promise in simulating human behavior in social studies \cite{park2024generativeagentsimulations1000, hewitt2024predicting, Park2023GenerativeAI}, as well as adversarial agents in text-based tasks \cite{cheng2024selfplayingadversariallanguagegame}. Our previous work identified the need for automated \ac{LLM}-based methods to evaluate “moderation strategies” \cite{korre2025evaluation}, and demonstrated that such methods may be viable in practice \cite{dtsirmpas_thesis}. We define “moderation strategies” as any set of instructions given to a moderator. This paper is part of ongoing work we are carrying out to answer the following questions: \textit{How can \acp{LLM} help moderate online discussions? Which moderation strategies work best for \ac{LLM} moderators? How effective are these moderation strategies compared to unmoderated discussions?}

We propose a methodology for leveraging synthetic experiments performed exclusively by \acp{LLM} to initially bypass the need for human participation in experiments involving online moderation. Using this methodology, we evaluate two moderation strategies employed by human moderators \cite{dimitra-guide, Cornell_eRulemaking2017}, a new strategy we propose, inspired by a \acf{RL} formulation of the moderation problem (though we do not yet use \ac{RL} in our work), two real-world baseline strategies: an out-of-the-box \ac{LLM} moderator and a \ac{LLM} moderator based on general guidelines from alignment work \cite{collective_constitution}, and a baseline with no moderation at all. Through synthetic discussions generated using this approach, we examine how varied the discussions by three different open-source \acp{LLM} are, and we analyze the behavior of \ac{LLM} moderators and role-based user-agents. We find that our own proposed \ac{RL}-inspired strategy significantly outperforms established moderation guidelines, as well as out-of-the-box \ac{LLM} moderation. We also find that smaller \acp{LLM}, with less intensive instruction-tuning, can create more varied discussions than larger models. Finally, we make available “SynDisco,” an open-source Python framework designed to create and assess synthetic discussions, as well as a publicly accessible dataset \datasetlink containing evaluated synthetic discussions. 