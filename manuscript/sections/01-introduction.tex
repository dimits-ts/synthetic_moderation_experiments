% !TEX root = ../main.tex
%
\section{Introduction}
\label{sec:introduction}

Research on conversational moderation/facilitation techniques (distinct from “content moderation”, which involves flagging and removing content) is crucial for adapting to evolving online environments, but lags significantly behind current demands \cite{seering_self_moderation, make_reddit_great}. A major challenge lies in the substantial human effort required—both in researching and moderating discussions, which results in slow development and elevated costs for researchers and platforms alike. Many platforms overcome this by outsourcing moderation to volunteers or users \cite{Matias2019TheCL, schaffner_community_guidelines}, while others turn to content moderation using traditional \ac{ML} models, which are not enough in practice \cite{horta_automated_moderation, schaffner_community_guidelines}. 

\acfp{LLM} have shown promise in simulating human behavior in social studies \cite{park2024generativeagentsimulations1000, hewitt2024predicting, Park2023GenerativeAI} and have thus been hypothesized to be capable of conversational moderation and facilitation tasks \cite{small-polis-llm, korre2025evaluation}. While studies exist for simulating user interactions in social media \cite{park_simulacra, mou_2024, tornberg_2023, y_social, balog_2024}, and separately for the use of synthetic moderators \cite{kim_et_al_chatbot, cho-etal-2024-language}, none so far have combined the two approaches. 

We propose a simple and generalizable approach using \ac{LLM}-driven synthetic experiments for online moderation research, enabling fast and inexpensive parameter testing (e.g., prompts, \ac{LLM} moderator instructions) without human involvement. We demonstrate that each step of our methodology significantly contributes to generating high-quality synthetic data and examine the effects of different \acp{LLM} through an ablation study.

Using this methodology, we examine four \ac{LLM} moderation strategies based on current Social Science facilitation research:  \ac{LLM} alignment guidelines \cite{collective_constitution}, prompts based on human facilitation guidelines \cite{Cornell_eRulemaking2017, dimitra-book} and our own prompt based on \ac{RL} (although we do not perform \ac{RL} in this paper). We compare them with two baselines involving no moderator intervention and no explicit instructions. The experiments were conducted using exclusively open-source \acp{LLM} and include all relevant configurations in order to make our study as reproducible as possible (see Appendix \ref{ssec:appendix:annotation}, \ref{ssec:appendix:prompts}).

We evaluate discussions using \ac{LLM} annotator-agents. Our analysis reveals two key findings: (1) our proposed \ac{RL} moderation strategy demonstrated superior performance compared to alternative strategies and baselines; (2) the presence of \ac{LLM} moderators exhibited a positive and statistically significant influence on the quality of synthetic discussions. We do not make the claim that the behavior of \ac{LLM} user-agents is representative of human behavior, as this claim can be scarcely made in Social Science studies involving \ac{LLM} test subjects \cite{rossi_2024}—we discuss this subject in depth in Section \ref{ssec:related:human-llm}. The synthetic data presented in this paper, however, can be used to glean insights on how various \acp{LLM} change their behavior under these strategies, and can be used to finetune specialized moderator \acp{LLM}.

Finally, we make available “SynDisco”, an open-source Python framework designed to create and evaluate synthetic discussions, as well as the \ac{VMD}\datasetlink; a publicly accessible dataset containing the evaluated synthetic discussions. 