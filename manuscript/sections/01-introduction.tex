% !TEX root = ../main.tex
%

\section{Introduction}
\label{sec:introduction}


Research on moderation techniques is crucial for adapting to evolving online environments, but lags significantly behind current demands \cite{seering_self_moderation, make_reddit_great}. A major challenge lies in the substantial human effort required—both in researching and moderating discussions, which results in slow development and elevated costs for researchers and platforms alike. Many platforms overcome this by outsourcing moderation to volunteers or users through self-moderation mechanisms \cite{Matias2019TheCL, schaffner_community_guidelines}, while others turn to automation (based on simple \ac{ML} models), which is however not enough in practice \cite{horta_automated_moderation, schaffner_community_guidelines}. \acfp{LLM} have shown promise in simulating human behavior in social studies \cite{park2024generativeagentsimulations1000, hewitt2024predicting, Park2023GenerativeAI}, as well as adversarial agents in text-based tasks \cite{cheng-self-play}. However, the extent to which they can authentically replicate human behavior remains uncertain. Additionally, no studies to our knowledge suggest and evaluate techniques with which these discussions can become more representative.

We propose a methodology for leveraging synthetic experiments performed exclusively by \acp{LLM} to initially bypass the need for human participation in experiments involving online moderation.  Through this approach, we conducted six distinct experiments focusing on the instructions provided to \ac{LLM} moderators (“moderation strategies”). These included two baselines: one with no moderator intervention and another where moderators were not provided explicit instructions. Subsequent experiments incorporated alignment guidelines derived from the Collective Constitution report \cite{collective_constitution}, as well as moderation manuals such as those from Cornell's e-Rulemaking Initiative \cite{Cornell_eRulemaking2017} and the MIT Center for Constructive Communications \cite{dimitra-book}. Additionally, we developed our own prompt based on \ac{RL} principles (although we do not perform \ac{RL} in this paper). Finally, an ablation study was conducted to examine the impact of varying prompts, turn-taking algorithms, \ac{LLM} architectures, and the inclusion of diverse roles and \acp{SDB} among synthetic participants.

To evaluate the outcomes of these experiments, we utilized \ac{LLM} annotator-agents to assess the generated discussions. Our analysis revealed two key findings: first, our proposed methodology demonstrated significantly superior performance compared to alternative strategies; second, \ac{LLM} moderators exhibited a positive and statistically significant influence on the characteristics of synthetic discussions. Furthermore, we observed that synthetic discussions displayed limited representativeness of real human discussions, although our methodology meaningfully enhanced their plausibility. Notably, persistent pathological speech patterns were identified in both synthetic and actual discussions.

Finally, we make available “SynDisco”, an open-source Python framework designed to create and assess synthetic discussions, as well as the \ac{VMD}; a publicly accessible dataset containing evaluated synthetic discussions. 