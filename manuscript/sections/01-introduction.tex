% !TEX root = ../main.tex
%

\section{Introduction}
\label{sec:introduction}

Research on moderation techniques is crucial for adapting to evolving online environments, but lags significantly behind current demands \cite{seering_self_moderation, make_reddit_great}. A major challenge lies in the substantial human effort required—both in researching and moderating discussions, which results in slow development and elevated costs for researchers and platforms alike. Many platforms overcome this by outsourcing moderation to volunteers or users through self-moderation mechanisms \cite{Matias2019TheCL, schaffner_community_guidelines}, while others turn to content moderation using traditional \ac{ML} models, which are not enough in practice \cite{horta_automated_moderation, schaffner_community_guidelines}. \acfp{LLM} have shown promise in simulating human behavior in social studies \cite{park2024generativeagentsimulations1000, hewitt2024predicting, Park2023GenerativeAI}, as well as adversarial agents in text-based tasks \cite{cheng-self-play} and have thus been hypothesized to be capable of conversational moderation and facilitation tasks \cite{small-polis-llm, korre2025evaluation}. However, there is no study to our knowledge applying synthetic discussions in the field of discussion moderation/facilitation (“moderation” for simplicity).

We propose a methodology for leveraging synthetic experiments performed exclusively by \acp{LLM} to initially bypass the need for human participation in experiments involving online moderation. While such methodologies do exist for social media simulations \cite{park_simulacra, mou_2024, tornberg_2023, y_social, balog_2024}, ours attempts to create useful synthetic data while being as simple and generalizable as possible in order for researchers to test and evaluate parameters such as prompting templates and agent instructions in a fast and inexpensive way. We prove that each step in our methodology meaningfully contributes to the generation of high-quality synthetic data through an ablation study. 

Using this methodology, we conduct six experiments examining \ac{LLM} moderation strategies: Two baselines involving no moderator intervention or explicit instructions, \ac{LLM} alignment guidelines \cite{collective_constitution}, prompts based on human facilitation guidelines \cite{Cornell_eRulemaking2017, dimitra-book} and our own prompt based on \ac{RL} (although we do not perform \ac{RL} in this paper). The experiments were conducted using exclusively open-source \acp{LLM} and include all relevant configurations in order to make our study as reproducible as possible. 

To evaluate the outcomes of these experiments, we evaluate discussions using \ac{LLM} annotator-agents. Our analysis reveals two key findings: (1) our proposed methodology demonstrated significantly superior performance compared to alternative strategies; (2) \ac{LLM} moderators exhibited a positive and statistically significant influence on the quality of synthetic discussions. We do not make the claim that the behavior of \ac{LLM} user-agents is representative of human behavior, as this claim can be scarcely made in Social Science studies involving \acp{LLM} \cite{rossi_2024}—we discuss this subject in-depth in Section \ref{ssec:related:human-llm}. The synthetic data presented in this paper, however, can be used to glean insights on how various \acp{LLM} change their behavior under these strategies, and can be used to finetune specialized moderator \acp{LLM}.

Finally, we make available “SynDisco”, an open-source Python framework designed to create and evaluate synthetic discussions, as well as the \ac{VMD}\datasetlink; a publicly accessible dataset containing the evaluated synthetic discussions. 