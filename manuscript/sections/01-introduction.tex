% !TEX root = ../main.tex
%
\section{Introduction}
\label{sec:introduction}

The modern social media environment has evolved to be extremely demanding, with large social networks facing an ever-increasing onslaught of targeted misinformation \cite{clemons2025disinformation, Denniss2025Social}, hate speech \cite{kolluri2025parler} and polarization \cite{pranesh2024impactsocialmediapolarization}.
Platform designers and researchers traditionally focused on flagging and removing problematic content (``content moderation'' --- \citet{seering_self_moderation, cresci_pesonalized_interventions}), but these methods are no longer sufficient in practice \cite{horta_automated_moderation, schaffner_community_guidelines, small-polis-llm, korre2025evaluation}. Instead, online communities are at their best when moderators actively discuss and explain their actions (``conversational moderation'' or ``facilitation'' --- \citet{argyle2023, korre2025evaluation, falk-etal-2021-predicting}); thus preventing problematic user behavior before it surfaces \cite{cho-etal-2024-language, seering_self_moderation, cresci_pesonalized_interventions, make_reddit_great}, as well as supporting community deliberation and group decision-making \cite{kim_et_al_chatbot, seering_self_moderation}. 

\begin{figure}[t]
	\centering
	\includegraphics[width=\columnwidth]{research_goal.png}
	\caption{\ac{LLM} user-agents with distinct \acp{SDB} participate in a discussion, while the \ac{LLM} moderator monitors and attempts to improve the quality of the discussion. We need to design prompts and configurations for both types of \ac{LLM} agents.}
	\label{fig::goals}
\end{figure}

\acfp{LLM} have been hypothesized to be capable of facilitation tasks and can be scaled to a far greater extent compared to human facilitators \cite{korre2025evaluation, small-polis-llm}. However, experimentation and development on these systems is hampered due to the costs of human participation (\citet{rossi_2024} --- in this case, human discussants and evaluators). 

We posit that simulations with all-\ac{LLM}-agents can be a cheap and fast way to develop and test  \ac{LLM} facilitators, initial versions of which may be unstable or unpredictable \cite{atil_2025, rossi_2024}, before testing them with human participants. We propose a simple and generalizable methodology which enables rapid model “debugging” and parameter testing (e.g., discarding sub-optimal  prompts for the \ac{LLM} facilitator) without human involvement (Fig.~\ref{fig::goals}, \S\ref{sec:methodology}). An ablation study demonstrates that each component of our methodology substantially contributes to generating high-quality data (\S\ref{ssec:results:ablation}). 

Through this methodology, we examine  four \ac{LLM} facilitation strategies based on current Social Science facilitation research and compare them with two common facilitation setups (no facilitator, \acp{LLM} with simplistic prompts; \S\ref{sec:experimental}). Our work thus asks two questions: \emph{(1) Can we produce high-quality synthetic discussions, by crafting an appropriate environment for simulations? (2) Are facilitation strategies proposed in modern Social Science research able to help \ac{LLM} facilitators?} We find that: (1) the presence of \ac{LLM} facilitators has a \emph{positive, statistically significant} influence on the quality of synthetic discussions, and (2) facilitation strategies inspired by Social Science research often \emph{do not outperform simpler strategies} (\S\ref{ssec:results:main}).

Finally, we release \syndisco, an open-source Python framework that implements our methodology at scale, enabling the research community to rapidly experiment with \ac{LLM}-based facilitators. We also release \vmd a large, publicly available dataset with \ac{LLM}-generated and annotated synthetic discussions (\S\ref{sec:data-soft}). Our dataset can be used for \ac{LLM} facilitator finetuning \cite{ulmer2024}, as well as for observing the behavior of out-of-the-box \acp{LLM} in the task. We use open-source \acp{LLM} and include all relevant configurations in order to make our study as reproducible as possible (see \S\ref{ssec:appendix:annotation}, \S\ref{ssec:appendix:prompts}).
