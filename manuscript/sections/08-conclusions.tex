% !TEX root = ../main.tex
%
\section{Conclusions \& Future Work}

Our study is the first to apply synthetic data generation to the field of online discussion conversational moderation and facilitation. We propose a simple and generalizable methodology that enables researchers to inexpensively conduct pilot moderation experiments using exclusively synthetic \ac{LLM} user-agents. We also conduct an ablation study to demonstrate that each component of our methodology meaningfully results in higher-quality synthetic data.

We create an open-source Python Framework, called \syndisco that applies this methodology to hundreds of experiments, which we use to create and publish \vmd a large-scale synthetic dataset. Using this dataset, we compare the effectiveness of numerous moderation strategies and baselines for \ac{LLM} moderators, elicited from current conversational moderation research. We demonstrate that (1) \ac{LLM} moderators significantly improve the quality of synthetic discussions; (2) established human moderation/facilitation guidelines often do not surpass simple baselines with regard to toxicity and \ac{AQ}; (3) smaller \acp{LLM} such as Mistral Nemo (12B) can exceed the performance of larger models in the task of generating high-quality synthetic data; (4) Specialized instruction prompts may be needed for models to break out of instruction-tuned alignment and feature toxic comments in synthetic discussions. We hope that the methodology, synthetic dataset, and software presented in this paper can help research in the domain of \ac{LLM}-based moderation, and that the data presented in this paper can help finetune models for online moderation.
