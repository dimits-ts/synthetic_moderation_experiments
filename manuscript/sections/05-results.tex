% !TEX root = ../main.tex
%

\section{Results}
\label{sec:results}


\subsection{Main findings}
\label{ssec:results:main}

\paragraph{Finding 1: \ac{LLM} facilitators significantly improve synthetic discussions.} As shown in Fig.~\ref{fig:toxicity_stats}, comments in unmoderated discussions exhibit significantly more intense toxicity (ANOVA $p<.000$).\footnote{The large size of our dataset allows using parametric tests.} 

\begin{figure}
	\includegraphics[width=\linewidth]{toxicity_stats.png}
	\centering
	\caption{Difference in average toxicity levels for comments following pairs of facilitation strategies. Red cells ($x>0$) indicate that the strategy on the left performs worse than the one on the bottom, for an average of $x$ points in a scale of 1-5. Conversely for blue ($x<0$) cells. Black cells denote minute changes. Asterisks denote pairwise Student-t tests (\asterisknote).}
	\label{fig:toxicity_stats}
\end{figure}

\paragraph{Finding 2: More elaborate facilitation strategies fail to decrease toxicity.}
More elaborate facilitation strategies, such as \emph{\strategyregroom}, \emph{\strategyconstrcomm}, and our proposed \emph{\strategymodgame}, lead to a statistically significant reduction in comment toxicity over time compared to \emph{unmoderated} discussions (Table~\ref{tab:toxicity}). However, their additional impact beyond that of the simpler \emph{\strategynoinstr} strategy is marginal and sometimes not statistically significant (Fig.~\ref{fig:toxicity_stats}), suggesting that out-of-the-box \acp{LLM} may struggle to effectively leverage advanced instructions---echoing prior findings on the limitations of \ac{LLM} facilitators \cite{cho-etal-2024-language}.

\begin{table}[t]
	\centering
	\begin{tabular}{p{5cm} p{1.5cm}}
		\toprule
		\textbf{Variable} & \textbf{Toxicity} \\
		\midrule
		Intercept & 2.164\textsuperscript{***} \\
		\strategynoinstr & -0.426\textsuperscript{***} \\
		\strategymodgame & -0.435\textsuperscript{***} \\
		\strategyrules & -0.461\textsuperscript{***} \\
		\strategyregroom & -0.277\textsuperscript{***} \\
		\strategyconstrcomm & -0.230\textsuperscript{***} \\
		time & -0.012\textsuperscript{**} \\
		\strategynoinstr$\times$time & -0.003 \\
		\strategymodgame$\times$time & -0.011\textsuperscript{*} \\
		\strategyrules$\times$time & -0.008 \\
		\strategyregroom$\times$time & -0.023\textsuperscript{***} \\
		\strategyconstrcomm$\times$time & -0.023\textsuperscript{***} \\
		\bottomrule
	\end{tabular}
	\small
	\asterisknote
	\normalsize
	\caption{\ac{OLS} regression coefficients for toxicity ($Adj. R^2=0.054$). Average toxicity without facilitators is $2.164$. Each new comment slightly reduces average toxicity ($time = \minus0.012$). Negative values next to each strategy (e.g., $\strategyregroom = \minus0.277$) mean that discussions using that strategy are less toxic  compared to \textit{\strategynomod}. (e.g., by an average of $\minus0.277$ toxicity). Terms such as $\strategyregroom \times time = \minus0.023$ show that toxicity decreases more after each comment.}
	\label{tab:toxicity}
\end{table}


\paragraph{Finding 3: \ac{LLM} facilitators choose to intervene far too frequently, which is tolerated by the other participants .} Fig.~\ref{fig:intervention_count} demonstrates that \ac{LLM} facilitators intervene at almost any opportunity, even though they are instructed to only do so when necessary. This confirms that \acp{LLM} generally can not decide not to speak (\S\ref{ssec:methodology:turn}). To our knowledge, this has not been reported in relevant literature, and \emph{is an example of ``debugging'' problems with \acp{LLM}} --- a core motivation of our work.

Additionally, a qualitative look through the dataset reveals that \ac{LLM} user-agents exhibit atypical tolerance for excessive facilitator interventions. Humans in contrast, typically become irritated and more toxic after repeated, unneeded interventions \citep{schaffner_community_guidelines, make_reddit_great, proactive_moderation, cresci_pesonalized_interventions}. This is likely another artifact caused by alignment procedures, making \acp{LLM} too agreeable \citep{park2023game}.

\begin{figure}[t]
	\centering
	\includegraphics[width=\columnwidth]{intervention_count.png}
	\caption{Histogram of interventions by \ac{LLM} facilitators per strategy used.}
	\label{fig:intervention_count}
\end{figure}


\subsection{Ablation Study}
\label{ssec:results:ablation}

\begin{figure*}[t]
    \begin{subfigure}{0.32\linewidth}
        \includegraphics[width=\textwidth]{rougel_model.png}
        \caption{Model}
        \label{fig:rougel_model}
    \end{subfigure}%
    \hfill
    \begin{subfigure}{0.32\linewidth}
        \includegraphics[width=\textwidth]{rougel_turns.png}
        \caption{Turn-taking function $t$}
        \label{fig:rougel_turns}
    \end{subfigure}%
    \hfill
    \begin{subfigure}{0.32\linewidth}
        \includegraphics[width=\textwidth]{rougel_prompts.png}
        \caption{Prompting function $\phi$}
        \label{fig:rougel_prompts}
    \end{subfigure}%

    \caption{Diversity (\S\ref{ssec:related:quality}) distribution for each discussion by \ac{LLM} (\S\ref{ssec:experimental:setup}), turn-taking function $t$ (\S\ref{ssec:methodology:turn}), and prompting function $\phi$ used (\S\ref{ssec:methodology:prompts-instructions}).}
    \label{fig:diversity}
\end{figure*}

We generate eight synthetic discussions per ablation experiment, using a single model (Qwen 2.5). We compare the diversity (cf. \S\ref{ssec:related:quality}, \ref{ssec:experimental:evaluation}) of these discussions with ones from the \ac{CeRI} “Regulation Room” dataset\footnote{\url{http://archive.regulationroom.org}. Disclaimer: Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the \ac{CeRI}.}, which includes moderated online deliberative discussions for ten diverse topics.


\subsubsection{Effects of LLMs}

\paragraph{Larger models do not lead to more high-quality discussions.} As shown in Fig.~\ref{fig:rougel_model}, Qwen demonstrated the highest diversity among the evaluated models, indicating limited participant interaction (\S\ref{ssec:related:quality}), followed by Mistral Nemo and LLaMa. However, none of the models closely matched the diversity observed in human discussions. 


\subsubsection{Effects of Turn-Taking Functions}

\begin{figure}[t]
	\centering
	\includegraphics[width=0.8\linewidth]{toxicity_trolls.png}
	\caption{Non-troll toxicity levels in discussions with and without trolls. There is a significant uptick on the number of ``somewhat toxic" ($Toxicity=2$) comments when the participants are primed to respond to toxic comments.}
	\label{fig:toxicity_trolls}
\end{figure}


\paragraph{Our proposed turn-taking function substantially improves the quality of synthetic data.} We compare our turn-taking function (\S\ref{ssec:methodology:turn}) to two baselines: Round Robin (participants speaking one after the other, then repeating) and Random Selection (uniformly sampling another participant each turn). Fig.~\ref{fig:rougel_turns} demonstrates that although all distributions diverge from the blue—human—distribution, our function is the only one not exhibiting extremely high diversity (i.e., very limited participant interaction \S\ref{ssec:experimental:evaluation}).


\subsubsection{Effects of User Prompting}

We conduct three separate experiments in which participants are subjected to one of the following conditions at a time: (1) no assigned \acp{SDB}, (2) no assigned roles, or (3) only a very basic instruction prompt given (\S\ref{sssec:appendix:actors}). 

\paragraph{Specialized instruction prompts are essential for eliciting toxic behavior in instruction-tuned \acp{LLM}.} Our instruction prompt for the participants (\S\ref{ssec:methodology:prompts-instructions}) incentivizes them to react to toxic behavior. Indeed, inserting “troll” participants to discussions, leads to more intense toxicity among \emph{other} participants \emph{only if we instruct participants to react to toxic posts} (Fig.~\ref{fig:toxicity_trolls}). 

\paragraph{\acp{SDB}, roles, and our instruction prompt all increase the quality of synthetic data.} Fig.~\ref{fig:rougel_prompts} illustrates that incorporating \acp{SDB}, roles, and specialized instruction prompts, results in diversity scores more closely aligned with human discussions.
