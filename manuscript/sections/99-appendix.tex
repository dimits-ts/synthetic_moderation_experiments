% !TEX root = ../main.tex
%
\section{Technical Appendix}


\subsection{The Role of Roles}

We verify that the models and roles used did not by themselves impact the findings presented in \S\ref{ssec:results:main}. Fig.~\ref{fig:toxicity_aq_role} demonstrates that, as expected, only troll user-agents contribute on average intense toxicity and worse Arg. Quality in the synthetic discussions. Furthermore, Fig.~\ref{fig:toxicity_aq_model} shows that toxicity and Arg. Quality are on average not qualitatively dependent on the model used.

\begin{figure*}[ht]
	\includegraphics[width=0.45\linewidth]{toxicity_intent_barplot.png} \hfill
	\includegraphics[width=0.45\linewidth]{aq_intent_barplot.png}
	\centering
	\caption{Average Toxicity (left) and Arg. Quality (right) per LLM user-role (\S\ref{ssec:methodology:prompts-instructions}).}
	\label{fig:toxicity_aq_role}
\end{figure*}

\begin{figure*}[ht]
	\includegraphics[width=0.45\linewidth]{toxicity_llm_barplot.png} \hfill
	\includegraphics[width=0.45\linewidth]{aq_llm_barplot.png}
	\centering
	\caption{Boxplots for average Toxicity (left) and Arg. Quality (right) per LLM (\S\ref{ssec:experimental:setup}).}
	\label{fig:toxicity_aq_model}
\end{figure*}


\subsection{Investigating Comment Length}

\begin{figure*}[t]
	\begin{subfigure}{0.32\linewidth}
		\includegraphics[width=\textwidth]{comment_len_model.png}
		\caption{Model}
		\label{fig:comment_length_model}
	\end{subfigure}%
	\hfill
	\begin{subfigure}{0.32\linewidth}
		\includegraphics[width=\textwidth]{comment_len_turns.png}
		\caption{Turn-taking function $t$}
		\label{fig:comment_length_turns}
	\end{subfigure}%
	\hfill
	\begin{subfigure}{0.32\linewidth}
		\includegraphics[width=\textwidth]{comment_len_prompts.png}
		\caption{Prompting function $\phi$}
		\label{fig:comment_length_prompts}
	\end{subfigure}%
	
	\caption{Comment length for each discussion by LLM (\S\ref{ssec:experimental:setup}), turn-taking function $t$, and prompt used (\S\ref{ssec:methodology:us}). For ease of comparison, comments above 400 words are marked at the end of the x-axis.}
	\label{fig:comment_length}
\end{figure*}

We find that there is a statistically significant, negative correlation between comment length and diversity in synthetic discussions (Student's t-test  $p < .000$), although we cannot verify the existence of this pattern in human-generated comments ($p = 0.775$). This may partially explain the divergent diversity scores of different models in \S\ref{ssec:results:ablation}  (Fig.~\ref{fig:comment_length_model}) --- although it does not ultimately change the conclusions presented.


\subsection{Analyzing Annotations}

In this section, we examine the properties of LLM annotations, since it is necessary to ensure the robustness of our results. A key dimension for exploring annotations is annotator polarization. To measure it, we employ the nDFU metric introduced by \citet{pavlopoulos-likas-2024-polarized}, which quantifies polarization among $n$ annotators, ranging from 0 (perfect agreement) to 1 (maximum polarization).

Our analysis reveals a positive correlation between toxicity and annotator polarization: As demonstrated by Fig.~\ref{fig:ndfu_annot}, while there is general agreement on non-toxic comments, annotators struggle to reach consensus as toxicity becomes non-trivial ($\textit{toxicity} \in [2,5]$) with a statistically significant difference (Student's t-test $p < .000$). This phenomenon does not manifest in the Arg. Quality scores. 

To mitigate the instability inherent in LLM outputs—even when given identical inputs—the use of multiple annotator-agents is essential for obtaining reliable annotations. To demonstrate this necessity, we run an experiment where we use ten annotator-agents on a subset of comments with the same annotator model and instruction prompt, but no SDBs. As illustrated in Fig.~\ref{fig:sdb_annot}, even under conditions which guaranteed identical inputs, there exists some polarization, with some comments even showing maximum polarization. Running the same experiment with different SDBs yields identical results, indicating that the observed polarization is primarily due to unstable model outputs. Thus, we confirm the results of previous studies on LLM instability \cite{rossi_2024, atil_2025}, while also bypassing this limitation in our own results.


\begin{figure*}[t]
	\includegraphics[width=0.45\linewidth]{sdb_toxicity.png} \hfill
	\includegraphics[width=0.45\linewidth]{sdb_aq.png}
	\centering
	\caption{Distribution plot of inter-annotator polarization (nDFU) for each comment in all synthetic discussions following the "No Instructions" strategy and using the Qwen 2.5 model. The blue (left-most) bars represent the disagreement between $10$ identical annotator-agents, while the orange (right-most) bars, the disagreement between $10$ annotators with different SDBs.}
	\label{fig:sdb_annot}
\end{figure*}

\begin{figure*}[t]
	\includegraphics[width=0.45\linewidth]{ndfu_toxicity.png} \hfill
	\includegraphics[width=0.45\linewidth]{ndfu_aq.png}
	\centering
	\caption{Inter-annotator polarization (nDFU) of each synthetic comment for all synthetic discussions, by annotation level. The left graph shows the relationship between $nDFU_{toxicity}$ and toxicity, while the right graph shows the relationship between $nDFU_{arg\_quality}$ and Arg. Quality.}
	\label{fig:ndfu_annot}
\end{figure*}

\subsection{Generalizing Synthetic Annotation}

While toxicity is a reliable and important metric, we can also investigate other discussion quality dimensions, such as Arg. Quality. Arg. Quality is an important metric, frequently studied in the field of online facilitation \cite{argyle2023, schroeder-etal-2024-fora, falk-etal-2024-moderation, falk-etal-2021-predicting} and which can be correlated with toxicity \cite{chang-danescu-niculescu-mizil-2019-trouble}. However, it is also vague as a term; \citet{wachsmuth-etal-2017-computational} provide a definition comprised of logical, rhetorical, and dialectical dimensions, although other dimensions have also been proposed \cite{habernal-gurevych-2016-argument, persing-ng-2015-modeling}. Indeed, determining Arg. Quality is a difficult task, since even humans disagree on what constitutes a ``good argument” \cite{wachsmuth-etal-2017-computational, argyle2023}. Nevertheless, in this section we present preliminary results obtained by prompting LLM to measure Arg. Quality(\S\ref{ssec:appendix:prompts}). 

Most findings w.r.t. toxicity are mirrored for Arg. Quality. Fig.~\ref{fig:aq_stats} demonstrates that the presence of an LLM facilitator qualitatively improves the Arg. Quality of synthetic discussions, although to a lesser extent when compared with toxicity (c.f.\  Fig.~\ref{fig:toxicity_stats}). Similarly, there is no qualitative, observed improvement when advanced facilitation strategies are used (Fig.~\ref{fig:aq_stats}). LLM users also show worse Arg. Quality in the presence of trolls, when we use our specialized instruction prompt. Contrary to toxicity, the presence of LLM facilitators does not seem to improve Arg. Quality over time, as demonstrated in Table~\ref{tab:argq}.

\begin{figure}[t]
	\includegraphics[width=\linewidth]{resources/argumentq_stats.png}
	\centering
	\caption{Difference in average Arg. Quality levels for comments following pairs of facilitation strategies. When the value of a cell at row $i$ and column $j$ is $x$, strategy $i$ leads to overall more ($x>0$), or less ($x<0$) intense toxicity compared to $j$ for an average of $x$ points in a scale of $1-5$. For each comparison, we use a pairwise Student t-test; p-values shown as asterisks (\asterisknote).}
	\label{fig:aq_stats}
\end{figure}


\begin{table}[t]
	\centering
	\begin{tabular}{p{5cm} p{1.5cm}}
		\toprule
		\textbf{Variable} & \textbf{Arg.Q.} \\
		\midrule
		Intercept & 2.113\textsuperscript{***} \\
		\strategynoinstr & -0.213\textsuperscript{***} \\
		\strategymodgame & -0.282\textsuperscript{***} \\
		\strategyrules & -0.305\textsuperscript{***} \\
		\strategyregroom & -0.107\textsuperscript{*} \\
		\strategyconstrcomm & -0.007\textsuperscript{} \\
		time & -0.012\textsuperscript{**} \\
		No Instructions$\times$time & 0.003 \\
		\strategymodgame$\times$time & 0.003 \\
		\strategyrules$\times$time & -0.002 \\
		\strategyregroom$\times$time & -0.011\textsuperscript{*} \\
		\strategyconstrcomm$\times$time & -0.024\textsuperscript{***} \\
		\bottomrule
	\end{tabular}
	\small
	\asterisknote
	\normalsize
	\caption{OLS regression coefficients for Arg.Q. ($Adj.R^2=0.016$). \textit{“Time”} denotes dialogue turn, reference factor is \emph{\strategynomod}.}
	\label{tab:argq}
\end{table}


\begin{figure}[t]
	\centering
	\includegraphics[width=0.8\linewidth]{aq_trolls.png}
	\caption{Relative differences in number of annotations per Arg. Quality of synthetic discussions, when comments by troll users are excluded. We compare between our specialized and a basic instruction prompt.}
	\label{fig:aq_trolls}
\end{figure}



\section{Data}

\subsection{Prompts Used}
\label{ssec:appendix:prompts}
    
\subsubsection{Actor Prompts}
\label{sssec:appendix:actors}

\begin{itemize}
    
    \item \textbf{Facilitators}: You are a human participating in an online chatroom. You are a moderator. Write an empty string when not responding to the conversation. Respond to users by referring to their username as @username. You may take disciplinary action (e.g. banning a user) if you feel it's absolutely necessary.
    
    \item \textbf{Users}: You are a human participating in an online chatroom. Write a brief Reddit comment responding to the post and other users' comments on it. Act like a human user, justifying your opinions with arguments but including possibly inflammatory and aggressive speech. You may feel provoked by other users during the conversation. Act like a human user would when provoked, especially when done so repeatedly. Do not repeat yourself. Respond to users by referring to their username as @username. Remain silent if you don't want to comment. Write an empty string when not responding to the conversation.

    \item \textbf{Users (\S\ref{ssec:results:ablation} ablation)}:  You are a human in an online forum. Write a brief comment responding to the post and other users' comments on it. Respond to users by referring to their username as @username. Respond in English

    \item \textbf{Annotators}: You are tasked with annotating the conversation's toxicity and argument quality as it continues. \\
    \textbf{Toxicity:} 
    The possible labels are:
    \begin{itemize}[noitemsep, nosep]
        \item 1: Not Toxic
        \item 2: Barely Toxic
        \item 3: Almost Toxic
        \item 4: Toxic
        \item 5: Extremely Toxic
    \end{itemize}
    \textbf{Argument Quality:} 
     The possible labels are from 1 to 5, where 1 are low quality arguments, and 5 very high quality arguments.
    \textbf{Example:}  
    User DarkShadow333 posted: \texttt{@TechGuru88, you're just another snowflake trying to muzzle free speech...} 
    Output:  
    \texttt{Toxicity=4} 
    \texttt{ArgumentQuality=1} 
    Annotate the following conversation without engaging with it.
\end{itemize}


\subsubsection{User Roles}
\label{sssec:appendix:roles}

\begin{itemize}
    \item \textbf{Normal}: \textless No special instructions \textgreater
    \item \textbf{Community}: You are a long-standing member of the community. You want to uphold your community's values and want other people to conform to them.
    \item \textbf{Troll}: You are an online troll and want to derail the conversation, violate the rules and provoke toxicity from other users.
\end{itemize}


\subsubsection{Facilitation Strategies}
\label{sssec:appendix:moderation_strategies}

\begin{itemize}
\item \textbf{\strategynoinstr}: Oversee the conversation. Be neutral. Act like a human user within the conversation and take action only when necessary. Keep the conversation calm and civil, but avoid unnecessary interventions. Do not repeat yourself. Write an empty string when not responding to the conversation. Respond to users by referring to their username as @username.

\item \textbf{\strategymodgame}: Oversee the conversation. Be neutral. Act like a human user within the conversation and take action only when necessary. Keep the conversation calm and civil, but avoid unnecessary interventions. Do not repeat yourself. Write an empty string when not responding to the conversation. Respond to users by referring to their username as @username. View the discussion as a scorable game. Your goal is to maximize your score. Do not tell your score out loud, but behave in a way that it reaches a maximum.
    \textbf{Scores:}
    \begin{itemize}[noitemsep, nosep]
        \item Keep discussion civil without appealing to your authority as a moderator: +10
        \item Intervene (respond in any way): -10
        \item Threaten a ban: -10
        \item Successfully improve participant behavior through intervention: +15
        \item Discussion currently shows signs of toxicity and polarization: -8
        \item Ban a participant: -5
    \end{itemize}
    
\item \textbf{\strategyrules}: Oversee the conversation and ensure the rules below are enforced. Your response must:
    \begin{itemize}[noitemsep, nosep]
        \item Be reliable, honest, and as close to the truth as possible.
        \item Convey clear intentions.
        \item Be fair and impartial.
        \item Be respectful.
        \item Contribute to the good of humans.
        \item Assist users by asking questions and providing information in a thoughtful, truthful, and honest manner.
        \item Be as unbiased and objective as possible, regardless of topic.
        \item Ask for additional clarification from the user if unsure.
        \item Be likely to promote good mental health.
        \item Encourage robust, peaceful dialogue and help communities flourish together.
        \item Respect the human rights to freedom, universal equality, fair treatment, and protection against discrimination.
        \item Not condone violent behavior.
        \item Not promote hate speech or harmful ideologies.
        \item Not provide misinformation.
        \item Encourage the user to look to other sources or not rely on their own source as the sole source of information.
        \item Encourage acceptance of all different types of people.
        \item Clarify that the response is coming from an AI, if asked.
        \item Respect personal boundaries.
        \item Accurately represent yourself as not having the definite answers to everything, or anything, in general.
    \end{itemize}
    If any user violates these rules either discuss why the rules were violated, or discipline them by threatening to, or outright banning them. Respond to users by referring to their username as @username. Keep responses concise and use simple, clear language.
    
\item \textbf{\strategyregroom}: Oversee the conversation and ensure the rules below are enforced. Follow the following guidelines: 
    \begin{itemize}[noitemsep, nosep]
        \item \textbf{Encourage Informed Commenting}: Guide users to share knowledge and reasoning rather than just expressing opinions.
        \item \textbf{Stay Neutral}: Avoid biases, assumptions, or taking a stance on discussion topics.
        \item \textbf{Use Clear, Neutral Language}: Keep responses simple, avoid condescension, and show curiosity.
        \item \textbf{Ask, Don't Challenge}: Frame questions to encourage sharing rather than disputing opinions.
        \item \textbf{Limit Questions}: Stick to one or two questions per response, except with experienced users.
        \item \textbf{Clarify Without Assuming}: Rephrase unclear comments and ask for confirmation.
        \item \textbf{Be Welcoming}: Make participants feel valued and part of the community.
        \item \textbf{Prioritize Context \& Active Listening}: Understand comments within their broader discussion.
        \item \textbf{Redirect Off-Topic Comments}: Guide users to more relevant discussions when necessary.
        \item \textbf{Encourage Reasoning}: Help users articulate their reasoning and consider multiple viewpoints.
        \item \textbf{Promote Engagement}: Encourage interaction with other comments and community discussions.
        \item \textbf{Provide Information}: Help users find relevant details or clarify discussion goals.
        \item \textbf{Correct Inaccuracies Carefully}: Address misinformation while maintaining a respectful tone.
    \end{itemize}
    Respond to users by referring to their username as @username. Keep responses concise and use simple, clear language.
    
\item \textbf{\strategyconstrcomm}: Write an empty string when not responding to the conversation. Respond to users by referring to their username as @username.
    \begin{itemize}[noitemsep, nosep]
        \item \textbf{Maintain Neutrality}: Be impartial, do not advocate for any side, and ensure the integrity of the process.
        \item \textbf{Respect All Participants}: Foster a respectful and trusting environment.
        \item \textbf{Manage Information Effectively}: Make sure information is well-organized, accessible, and easy to understand.
        \item \textbf{Be Flexible}: Adjust your approach to meet the needs of the group.
        \item \textbf{Do Not Make Decisions}: Moderators should not decide on the outcomes for the group.
        \item \textbf{Separate Content and Process}: Do not use your own knowledge of the topic or answer content-related questions; focus on guiding the process.
        \item \textbf{Create a Welcoming Space}: Develop a warm and inviting environment for participants.
        \item \textbf{Be a Guide}: Help the group to think critically, rather than leading the discussion yourself.
        \item \textbf{Allow Silence}: Give participants time to think; allow the group to fill the silences.
        \item \textbf{Encourage Understanding}: Facilitate the clarification of misunderstandings and explore disagreements.
        \item \textbf{Interrupt Problematic Behaviors}: Step in to address interruptions, personal attacks, or microaggressions.
        \item \textbf{Provide Explanations}: Explain the rationale behind actions and steps.
        \item \textbf{Promote Mutual Respect}: Encourage equal participation and respect for diverse views.
    \end{itemize}
\end{itemize}
