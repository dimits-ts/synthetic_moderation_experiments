\section{Experimental Setup}
\label{sec:experimental}

\subsection{Moderation Strategies}
\label{ssec:experimental:strategies}

We test four different facilitation strategies,\footnote{The exact prompts used per strategy are in \S\ref{sssec:appendix:moderation_strategies}.} along with two naive ones that serve as baselines for discussion facilitation:

\begin{enumerate}[nosep, noitemsep]
    \item \textbf{No Moderator (NM)}: A \emph{baseline} where no moderator is present.

    \item \textbf{No Instructions (NI)}: A \emph{baseline} where a \ac{LLM} moderator is active, but is provided only with basic instructions (e.g., “You are a moderator, keep the discussion civil”).

     \item \textbf{Moderation Game (MGa)}: Our own proposed \emph{experimental} strategy, inspired by the experiments of \citet{abdelnabi_negotiations}. Basic instructions are formulated as a scorable, non-zero-sum game, where the moderator tries to maximize their scores by avoiding certain actions and arriving at specific outcomes (e.g., “User is toxic: $-5$ points, User corrects behavior: $+10$ points”). No actual score is being kept; the scores only exist to act as indications for how desirable an action or outcome is. The participants are not provided with scores.

    \item \textbf{Rules Only (RO)}: A \emph{real-life} strategy where the prompt is adapted from \ac{LLM} alignment guidelines \cite{collective_constitution}. This provides the moderator with a set of rules to uphold, without specifying how to uphold them (e.g, “Be fair and impartial, assist users, don't spread misinformation”).

    \item \textbf{Moderation Guidelines (MGu)}: A \emph{real-life} strategy based on guidelines given to human moderators of \ac{CeRI} \citep{Cornell_eRulemaking2017} (e.g., “Stick to a maximum of two questions, use simple and clear language, deal with off-topic comments”). %These moderators were deployed to the “Regulation Room”, an online platform designed to facilitate public engagement with U.S. government policy decisions, which has been used in online moderation literature \cite{seering_self_moderation, park_et_al_2012_facilitation}.

    \item \textbf{Facilitation Guidelines (FG)}: A \emph{real-life} strategy based on the human facilitation guidelines used by the MIT Center for Constructive Communications \cite{dimitra-book} (e.g., “Do not make decisions, be a guide, provide explanations”). %It approaches moderation from a more personalized and facilitative angle, rather than the more strict and discipline-focused guidelines of the former.
\end{enumerate}


\subsection{Turn taking}
\label{ssec:experimental:turn}

In online discussions, users don't respond to comments uniformly, instead often creating ``comment chains'' where they follow up on responses to their previous comments. To simulate this, our proposed algorithm chooses between the preceding user and another random user for each turn in the discussion:
\small
\begin{equation}
\label{eq:turn_taking}
    t(i) = \left\{
\begin{array}{ll}
    \textit{unif}(U) & i=0, i=1\\
    \textit{unif}(U/\{t(i-1)\}) & i \neq 0, p=0.6 \\
    t(i-2) & i > 1, p=0.4 
\end{array} 
\right.
\end{equation}
\normalsize

\noindent where $U$ is the set of all non-moderator users, \textit{unif} is a function sampling from the uniform distribution, and $p$ represents the probability of the option being selected. When a moderator is present, $t$ alternates between picking a normal user and the moderator.


\subsection{Prompting}
\label{ssec:experimental:prompts}

\acp{SDB} have proven promising in generating varied responses, and alleviating the Western bias exhibited by \acp{LLM} \cite{burton2024large}. We generate 30 \ac{LLM} user personas with unique \acp{SDB} using a GPT-4 model \cite{openai2024gpt4technicalreport} (\S\ref{sssec:appendix:sdbs}). We do not explicitly encode political positions in our agents' prompts, since instruction-tuned \acp{LLM} have been shown to be inherently left-leaning, and research in the field has predominantly occupied Western politics \cite{Taubenfeld2024SystematicBI, potter-etal-2024-hidden, political_2024, pit2024oninvestigatingpoliticalstance}. %In the interest of keeping our methodology generalizable, we let our \ac{LLM} agents implicitly select their own political beliefs.

Following the paradigm presented by \citet{abdelnabi_negotiations}, we assign roles to non-moderator user-agents, which inform their incentives for participating in the discussion (e.g., helping the community or disrupting discussions). Each role was mapped to specific instructions (\S\ref{sssec:appendix:roles}). We create three types of users: neutral, trolls, and community-focused users. Finally, we select a user instruction prompt (\S\ref{sssec:appendix:actors}) which balances breaking out of overly polite \ac{LLM} behavior while avoiding as much as possible injecting our own biases and expectations in synthetic interactions. 


\subsection{Discussion Quality Metrics}
\label{ssec:experimental:metrics}

We define two objectives for monitoring the discussion quality in this work: comments should not be toxic, and the arguments used should be of high quality. \ac{LLM}—based toxicity detection is reliable \citep{kang-qian-2024-implanting, Wang2022ToxicityDW, anjum2024hate}, while toxicity can inhibit online and deliberative discussions \citep{dekock2022disagree, XiaToxicity}.\footnote{We note that this is not always true \citep{Avalle2024PersistentIP}.} 
\ac{AQ} on the other hand, is an important metric, frequently studied in the field of online facilitation \cite{argyle2023, schroeder-etal-2024-fora, falk-etal-2024-moderation, falk-etal-2021-predicting} and which can be correlated with toxicity \cite{chang-danescu-niculescu-mizil-2019-trouble}. However, \ac{AQ} can be a vague term; \citet{wachsmuth-etal-2017-computational} provide a definition comprised of logical, rhetorical, and dialectical dimensions, although other dimensions have also been proposed \cite{habernal-gurevych-2016-argument, persing-ng-2015-modeling}. Indeed, determining \ac{AQ} is a difficult task, since even humans disagree on what constitutes a ``good argument” \cite{wachsmuth-etal-2017-computational, argyle2023}. 
%Details on synthetic annotation can be found in Appendix~\ref{ssec:appendix:annotation}.

\subsection{Setup}
\label{ssec:experimental:setup}

We use three open-source models from different families: LLaMa 3.2 (70B), Qwen2.5 (33B) and Mistral Nemo (12B). We select the instruction-tuned variants and quantize them to 4 bits. The original and ablation experiments were collectively completed within roughly four weeks of computational time, using two Quadro RTX 6000 GPUs. The execution script is available in the project's repository\analysislink. The automated discussion generation is detailed in \S\ref{ssec:appendix:discussion}.

