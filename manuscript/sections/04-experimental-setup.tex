\section{Configuring Interaction Dynamics}

\subsection{Model}

We use three open-source models from different families of \acp{LLM} for the \ac{LLM} user-agents and moderators (Table \ref{tab:models}). We use a well-established model (LLaMa-3.1-70b), a relatively smaller model (Qwen-2.5), as well as an abliterated model (Mistral Nemo Abliterated)\footnote{Abliteration is a technique used to modify \acp{LLM} by removing their built-in refusal mechanisms, allowing them to respond to all types of prompts without censorship.}. The intuition behind using an abliterated model is to circumvent model alignment, which occasionally prevents the \ac{LLM} user agent from responding as a human would \cite{Park2023GenerativeAI}. %We use the LLaMa-3.1-70b model exclusively for the synthetic annotation of the dataset, since it has been proven reliable for toxicity annotation \cite{koh-etal-2024-llms}. 

\begin{table}[ht]
\centering
    \begin{tabular}{|l|c|c|}
        \hline
        \textbf{Model Name} & \textbf{\#Parameters} & \textbf{Quant.} \\
        \hline
        LLaMa-3.1-70b & 70 billion & 4 bits \\
        Qwen-2.5 & 33 billion & 4 bits \\
        Mistral Nemo (abl.) & 22 billion & 16 bits\\
        \hline
    \end{tabular}
\caption{Summary of models used for synthetic discussion generation. The Mistral Nemo model is an “abliterated” variant, guiding its outputs away from alignment procedures.}
\label{tab:models}
\end{table}

\subsection{Turn taking}
We investigate two turn taking functions: The Round Robin algorithm, which gets each user to talk in a predetermined, unchanging order, and an algorithm which allows users to respond to a previous comment with a set probability (Equation \ref{eq:turn_taking}).

\small
\begin{equation}
\label{eq:turn_taking}
    u(i) = \left\{
\begin{array}{ll}
\textit{unif}(U) & i=0\\
    \textit{unif}(M) & i \text{ odd}\\
    \textit{unif}(U/\{u(i-1)\}) & i \textit{ even}, p=0.6 \\
    u(i-2) & i \textit{ even}, p=0.4 
\end{array} 
\right.
\end{equation}
\normalsize

\noindent where $U$ is the current set of participants, $M$ the set of moderators (in our case a set with one element), and $p$ represents the probability of the option being selected.


\subsection{Prompting}

We designate “roles” for each of the user-agents in the discussions. A role is a parameter which gives each user a “reason” to participate in the discussion; for example, a user may want to participate because they want to help their community, while another because they want to sabotage the discussion and rile up other users. In our experiments, each role is mapped to special instructions given to that user (see Addendum Table \ref{tab:intents}). We create three types of users: users with no special role, trolls, and community-focused users.

We also create 30 \ac{LLM} user “personas”,  each one supplied with a unique \ac{SDB}, personality. These personas were automatically generated by prompting a GPT-4 model to generate $30$ \acp{SDB} \cite{openai2024gpt4technicalreport}.
