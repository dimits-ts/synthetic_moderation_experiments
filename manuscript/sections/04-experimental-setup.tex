\section{Experimental Setup}
\label{sec:experimental}

\subsection{Synthetic Discussion Generation}
\label{ssec:appendix:discussion}

We use a set of starting comments (``seed opinions'')---in our case controversial statements from \citet{pavlopoulos-likas-2024-polarized}. We then run $N_d=8$ discussions for each pair of facilitation strategies $S$ and LLM (\S\ref{ssec:experimental:setup}). An overview of how the experiments are generated can be found in Algorithm~\ref{alg:exp_generation}.  The $RandomSample$ function returns a number of samples from a set following the uniform distribution. The $Actors$ function creates a LLM agent using a model and a prompt.

\begin{algorithm}[t]
	\caption{Synthetic discussion setup generation}
	\label{alg:exp_generation}
	\hspace*{\algorithmicindent} \textbf{Input:} 
	\begin{itemize}[noitemsep, nosep]
		\item User SDBs $\Theta = \{\theta_1, \dots, \theta_{30}\}$
		\item Strategies $S = \{s_1, \ldots, s_6\}$
		\item Seed opinions $O = \{o_1, \ldots, o_7\}$
		\item LLMs = $\{LLaMa, Mistral, Qwen\}$
	\end{itemize}
	\hspace*{\algorithmicindent} \textbf{Output:} Set of discussions $D$
	\begin{algorithmic}[1]
		\State $D = \{\}$
		\For{$llm \in LLMs$}
		\For{$s \in S$}
		\For{$i = 1, 2, \ldots, N_d$}
		\State $\hat{\Theta} = $ \Call{RandomSample}{$\Theta, num=7$}
		\State $U =$  \Call{Actors}{llm, $\hat{\Theta}$}
		\State $m = $ \Call{Actors}{llm, $s$}
		\State $o = $ \Call{RandomSample}{$O, num=1$}
		\State $d =$ \{users: $U$, mod: $m$, topic: $o$\}
		\State $D = D \cup d$
		\EndFor
		\EndFor
		\EndFor
		\State \Return $D$
	\end{algorithmic}
\end{algorithm}


\subsection{Facilitation Strategies}
\label{ssec:experimental:strategies}

We test four different facilitation strategies, three of which are derived from Social Science research, along with two common-place strategies for discussion facilitation. Note that the process of turning sometimes extensive documents into short prompts, necessitated by open-source LLMs, is necessarily imperfect. We leave the optimal derivation of strategy prompts to future work.

\begin{enumerate}[nosep, noitemsep]
	\item \textbf{\strategynomod}: A \emph{common} strategy where no facilitator is present.
	
	\item \textbf{\strategynoinstr}: A \emph{common} strategy where a LLM facilitator is present, but is provided only with basic instructions. This approach is already being used in some platforms \citep{Tsai_Deliberate_Lab_Open-Source_2025}. Example: “You are a moderator, keep the discussion civil”. 
	
	\item \textbf{\strategyrules}: A \emph{real-life} strategy where the prompt is adapted from LLM alignment guidelines \cite{collective_constitution}. These guidelines were selected to be as unanimously agreed upon as possible across various human groups. They thus provide a set of rules to uphold, without specifying \emph{how} to uphold them, leaving the LLM completely unconstrained. Example: ``Be fair and impartial, assist users, don't spread misinformation''.
	
	\item \textbf{\strategyregroom}: A \emph{real-life} strategy based on guidelines given to human facilitators of the ``Regulation Room'' platform \citep{Cornell_eRulemaking2017}. The instructions are suitable for online fora, where facilitators also engage in content moderation, and their effectiveness must be balanced by their throughput. Example: ``Stick to a maximum of two questions, use simple and clear language, deal with off-topic comments''.
	
	\item \textbf{\strategyconstrcomm}: A \emph{real-life} strategy based on the human facilitation guidelines used by the MIT Center for Constructive Communications \cite{dimitra-book}. It approaches facilitation from a more personalized and indirect angle, forbidding facilitators from directly providing opinions or directions. This makes the strategy ideal for deliberative environments. Example: ``Do not make decisions, be a guide, provide explanations''.
	
	\item \textbf{\strategymodgame}: Our proposed \emph{experimental} strategy, inspired by \citet{abdelnabi_negotiations} (see \S\ref{ssec:related:discussions}). Instructions are formulated as a game, where the facilitator LLM tries to maximize their scores by arriving at specific outcomes. No actual score is being kept; they exist to act as indications for how desirable an outcome is. The other participants are not provided with scores, nor are they aware of the game rules. Example: ``User is toxic: $-5$ points, User corrects behavior: $+10$ points''.\footnote{This could serve as a basis for a similar methodology based on game-theory, or as a Reinforcement Learning formulation for training. In this work we only explore whether the prompt itself can have an effect on the LLM facilitator; we leave the aforementioned approaches for future work.}
\end{enumerate}



\subsection{Evaluation}
\label{ssec:experimental:evaluation}

In our study, we use \emph{toxicity} as a proxy for discussion quality, since it can inhibit online and deliberative discussions \citep{dekock2022disagree, XiaToxicity}\footnote{We note that this is not always true \citep{Avalle2024PersistentIP}.}. We use ten LLM annotator-agents controlled by a model already used in prior work---LLaMa3.1 70B \citep{kang-qian-2024-implanting}---as LLMs are reliable for toxicity detection \citep{kang-qian-2024-implanting, Wang2022ToxicityDW, anjum2024hate}, which avoids problems of circular bias in our analysis. We supply each LLM annotator with a different SDB (as in \S\ref{ssec:methodology:us}).

In order to gauge the quality of our synthetic discussions, since we can not reliably measure ``realism'' (\S\ref{ssec:related:human-llm}), we use the ``diversity'' metric \citep{ulmer2024}. Low diversity points to pathological problems (e.g., LLMs repeating previous comments). On the other hand, extremely high diversity may point to a lack of interaction between participants; a discussion in which participants engage with each other will feature some lexical overlap (e.g., common terms, paraphrasing points of other participants). We compare the distribution of diversity scores for synthetic discussions with that measured on sampled human discussions. This allows us to estimate the extent to which synthetic discussions approximate real-world content variety and participant interaction. 

We note again that these metrics are better interpreted as heuristics of actual discussion and synthetic data quality respectively. More research is needed w.r.t. reliable and generalizable quality metrics.


\subsection{Technical Details}
\label{ssec:experimental:setup}

We use three instruction-tuned, open-source models: LLaMa3.1 (70B), Qwen2.5 (33B),  Mistral Nemo (12B), quantized to 4 bits and run using a set seed (42). All the experiments were collectively completed within four weeks of computational time, using two Quadro RTX 6000 GPUs. The execution script is available in the project's repository.\analysislink 