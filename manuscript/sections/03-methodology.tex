% !TEX root = ../main.tex
%

\section{Methodology}
\label{sec:methodology}

\subsection{Defining Synthetic Discussions}

Let $U$ be the set of users participating in discussions and $M$ the set of moderators, where $M \cap U = \emptyset$. We define a conversation $d$ of $\lvert d \rvert$ comments\footnote{Also referred to as “dialogue turns” in some publications.} $c(d, i)$ as an ordered set:
\begin{equation}
    d = \{c(d, 1), c(d, 2), \ldots, c(d, \lvert d \rvert)\}
\end{equation}


Next, we define a turn-taking function $u: D  \times \mathbb{N} \rightarrow U \cup M$ mapping a comment in the $i$-th turn of a discussion $d \in D$ to an arbitrary user in $U$, or moderator in $M$. In real discussions, $u$ is not strictly defined, since which user responds to each comment can not be reliably determined. However, in a synthetic environment $u$ can be made deterministic (see the function formulation at Equation \ref{eq:turn_taking} in Section \ref{ssec:experimental:discussions} for an example).

In our case, all comments are synthetic, hence, a comment $c$ in a discussion $d \in D$, at the $i$-th turn is defined recursively as:

\begin{equation}
\begin{split}
    c(d, i) = & LLM([c(d, j)]^{i-1}_{j=max(1, i-h)};\\
    &\phi(u(d, i)))
\end{split}
\end{equation}

\noindent where $[\cdot]$ is string concatenation and $h$ is the context length of the \ac{LLM} user-agent (how many past comments they can “remember”) and $\phi: U \times M \rightarrow s$ is a function mapping a user $u$ to their instruction prompt $s$.

Our methodology thus assumes that the contents of any synthetic discussion are dependent on the following parameters:
\begin{itemize}[noitemsep]
    \item The underlying model ($LLM(\cdot)$)
    \item The turn-taking function $u$
    \item The prompting function $\phi$
\end{itemize}


\subsection{Gauging Diversity}
\label{ssec:methodology:evaluating}

One simple metric that distinguishes between LLM generated and human texts, is the one presented in \citet{ulmer2024}. In their work, the authors use average pairwise F1 ROUGE-L scores \cite{lin-2004-rouge} to gauge diversity in synthetic discussions. This is built on the observation that a common shortcoming of completely synthetic discussions is word repetition; either because the discussion does not meaningfully advance, or because the discussion collapses (and the models keep repeating the same words/phrases). The score is computed by computing the individual F1 ROUGE-L scores for each comment in the discussion with all the rest, then averaging them. A larger ROUGE-L score indicates less variance in the discussion. The equation itself can be written as:

\small
\begin{equation}
\label{eq:variety}
    div(d) = \frac{1}{N(N-1)} \sum_{i=1}^N \sum_{j=1, j \neq i}^N F1(c(i, d), c(j, d))
\end{equation}
\normalsize