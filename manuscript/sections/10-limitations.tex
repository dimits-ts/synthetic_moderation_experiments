% !TEX root = ../main.tex
%
\section{Limitations} 
\label{sec:limitations}

%Because synthetic data generation with \acp{LLM} is a relatively new area of research, the literature review in this paper is partially based on relevant unpublished work (preprints). These sources are considered when appropriate, as they offer important insights for the interpretation and inherent limitations of our results.

While we investigate the impact of moderation strategies in synthetic discussions, we cannot claim that the behavior of \ac{LLM} users and facilitator-agents is representative of human behavior. This claim can be scarcely made in Social Science studies involving \ac{LLM} subjects \cite{rossi_2024, zhou-etal-2024-real}â€”as discussed in \S\ref{ssec:related:human-llm}.

Furthermore, our experimental setup makes several assumptions that may affect the generalizability of our findings. We examine only three \acp{LLM}, assume a maximum of one facilitator per discussion, and use a turn-taking algorithm that overlooks contextual factors like relevance and emotional engagement, which are crucial in human interactions. Additionally,we do not account for the fact that humans may behave differently when knowing they are interacting with \acp{LLM} instead of humans. Our methodology also does not take into account interactions where the user-agents and moderator-agents are based on different LLMs (cf. Eq~\ref{eq:comment}). Finally, our analysis partly relies on \ac{LLM}-generated annotations, potentially introducing known biases associated with \ac{LLM} annotation (\S\ref{ssec:appendix:annotation}).