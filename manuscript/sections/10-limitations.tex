% !TEX root = ../main.tex
%
\section{Limitations} 
\label{sec:limitations}

Because synthetic data generation with \acp{LLM} is a relatively new area of research, the literature review in this paper is partially based on relevant unpublished work (preprints). These sources are considered when appropriate, as they offer important insights for the interpretation and inherent limitations of our results.

While we investigate the impact of moderation strategies in synthetic discussions, we can not make the claim that the behavior of \ac{LLM} user-agents is representative of human behavior. This claim can be scarcely made in Social Science studies involving \ac{LLM} subjects \cite{rossi_2024, zhou-etal-2024-real}â€”as discussed in Section~\ref{ssec:related:human-llm}.

Furthermore, our experimental setup makes several assumptions that may affect the generalizability of our findings. We examine only three \acp{LLM}, assume a maximum of one moderator, and use a turn-taking algorithm that overlooks contextual factors like relevance and emotional engagement, which are crucial in human interactions. Additionally, we do not account for participants' meta-knowledge, as users may behave differently when knowing they are interacting with \acp{LLM} instead of humans. Our methodology also does not take into account interactions where the user-agents and moderator-agents are controlled by different models. Finally, our analysis partly relies on \ac{LLM}-generated annotations, potentially introducing known biases associated with \ac{LLM} annotation (\S\ref{ssec:appendix:annotation}).