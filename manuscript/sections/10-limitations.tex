% !TEX root = ../main.tex
%
\section{Limitations} 
\label{sec:limitations}

Due to limited research in the area, our analysis only uses one synthetic discussion quality metric to gauge data quality. Additionally, while we investigate the impact of facilitation strategies in synthetic discussions, we cannot claim that the behavior of \ac{LLM} user and facilitator-agents is representative of human behavior. This claim can be scarcely made in Social Science studies involving \ac{LLM} subjects \cite{rossi_2024, zhou-etal-2024-real}â€”as discussed in \S\ref{ssec:related:human-llm}.

Furthermore, our experimental setup makes several assumptions that may affect the generalizability of our findings. We examine only three \acp{LLM}, assume a maximum of one facilitator per discussion, and use a turn-taking algorithm that overlooks contextual factors like relevance and emotional engagement, which are important in human interactions \cite{robert_2016_comment, Ziegele03102018}. Moreover, due to resource constraints, we are unable to research multiple instruction prompts besides facilitation strategies, or use lengthy prompts which would necessitate large context windows. 

Our methodology also does not account for the fact that humans may behave differently when knowing they are interacting with \acp{LLM} instead of humans, nor  does it account for interactions where the user and facilitator-agents are based on different \acp{LLM} (cf. Eq~\ref{eq:comment}). Finally, our analysis partly relies on \ac{LLM}-generated annotations, potentially introducing known biases associated with \ac{LLM} annotation (\S\ref{ssec:appendix:annotation}).