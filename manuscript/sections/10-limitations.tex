% !TEX root = ../main.tex
%
\section{Limitations} 
\label{sec:limitations}

Due to limited research in the area, our analysis uses only two quality metrics to gauge discussion quality: diversity and toxicity. Additionally, while we investigate the impact of facilitation strategies in synthetic discussions, we cannot claim that the behavior of \ac{LLM} user- and facilitator-agents is representative of human behavior. This claim can be scarcely made in Social Science studies involving \ac{LLM} subjects \cite{rossi_2024, zhou-etal-2024-real}, as discussed in \S\ref{ssec:related:human-llm}.

Furthermore, our experimental setup makes several assumptions that may affect the generalizability of our findings. We examine only three \acp{LLM}, assume a maximum of one facilitator per discussion, and use a turn-taking algorithm that overlooks contextual factors like relevance and emotional engagement, which are important in human interactions \cite{robert_2016_comment, Ziegele03102018}. Moreover, due to resource constraints, we were unable to experiment with more elaborate instruction prompts, due to the need for large context windows. 

Our methodology also does not account for the fact that humans may behave differently when knowing they are interacting with \acp{LLM} instead of humans, nor  does it account for interactions where the user and facilitator-agents are based on different \acp{LLM} (cf. Eq~\ref{eq:comment}). Finally, our analysis partly relies on \ac{LLM}-generated annotations of toxicity, potentially introducing known biases associated with \ac{LLM} annotation (\S\ref{ssec:appendix:annotation}).