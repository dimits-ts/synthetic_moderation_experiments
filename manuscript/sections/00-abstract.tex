% !TEX root = ../main.tex
%

\begin{abstract}
	Limited large-scale evaluations exist for facilitation strategies of online discussions due to significant costs associated with human involvement. An effective solution is synthetic discussion simulations using \acp{LLM} to create initial pilot experiments. We propose a simple, generalizable, \ac{LLM}-driven methodology to prototype the development of \ac{LLM} facilitators, and produce high-quality synthetic data without human involvement. We use our methodology to test whether current Social Science strategies for facilitation can improve the performance of \ac{LLM} facilitators. We find that, while \ac{LLM} facilitators significantly improve synthetic discussions, there is no evidence that the application of these strategies leads to further improvements in discussion quality. We confirm that each component of our methodology contributes substantially to high quality data via an ablation study. In an effort to aid research in the field of facilitation, we release \vmd a large, publicly available dataset containing \ac{LLM}-generated and \ac{LLM}-annotated discussions using multiple open-source \acp{LLM}, which can be used for \ac{LLM} facilitator finetuning as well as behavioral analysis of current out-of-the-box \acp{LLM} in the task.\datasetlink. We  also release an open-source framework \syndisco\syndiscolink\pip, which implements our methodology at great scale.
\end{abstract}