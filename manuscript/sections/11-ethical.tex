% !TEX root = ../main.tex
%
\section{Ethical Considerations}
\label{sec:ethical}

The software and methodology presented raises significant ethical concerns, as synthetic discussions involving \acp{LLM} could be exploited by malicious actors to make \ac{LLM} user-agents more capable at performing unethical tasks. Such actors could trivially adapt our methodology to  maximize toxicity, disrupt human discussions, or learn to circumvent moderation mechanisms to propagate misinformation or spread specific agendas. 

Additionally, we note that researchers considering the deployment of their now-configured \ac{LLM} moderators in existing online communities must do so transparently and with the explicit consent of the community. Embedding \ac{LLM} agents without disclosure can erode trust, be perceived as manipulative \cite{retraction_watch}, as well as potentially violating regulatory frameworks such as the EU AI Act \cite{eu_ai_act_2024}.

Finally, we feel the need to reiterate that while \acp{LLM} can seem to approximate human behavior, they cannot reliably replicate it. Therefore, this research should primarily serve to create pilot experiments, followed by rigorous human-subject studies to ensure the reliability and validity of findings. Researchers should avoid making conclusions and interpretations on human behavior based on synthetic data.