@article{small-polis-llm,
	title={Opportunities and Risks of LLMs for Scalable Deliberation with Polis},
	author={Christopher T. Small and Ivan Vendrov and Esin Durmus and Hadjar Homaei and Elizabeth Barry and Julien Cornebise and Ted Suzman and Deep Ganguli and Colin Megill},
	journal={ArXiv},
	year={2023},
	volume={abs/2306.11932},
	url={https://api.semanticscholar.org/CorpusID:259211996}
}

@article{small2021polis,
	title={Polis: Scaling deliberation by mapping high dimensional opinion spaces},
	author={Small, Christopher and Bjorkegren, Michael and Erkkil{\"a}, Timo and Shaw, Lynette and Megill, Colin},
	journal={Recerca: revista de pensament i an{\`a}lisi},
	volume={26},
	number={2},
	year={2021},
	publisher={Universitat Jaume I Servei de Comunicacio i Publicacions}
}

@inproceedings{zhang2016-oxford,
	author = {Zhang, Justine and Kumar, Ravi and Ravi, Sujith and Danescu-Niculescu-Mizil, Cristian},
	year = {2016},
	month = {04},
	pages = {136-141},
	title = {Conversational Flow in Oxford-style Debates},
	doi = {10.18653/v1/N16-1017}
}

@inproceedings{vecchi-2021-towards,
	title = "Towards Argument Mining for Social Good: A Survey",
	author = "Vecchi, Eva Maria  and
	Falk, Neele  and
	Jundi, Iman  and
	Lapesa, Gabriella",
	editor = "Zong, Chengqing  and
	Xia, Fei  and
	Li, Wenjie  and
	Navigli, Roberto",
	booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
	month = aug,
	year = "2021",
	address = "Online",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2021.acl-long.107",
	doi = "10.18653/v1/2021.acl-long.107",
	pages = "1338--1352",
	abstract = "This survey builds an interdisciplinary picture of Argument Mining (AM), with a strong focus on its potential to address issues related to Social and Political Science. More specifically, we focus on AM challenges related to its applications to social media and in the multilingual domain, and then proceed to the widely debated notion of argument quality. We propose a novel definition of argument quality which is integrated with that of deliberative quality from the Social Science literature. Under our definition, the quality of a contribution needs to be assessed at multiple levels: the contribution itself, its preceding context, and the consequential effect on the development of the upcoming discourse. The latter has not received the deserved attention within the community. We finally define an application of AM for Social Good: (semi-)automatic moderation, a highly integrative application which (a) represents a challenging testbed for the integrated notion of quality we advocate, (b) allows the empirical quantification of argument/deliberative quality to benefit from the developments in other NLP fields (i.e. hate speech detection, fact checking, debiasing), and (c) has a clearly beneficial potential at the level of its societal thanks to its real-world application (even if extremely ambitious).",
}

@article{stab-gurevych-2017-parsing,
	title = "Parsing Argumentation Structures in Persuasive Essays",
	author = "Stab, Christian  and
	Gurevych, Iryna",
	journal = "Computational Linguistics",
	volume = "43",
	number = "3",
	month = sep,
	year = "2017",
	address = "Cambridge, MA",
	publisher = "MIT Press",
	url = "https://aclanthology.org/J17-3005",
	doi = "10.1162/COLI_a_00295",
	pages = "619--659",
	abstract = "In this article, we present a novel approach for parsing argumentation structures. We identify argument components using sequence labeling at the token level and apply a new joint model for detecting argumentation structures. The proposed model globally optimizes argument component types and argumentative relations using Integer Linear Programming. We show that our model significantly outperforms challenging heuristic baselines on two different types of discourse. Moreover, we introduce a novel corpus of persuasive essays annotated with argumentation structures. We show that our annotation scheme and annotation guidelines successfully guide human annotators to substantial agreement.",
}

@book{Steiner2005-STEDPI-8,
	address = {Cambridge},
	author = {J\"{u}rg Steiner and Andr\'e B\"{a}chtiger and Markus Sp\"{o}rndli and Marco R. Steenbergen},
	editor = {},
	publisher = {Cambridge University Press},
	title = {Deliberative Politics in Action. Analysing Parliamentary Discourse},
	year = {2005}
}

@inproceedings{wachsmuth-etal-2017-computational,
	title = "Computational Argumentation Quality Assessment in Natural Language",
	author = "Wachsmuth, Henning  and
	Naderi, Nona  and
	Hou, Yufang  and
	Bilu, Yonatan  and
	Prabhakaran, Vinodkumar  and
	Thijm, Tim Alberdingk  and
	Hirst, Graeme  and
	Stein, Benno",
	editor = "Lapata, Mirella  and
	Blunsom, Phil  and
	Koller, Alexander",
	booktitle = "Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 1, Long Papers",
	month = apr,
	year = "2017",
	address = "Valencia, Spain",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/E17-1017",
	pages = "176--187",
	abstract = "Research on computational argumentation faces the problem of how to automatically assess the quality of an argument or argumentation. While different quality dimensions have been approached in natural language processing, a common understanding of argumentation quality is still missing. This paper presents the first holistic work on computational argumentation quality in natural language. We comprehensively survey the diverse existing theories and approaches to assess logical, rhetorical, and dialectical quality dimensions, and we derive a systematic taxonomy from these. In addition, we provide a corpus with 320 arguments, annotated for all 15 dimensions in the taxonomy. Our results establish a common ground for research on computational argumentation quality assessment.",
}

@inproceedings{de-kock-vlachos-2021-beg,
	title = "{I} Beg to Differ: A study of constructive disagreement in online conversations",
	author = "De Kock, Christine  and
	Vlachos, Andreas",
	editor = "Merlo, Paola  and
	Tiedemann, Jorg  and
	Tsarfaty, Reut",
	booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
	month = apr,
	year = "2021",
	address = "Online",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2021.eacl-main.173",
	doi = "10.18653/v1/2021.eacl-main.173",
	pages = "2017--2027",
	abstract = "Disagreements are pervasive in human communication. In this paper we investigate what makes disagreement constructive. To this end, we construct WikiDisputes, a corpus of 7425 Wikipedia Talk page conversations that contain content disputes, and define the task of predicting whether disagreements will be escalated to mediation by a moderator. We evaluate feature-based models with linguistic markers from previous work, and demonstrate that their performance is improved by using features that capture changes in linguistic markers throughout the conversations, as opposed to averaged values. We develop a variety of neural models and show that taking into account the structure of the conversation improves predictive accuracy, exceeding that of feature-based models. We assess our best neural model in terms of both predictive accuracy and uncertainty by evaluating its behaviour when it is only exposed to the beginning of the conversation, finding that model accuracy improves and uncertainty reduces as models are exposed to more information.",
}

@online{graham2008disagree,
	author = {Paul Graham},
	title = {How to Disagree},
	year = {2008},
	month = {3},
	url = {https://paulgraham.com/disagree.html},
	note = {Accessed: 2024-06-24}
}

@inproceedings{dekock2022disagree,
	title = "How to disagree well: Investigating the dispute tactics used on {W}ikipedia",
	author = "De Kock, Christine  and
	Stafford, Tom  and
	Vlachos, Andreas",
	editor = "Goldberg, Yoav  and
	Kozareva, Zornitsa  and
	Zhang, Yue",
	booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
	month = dec,
	year = "2022",
	address = "Abu Dhabi, United Arab Emirates",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2022.emnlp-main.252",
	doi = "10.18653/v1/2022.emnlp-main.252",
	pages = "3824--3837",
	abstract = "Disagreements are frequently studied from the perspective of either detecting toxicity or analysing argument structure. We propose a framework of dispute tactics which unifies these two perspectives, as well as other dialogue acts which play a role in resolving disputes, such as asking questions and providing clarification. This framework includes a preferential ordering among rebuttal-type tactics, ranging from ad hominem attacks to refuting the central argument. Using this framework, we annotate 213 disagreements (3,865 utterances) from Wikipedia Talk pages. This allows us to investigate research questions around the tactics used in disagreements; for instance, we provide empirical validation of the approach to disagreement recommended by Wikipedia. We develop models for multilabel prediction of dispute tactics in an utterance, achieving the best performance with a transformer-based label powerset model. Adding an auxiliary task to incorporate the ordering of rebuttal tactics further yields a statistically significant increase. Finally, we show that these annotations can be used to provide useful additional signals to improve performance on the task of predicting escalation.",
}

@inproceedings{walker-etal-2012-corpus,
	title = "A Corpus for Research on Deliberation and Debate",
	author = "Walker, Marilyn  and
	Tree, Jean Fox  and
	Anand, Pranav  and
	Abbott, Rob  and
	King, Joseph",
	editor = "Calzolari, Nicoletta  and
	Choukri, Khalid  and
	Declerck, Thierry  and
	Do{\u{g}}an, Mehmet U{\u{g}}ur  and
	Maegaard, Bente  and
	Mariani, Joseph  and
	Moreno, Asuncion  and
	Odijk, Jan  and
	Piperidis, Stelios",
	booktitle = "Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12)",
	month = may,
	year = "2012",
	address = "Istanbul, Turkey",
	publisher = "European Language Resources Association (ELRA)",
	url = "http://www.lrec-conf.org/proceedings/lrec2012/pdf/1078_Paper.pdf",
	pages = "812--817",
	abstract = "Deliberative, argumentative discourse is an important component of opinion formation, belief revision, and knowledge discovery; it is a cornerstone of modern civil society. Argumentation is productively studied in branches ranging from theoretical artificial intelligence to political rhetoric, but empirical analysis has suffered from a lack of freely available, unscripted argumentative dialogs. This paper presents the Internet Argument Corpus (IAC), a set of 390,704 posts in 11,800 discussions extracted from the online debate site 4forums.com. A 2866 thread/130,206 post extract of the corpus has been manually sided for topic of discussion, and subsets of this topic-labeled extract have been annotated for several dialogic and argumentative markers: degrees of agreement with a previous post, cordiality, audience-direction, combativeness, assertiveness, emotionality of argumentation, and sarcasm. As an application of this resource, the paper closes with a discussion of the relationship between discourse marker pragmatics, agreement, emotionality, and sarcasm in the IAC corpus.",
}

@online{benesch2016counterspeech,
	title={Counterspeech on twitter: A field study. Dangerous Speech Project},
	author={Benesch, Susan and Ruths, Derek and Dillon, Kelly P and Saleem, Haji Mohammad and Wright, Lucas},
	year={2016}
}

@article{karadzhov2023delidata,
	title={DeliData: A Dataset for Deliberation in Multi-party Problem Solving},
	author={Georgi Karadzhov and Tom Stafford and Andreas Vlachos},
	journal={Proceedings of the ACM on Human-Computer Interaction},
	year={2021},
	volume={7},
	pages={1 - 25},
	url={https://api.semanticscholar.org/CorpusID:236975941}
}


@article{stefan-dissent,
	title = "Group decision making in hidden profile situations: Dissent as a facilitator for decision quality",
	abstract = "The effect of diversity in individual prediscussion preferences on group decision quality was examined in an experiment in which 135 three-person groups worked on a personnel selection case with 4 alternatives. The information distribution among group members constituted a hidden profile (i.e., the correct solution was not identifiable on the basis of the members' individual information and could be detected only by pooling and integrating the members' unique information). Whereas groups with homogeneous suboptimal prediscussion preferences (no dissent) hardly ever solved the hidden profile, solution rates were significantly higher in groups with prediscussion dissent, even if none of these individual prediscussion preferences were correct. If dissent came from a proponent of the correct solution, solution rates were even higher than in dissent groups without such a proponent. The magnitude of dissent (i.e., minority dissent or full diversity of individual preferences) did not affect decision quality. The beneficial effect of dissent on group decision quality was mediated primarily by greater discussion intensity and to some extent also by less discussion bias in dissent groups. (PsycINFO Database Record (c) 2006 APA, all rights reserved).",
	keywords = "Dissent, Group decision making, Hidden profile, Information pooling, Minority influence",
	author = "Stefan Schulz-Hardt and Brodbeck, {Felix C.} and Andreas Mojzisch and Rudolf Kerschreiter and Dieter Frey",
	year = "2006",
	month = dec,
	day = "1",
	doi = "10.1037/0022-3514.91.6.1080",
	language = "English",
	volume = "91",
	pages = "1080--1093",
	journal = "Journal of Personality and Social Psychology",
	issn = "0022-3514",
	publisher = "American Psychological Association Inc.",
	number = "6",
	
}

@article{david-collaborative,
	author = {David Moshman and Molly Geil},
	title = {Collaborative Reasoning: Evidence for Collective Rationality},
	journal = {Thinking \& Reasoning},
	volume = {4},
	number = {3},
	pages = {231--248},
	year = {1998},
	publisher = {Routledge},
	doi = {10.1080/135467898394148},
	URL = {https://doi.org/10.1080/135467898394148},
	eprint = {https://doi.org/10.1080/135467898394148},
	abstract = { Reasoning may be defined as a deliberate effort to coordinate inferences so as to reach justifiable conclusions. Thus defined, reasoning includes collaborative as well as individual forms of cognitive action. The purpose of the present study was to demonstrate a circumstance in which collaborative reasoning is qualitatively superior to individual reasoning. The selection task, a well known logical hypothesis-testing problem, was presented to 143 college undergraduates—32 individuals and 20 groups of 5 or 6 interacting peers. The correct (falsification) response pattern was selected by only 9\% of the individuals but by 75\% of the groups. The superior performance of the groups was due to collaborative reasoning rather than to imitation or peer pressure. Groups typically co-constructed a structure of arguments qualitatively more sophisticated than that generated by most individuals. The results support Piagetian and Habermasian views of peer interaction as a locus of rational social processes. }
}

@misc{shumailov2024curserecursiontraininggenerated,
	title={The Curse of Recursion: Training on Generated Data Makes Models Forget}, 
	author={Ilia Shumailov and Zakhar Shumaylov and Yiren Zhao and Yarin Gal and Nicolas Papernot and Ross Anderson},
	year={2024},
	eprint={2305.17493},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	url={https://arxiv.org/abs/2305.17493}, 
}

@article{alemohammad2023selfconsuminggenerativemodelsmad,
	title={Self-Consuming Generative Models Go MAD}, 
	author={Sina Alemohammad and Josue Casco-Rodriguez and Lorenzo Luzi and Ahmed Imtiaz Humayun and Hossein Babaei and Daniel LeJeune and Ali Siahkoohi and Richard G. Baraniuk},
	year={2023},
	eprint={2307.01850},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	url={https://arxiv.org/abs/2307.01850}, 
}

@article{ulmer2024,
	title={Bootstrapping LLM-based Task-Oriented Dialogue Agents via Self-Talk},
	author={Dennis Ulmer and Elman Mansimov and Kaixiang Lin and Justin Sun and Xibin Gao and Yi Zhang},
	journal={ArXiv},
	year={2024},
	volume={abs/2401.05033},
	url={https://api.semanticscholar.org/CorpusID:266902624}
}

@inproceedings{lin-2004-rouge,
	title = "{ROUGE}: A Package for Automatic Evaluation of Summaries",
	author = "Lin, Chin-Yew",
	booktitle = "Text Summarization Branches Out",
	month = jul,
	year = "2004",
	address = "Barcelona, Spain",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/W04-1013",
	pages = "74--81",
}

@inproceedings{urbanek-etal-2019-learning,
	title = "Learning to Speak and Act in a Fantasy Text Adventure Game",
	author = {Urbanek, Jack  and
	Fan, Angela  and
	Karamcheti, Siddharth  and
	Jain, Saachi  and
	Humeau, Samuel  and
	Dinan, Emily  and
	Rockt{\"a}schel, Tim  and
	Kiela, Douwe  and
	Szlam, Arthur  and
	Weston, Jason},
	editor = "Inui, Kentaro  and
	Jiang, Jing  and
	Ng, Vincent  and
	Wan, Xiaojun",
	booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
	month = nov,
	year = "2019",
	address = "Hong Kong, China",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/D19-1062",
	doi = "10.18653/v1/D19-1062",
	pages = "673--683",
	abstract = "We introduce a large-scale crowdsourced text adventure game as a research platform for studying grounded dialogue. In it, agents can perceive, emote, and act whilst conducting dialogue with other agents. Models and humans can both act as characters within the game. We describe the results of training state-of-the-art generative and retrieval models in this setting. We show that in addition to using past dialogue, these models are able to effectively use the state of the underlying world to condition their predictions. In particular, we show that grounding on the details of the local environment, including location descriptions, and the objects (and their affordances) and characters (and their previous actions) present within it allows better predictions of agent behavior and dialogue. We analyze the ingredients necessary for successful grounding in this setting, and how each of these factors relate to agents that can talk and act successfully.",
}

@article{he2023debertav3improvingdebertausing,
	title={DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing}, 
	author={Pengcheng He and Jianfeng Gao and Weizhu Chen},
	year={2023},
	eprint={2111.09543},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2111.09543}, 
}

@article{silver2017masteringchessshogiselfplay,
	title={Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm}, 
	author={David Silver and Thomas Hubert and Julian Schrittwieser and Ioannis Antonoglou and Matthew Lai and Arthur Guez and Marc Lanctot and Laurent Sifre and Dharshan Kumaran and Thore Graepel and Timothy Lillicrap and Karen Simonyan and Demis Hassabis},
	year={2017},
	eprint={1712.01815},
	archivePrefix={arXiv},
	primaryClass={cs.AI},
	url={https://arxiv.org/abs/1712.01815}, 
}

@article{MosaicML2023,
	author       = {The MosaicML NLP Team},
	title        = {Introducing MPT-7B: A New Standard for Open-Source, Commercially Usable LLMs},
	journal      = {Mosaic AI Research},
	year         = {2023},
	month        = {5},
	day          = {5},
	url          = {https://www.databricks.com/blog/mpt-7b}
}

@inproceedings{hua2018wikiconvcorpuscompleteconversational,
	title = "{W}iki{C}onv: A Corpus of the Complete Conversational History of a Large Online Collaborative Community",
	author = "Hua, Yiqing  and
	Danescu-Niculescu-Mizil, Cristian  and
	Taraborelli, Dario  and
	Thain, Nithum  and
	Sorensen, Jeffery  and
	Dixon, Lucas",
	editor = "Riloff, Ellen  and
	Chiang, David  and
	Hockenmaier, Julia  and
	Tsujii, Jun{'}ichi",
	booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
	month = {10},
	year = "2018",
	address = "Brussels, Belgium",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/D18-1305",
	doi = "10.18653/v1/D18-1305",
	pages = "2818--2823",
	abstract = "We present a corpus that encompasses the complete history of conversations between contributors to Wikipedia, one of the largest online collaborative communities. By recording the intermediate states of conversations - including not only comments and replies, but also their modifications, deletions and restorations - this data offers an unprecedented view of online conversation. Our framework is designed to be language agnostic, and we show that it extracts high quality data in both Chinese and English. This level of detail supports new research questions pertaining to the process (and challenges) of large-scale online collaboration. We illustrate the corpus{'} potential with two case studies on English Wikipedia that highlight new perspectives on earlier work. First, we explore how a person{'}s conversational behavior depends on how they relate to the discussion{'}s venue. Second, we show that community moderation of toxic behavior happens at a higher rate than previously estimated.",
}


@inproceedings{beck-etal-2024-sensitivity,
	title = "Sensitivity, Performance, Robustness: Deconstructing the Effect of Sociodemographic Prompting",
	author = "Beck, Tilman  and
	Schuff, Hendrik  and
	Lauscher, Anne  and
	Gurevych, Iryna",
	editor = "Graham, Yvette  and
	Purver, Matthew",
	booktitle = "Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers)",
	month = mar,
	year = "2024",
	address = "St. Julian{'}s, Malta",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.eacl-long.159",
	pages = "2589--2615",
	abstract = "Annotators{'} sociodemographic backgrounds (i.e., the individual compositions of their gender, age, educational background, etc.) have a strong impact on their decisions when working on subjective NLP tasks, such as toxic language detection. Often, heterogeneous backgrounds result in high disagreements. To model this variation, recent work has explored sociodemographic prompting, a technique, which steers the output of prompt-based models towards answers that humans with specific sociodemographic profiles would give. However, the available NLP literature disagrees on the efficacy of this technique {---} it remains unclear for which tasks and scenarios it can help, and the role of the individual factors in sociodemographic prompting is still unexplored. We address this research gap by presenting the largest and most comprehensive study of sociodemographic prompting today. We use it to analyze its influence on model sensitivity, performance and robustness across seven datasets and six instruction-tuned model families. We show that sociodemographic information affects model predictions and can be beneficial for improving zero-shot learning in subjective NLP tasks.However, its outcomes largely vary for different model types, sizes, and datasets, and are subject to large variance with regards to prompt formulations. Most importantly, our results show that sociodemographic prompting should be used with care when used for data annotation or studying LLM alignment.",
}

@article{durmus2024measuringrepresentationsubjectiveglobal,
	title={Towards Measuring the Representation of Subjective Global Opinions in Language Models}, 
	author={Esin Durmus and Karina Nguyen and Thomas I. Liao and Nicholas Schiefer and Amanda Askell and Anton Bakhtin and Carol Chen and Zac Hatfield-Dodds and Danny Hernandez and Nicholas Joseph and Liane Lovitt and Sam McCandlish and Orowa Sikder and Alex Tamkin and Janel Thamkul and Jared Kaplan and Jack Clark and Deep Ganguli},
	year={2024},
	eprint={2306.16388},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2306.16388}, 
}

@inproceedings{hwang-etal-2023-aligning,
	title = "Aligning Language Models to User Opinions",
	author = "Hwang, EunJeong  and
	Majumder, Bodhisattwa  and
	Tandon, Niket",
	editor = "Bouamor, Houda  and
	Pino, Juan  and
	Bali, Kalika",
	booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
	month = dec,
	year = "2023",
	address = "Singapore",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2023.findings-emnlp.393",
	doi = "10.18653/v1/2023.findings-emnlp.393",
	pages = "5906--5919",
	abstract = "An important aspect of developing LLMs that interact with humans is to align models{'} behavior to their users. It is possible to prompt an LLM into behaving as a certain persona, especially a user group or ideological persona the model captured during its pertaining stage. But, how to best align an LLM with a specific user and not a demographic or ideological group remains an open question. Mining public opinion surveys (by PEW research), we find that the opinions of a user and their demographics and ideologies are not mutual predictors. We use this insight to align LLMs by modeling relevant past user opinions in addition to user demographics and ideology, achieving up to 7 points accuracy gains in predicting public opinions from survey questions across a broad set of topics. Our work opens up the research avenues to bring user opinions as an important ingredient in aligning language models.",
}

@inproceedings{deshpande-etal-2023-toxicity,
	title = "Toxicity in chatgpt: Analyzing persona-assigned language models",
	author = "Deshpande, Ameet  and
	Murahari, Vishvak  and
	Rajpurohit, Tanmay  and
	Kalyan, Ashwin  and
	Narasimhan, Karthik",
	editor = "Bouamor, Houda  and
	Pino, Juan  and
	Bali, Kalika",
	booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
	month = dec,
	year = "2023",
	address = "Singapore",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2023.findings-emnlp.88",
	doi = "10.18653/v1/2023.findings-emnlp.88",
	pages = "1236--1270",
	abstract = "Large language models (LLMs) have shown incredible capabilities and transcended the natural language processing (NLP) community, with adoption throughout many services like healthcare, therapy, education, and customer service. Since users include people with critical information needs like students or patients engaging with chatbots, the safety of these systems is of prime importance. Legislation has recognized its significance and recently drafted a {``}Blueprint For An AI Bill Of Rights{''} which calls for domain experts to identify risks and potential impact of AI systems. To this end, we systematically evaluate toxicity in over half a million generations of ChatGPT, a popular dialogue-based LLM. We find that setting the system parameter of ChatGPT by assigning it a persona, say that of the boxer Muhammad Ali, significantly increases the toxicity of generations. Depending on the persona assigned to ChatGPT, its toxicity can increase up to $6\times$, with outputs engaging in incorrect stereotypes, harmful dialogue, and hurtful opinions. Furthermore, we find concerning patterns where specific entities (e.g., certain races) are targeted more than others ($3\times$ more) irrespective of the assigned persona, reflecting discriminatory biases in the model. Our findings show that multiple provisions in the legislative blueprint are being violated, and we hope that the broader AI community rethinks the efficacy of current safety guardrails and develops better techniques that lead to robust, safe, and trustworthy AI.",
}

@inproceedings{cheng-etal-2023-marked,
	title = "Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models",
	author = "Cheng, Myra  and
	Durmus, Esin  and
	Jurafsky, Dan",
	editor = "Rogers, Anna  and
	Boyd-Graber, Jordan  and
	Okazaki, Naoaki",
	booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
	month = jul,
	year = "2023",
	address = "Toronto, Canada",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2023.acl-long.84",
	doi = "10.18653/v1/2023.acl-long.84",
	pages = "1504--1532",
	abstract = "To recognize and mitigate harms from large language models (LLMs), we need to understand the prevalence and nuances of stereotypes in LLM outputs. Toward this end, we present Marked Personas, a prompt-based method to measure stereotypes in LLMs for intersectional demographic groups without any lexicon or data labeling. Grounded in the sociolinguistic concept of markedness (which characterizes explicitly linguistically marked categories versus unmarked defaults), our proposed method is twofold: 1) prompting an LLM to generate personas, i.e., natural language descriptions, of the target demographic group alongside personas of unmarked, default groups; 2) identifying the words that significantly distinguish personas of the target group from corresponding unmarked ones. We find that the portrayals generated by GPT-3.5 and GPT-4 contain higher rates of racial stereotypes than human-written portrayals using the same prompts. The words distinguishing personas of marked (non-white, non-male) groups reflect patterns of othering and exoticizing these demographics. An intersectional lens further reveals tropes that dominate portrayals of marginalized groups, such as tropicalism and the hypersexualization of minoritized women. These representational harms have concerning implications for downstream applications like story generation.",
}

@inproceedings{santy-etal-2023-nlpositionality,
	title = "{NLP}ositionality: Characterizing Design Biases of Datasets and Models",
	author = "Santy, Sebastin  and
	Liang, Jenny  and
	Le Bras, Ronan  and
	Reinecke, Katharina  and
	Sap, Maarten",
	editor = "Rogers, Anna  and
	Boyd-Graber, Jordan  and
	Okazaki, Naoaki",
	booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
	month = jul,
	year = "2023",
	address = "Toronto, Canada",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2023.acl-long.505",
	doi = "10.18653/v1/2023.acl-long.505",
	pages = "9080--9102",
	abstract = "Design biases in NLP systems, such as performance differences for different populations, often stem from their creator{'}s positionality, i.e., views and lived experiences shaped by identity and background. Despite the prevalence and risks of design biases, they are hard to quantify because researcher, system, and dataset positionality is often unobserved. We introduce NLPositionality, a framework for characterizing design biases and quantifying the positionality of NLP datasets and models. Our framework continuously collects annotations from a diverse pool of volunteer participants on LabintheWild, and statistically quantifies alignment with dataset labels and model predictions. We apply NLPositionality to existing datasets and models for two tasks{---}social acceptability and hate speech detection. To date, we have collected 16,299 annotations in over a year for 600 instances from 1,096 annotators across 87 countries. We find that datasets and models align predominantly with Western, White, college-educated, and younger populations. Additionally, certain groups, such as non-binary people and non-native English speakers, are further marginalized by datasets and models as they rank least in alignment across all tasks. Finally, we draw from prior literature to discuss how researchers can examine their own positionality and that of their datasets and models, opening the door for more inclusive NLP systems.",
}


@inproceedings{pmlr-v202-santurkar23a,
	title = 	 {Whose Opinions Do Language Models Reflect?},
	author =       {Santurkar, Shibani and Durmus, Esin and Ladhak, Faisal and Lee, Cinoo and Liang, Percy and Hashimoto, Tatsunori},
	booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
	pages = 	 {29971--30004},
	year = 	 {2023},
	editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
	volume = 	 {202},
	series = 	 {Proceedings of Machine Learning Research},
	month = 	 {7},
	publisher =    {PMLR},
	pdf = 	 {https://proceedings.mlr.press/v202/santurkar23a/santurkar23a.pdf},
	url = 	 {https://proceedings.mlr.press/v202/santurkar23a.html},
	abstract = 	 {Language models (LMs) are increasingly being used in open-ended contexts, where the opinions they reflect in response to subjective queries can have a profound impact, both on user satisfaction, and shaping the views of society at large. We put forth a quantitative framework to investigate the opinions reflected by LMs – by leveraging high-quality public opinion polls. Using this framework, we create OpinionQA, a dataset for evaluating the alignment of LM opinions with those of 60 US demographic groups over topics ranging from abortion to automation. Across topics, we find substantial misalignment between the views reflected by current LMs and those of US demographic groups: on par with the Democrat-Republican divide on climate change. Notably, this misalignment persists even after explicitly steering the LMs towards particular groups. Our analysis not only confirms prior observations about the left-leaning tendencies of some human feedback-tuned LMs, but also surfaces groups whose opinions are poorly reflected by current LMs (e.g., 65+ and widowed individuals).}
}

@online{gdpr,
	date       = {2016-05-04},
	location   = {OJ L 119, 4.5.2016, p. 1--88},
	title      = {Regulation ({EU}) 2016/679 of the {European} {Parliament} and of the {Council}},
	url        = {https://data.europa.eu/eli/reg/2016/679/oj},
	titleaddon = {of 27 {April} 2016 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing {Directive} 95/46/{EC} ({General} {Data} {Protection} {Regulation})},
	abstract   = {The General Data Protection Regulation (2016/679, "GDPR") is a Regulation in European Union (EU) law on data protection and privacy in the EU and the European Economic Area (EEA).},
	author     = {{European Parliament} and {Council of the European Union}},
	keywords   = {access consumer data data-processing freedom gdpr information justice law personal privacy protection security verification},
	urldate    = {2023-04-13},
}

@inproceedings{park_simulacra,
author = {Park, Joon Sung and Popowski, Lindsay and Cai, Carrie and Morris, Meredith Ringel and Liang, Percy and Bernstein, Michael S.},
title = {Social Simulacra: Creating Populated Prototypes for Social Computing Systems},
year = {2022},
isbn = {9781450393201},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3526113.3545616},
doi = {10.1145/3526113.3545616},
abstract = {Social computing prototypes probe the social behaviors that may arise in an envisioned system design. This prototyping practice is currently limited to recruiting small groups of people. Unfortunately, many challenges do not arise until a system is populated at a larger scale. Can a designer understand how a social system might behave when populated, and make adjustments to the design before the system falls prey to such challenges? We introduce social simulacra, a prototyping technique that generates a breadth of realistic social interactions that may emerge when a social computing system is populated. Social simulacra take as input the designer’s description of a community’s design—goal, rules, and member personas—and produce as output an instance of that design with simulated behavior, including posts, replies, and anti-social behaviors. We demonstrate that social simulacra shift the behaviors that they generate appropriately in response to design changes, and that they enable exploration of “what if?” scenarios where community members or moderators intervene. To power social simulacra, we contribute techniques for prompting a large language model to generate thousands of distinct community members and their social interactions with each other; these techniques are enabled by the observation that large language models’ training data already includes a wide variety of positive and negative behavior on social media platforms. In evaluations, we show that participants are often unable to distinguish social simulacra from actual community behavior and that social computing designers successfully refine their social computing designs when using social simulacra.},
booktitle = {Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology},
articleno = {74},
numpages = {18},
keywords = {prototyping, social computing},
location = {Bend, OR, USA},
series = {UIST '22}
}


@misc{abdelnabi_negotiations,
	title={Cooperation, Competition, and Maliciousness: LLM-Stakeholders Interactive Negotiation}, 
	author={Sahar Abdelnabi and Amr Gomaa and Sarath Sivaprasad and Lea Schönherr and Mario Fritz},
	year={2024},
	eprint={2309.17234},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2309.17234}, 
}


@article{liu2024largelanguagemodelsagents,
	title={Large Language Models as Agents in Two-Player Games},
	author={Yang Liu and Peng Sun and Hang Li},
	journal={ArXiv},
	year={2024},
	volume={abs/2402.08078},
	url={https://api.semanticscholar.org/CorpusID:267637288}
}


@article{zheng2024optimalllmalignmentsusing,
	title={Toward Optimal LLM Alignments Using Two-Player Games},
	author={Rui Zheng and Hongyi Guo and Zhihan Liu and Xiaoying Zhang and Yuanshun Yao and Xiaojun Xu and Zhaoran Wang and Zhiheng Xi and Tao Gui and Qi Zhang and Xuanjing Huang and Hang Li and Yang Liu},
	journal={ArXiv},
	year={2024},
	volume={abs/2406.10977},
	url={https://api.semanticscholar.org/CorpusID:270559576}
}

@article{cheng-self-play,
	title={Self-playing Adversarial Language Game Enhances LLM Reasoning},
	author={Pengyu Cheng and Tianhao Hu and Han Xu and Zhisong Zhang and Yong Dai and Lei Han and Nan Du},
	journal={ArXiv},
	year={2024},
	volume={abs/2404.10642},
	url={https://api.semanticscholar.org/CorpusID:269157364}
}

@inproceedings{chang-danescu-niculescu-mizil-2019-trouble,
	title = "Trouble on the Horizon: Forecasting the Derailment of Online Conversations as they Develop",
	author = "Chang, Jonathan P.  and
	Danescu, Cristian",
	editor = "Inui, Kentaro  and
	Jiang, Jing  and
	Ng, Vincent  and
	Wan, Xiaojun",
	booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
	month = nov,
	year = "2019",
	address = "Hong Kong, China",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/D19-1481",
	doi = "10.18653/v1/D19-1481",
	pages = "4743--4754",
	abstract = "Online discussions often derail into toxic exchanges between participants. Recent efforts mostly focused on detecting antisocial behavior after the fact, by analyzing single comments in isolation. To provide more timely notice to human moderators, a system needs to preemptively detect that a conversation is heading towards derailment before it actually turns toxic. This means modeling derailment as an emerging property of a conversation rather than as an isolated utterance-level event. Forecasting emerging conversational properties, however, poses several inherent modeling challenges. First, since conversations are dynamic, a forecasting model needs to capture the flow of the discussion, rather than properties of individual comments. Second, real conversations have an unknown horizon: they can end or derail at any time; thus a practical forecasting model needs to assess the risk in an online fashion, as the conversation develops. In this work we introduce a conversational forecasting model that learns an unsupervised representation of conversational dynamics and exploits it to predict future derailment as the conversation develops. By applying this model to two new diverse datasets of online conversations with labels for antisocial events, we show that it outperforms state-of-the-art systems at forecasting derailment.",
}


@article{zhang-2018-gone-awry,
	author       = {Justine Zhang and
	Jonathan P. Chang and
	Cristian Danescu{-}Niculescu{-}Mizil and
	Lucas Dixon and
	Yiqing Hua and
	Nithum Thain and
	Dario Taraborelli},
	title        = {Conversations Gone Awry: Detecting Early Signs of Conversational Failure},
	journal      = {CoRR},
	volume       = {abs/1805.05345},
	year         = {2018},
	url          = {http://arxiv.org/abs/1805.05345},
	eprinttype    = {arXiv},
	eprint       = {1805.05345},
	timestamp    = {Sun, 02 Oct 2022 15:31:52 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/abs-1805-05345.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{boschi2021wordunderstandingsampleonline,
	title={Who Has the Last Word? Understanding How to Sample Online Discussions},
	author={Gioia Boschi and Anthony Peter Young and Sagar Joglekar and Chiara Cammarota and Nishanth R. Sastry},
	journal={Companion Proceedings of the Web Conference 2022},
	year={2021},
	url={https://api.semanticscholar.org/CorpusID:236267031}
}

@misc{Cornell_eRulemaking2017,
	author = {Cornell eRulemaking Initiative},
	title = {CeRI (Cornell e-Rulemaking) Moderator Protocol},
	year = {2017},
	note = {Cornell e-Rulemaking Initiative Publications, 21},
	url = {https://scholarship.law.cornell.edu/ceri/21}
}

@inproceedings{vaswani2023attentionneed,
	title={Attention is All you Need},
	author={Ashish Vaswani and Noam M. Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
	booktitle={Neural Information Processing Systems},
	year={2017},
	url={https://api.semanticscholar.org/CorpusID:13756489}
}

@article{Vezhnevets2023GenerativeAM,
	title={Generative agent-based modeling with actions grounded in physical, social, or digital space using Concordia},
	author={Alexander Sasha Vezhnevets and John P. Agapiou and Avia Aharon and Ron Ziv and Jayd Matyas and Edgar A. Du'enez-Guzm'an and William A. Cunningham and Simon Osindero and Danny Karmon and Joel Z. Leibo},
	journal={ArXiv},
	year={2023},
	volume={abs/2312.03664},
	url={https://api.semanticscholar.org/CorpusID:265692435}
}

@article{Birkun_Gautam_2023,
	title={Large Language Model (LLM)-Powered Chatbots Fail to Generate Guideline-Consistent Content on Resuscitation and May Provide Potentially Harmful Advice}, 
	volume={38}, 
	DOI={10.1017/S1049023X23006568}, 
	number={6},
	journal={Prehospital and Disaster Medicine}, 
	author={Birkun, Alexei A. and Gautam, Adhish}, 
	year={2023}, 
	pages={757–763}} 

@InProceedings{aher2023usinglargelanguagemodels,
	title = 	 {Using Large Language Models to Simulate Multiple Humans and Replicate Human Subject Studies},
	author =       {Aher, Gati V and Arriaga, Rosa I. and Kalai, Adam Tauman},
	booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
	pages = 	 {337--371},
	year = 	 {2023},
	editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
	volume = 	 {202},
	series = 	 {Proceedings of Machine Learning Research},
	month = 	 {7},
	publisher =    {PMLR},
	pdf = 	 {https://proceedings.mlr.press/v202/aher23a/aher23a.pdf},
	url = 	 {https://proceedings.mlr.press/v202/aher23a.html},
	abstract = 	 {We introduce a new type of test, called a Turing Experiment (TE), for evaluating to what extent a given language model, such as GPT models, can simulate different aspects of human behavior. A TE can also reveal consistent distortions in a language model’s simulation of a specific human behavior. Unlike the Turing Test, which involves simulating a single arbitrary individual, a TE requires simulating a representative sample of participants in human subject research. We carry out TEs that attempt to replicate well-established findings from prior studies. We design a methodology for simulating TEs and illustrate its use to compare how well different language models are able to reproduce classic economic, psycholinguistic, and social psychology experiments: Ultimatum Game, Garden Path Sentences, Milgram Shock Experiment, and Wisdom of Crowds. In the first three TEs, the existing findings were replicated using recent models, while the last TE reveals a “hyper-accuracy distortion” present in some language models (including ChatGPT and GPT-4), which could affect downstream applications in education and the arts.}
}


@article{lambert2024,
	title={Self-Directed Synthetic Dialogues and Revisions Technical Report},
	author={Nathan Lambert and Hailey Schoelkopf and Aaron Gokaslan and Luca Soldaini and Valentina Pyatkin and Louis Castricato},
	journal={ArXiv},
	year={2024},
	volume={abs/2407.18421},
	url={https://api.semanticscholar.org/CorpusID:271516167}
}

@article{Bai2022ConstitutionalAH,
	title={Constitutional AI: Harmlessness from AI Feedback},
	author={Yuntao Bai and Saurav Kadavath and Sandipan Kundu and Amanda Askell and John Kernion and Andy Jones and Anna Chen and Anna Goldie and Azalia Mirhoseini and Cameron McKinnon and Carol Chen and Catherine Olsson and Christopher Olah and Danny Hernandez and Dawn Drain and Deep Ganguli and Dustin Li and Eli Tran-Johnson and E Perez and Jamie Kerr and Jared Mueller and Jeff Ladish and J Landau and Kamal Ndousse and Kamilė Lukovsiūtė and Liane Lovitt and Michael Sellitto and Nelson Elhage and Nicholas Schiefer and Noem'i Mercado and Nova Dassarma and Robert Lasenby and Robin Larson and Sam Ringer and Scott Johnston and Shauna Kravec and Sheer El Showk and Stanislav Fort and Tamera Lanham and Timothy Telleen-Lawton and Tom Conerly and Tom Henighan and Tristan Hume and Sam Bowman and Zac Hatfield-Dodds and Benjamin Mann and Dario Amodei and Nicholas Joseph and Sam McCandlish and Tom B. Brown and Jared Kaplan},
	journal={ArXiv},
	year={2022},
	volume={abs/2212.08073},
	url={https://api.semanticscholar.org/CorpusID:254823489}
}

@article{Castricato2024SuppressingPE,
	title={Suppressing Pink Elephants with Direct Principle Feedback},
	author={Louis Castricato and Nathan Lile and Suraj Anand and Hailey Schoelkopf and Siddharth Verma and Stella Biderman},
	journal={ArXiv},
	year={2024},
	volume={abs/2402.07896},
	url={https://api.semanticscholar.org/CorpusID:267627799}
}

@article{Gretz2019ALD,
	title={A Large-scale Dataset for Argument Quality Ranking: Construction and Analysis},
	author={Shai Gretz and Roni Friedman and Edo Cohen-Karlik and Assaf Toledo and Dan Lahav and Ranit Aharonov and Noam Slonim},
	journal={ArXiv},
	year={2019},
	volume={abs/1911.11408},
	url={https://api.semanticscholar.org/CorpusID:208291067}
}

@inproceedings{al-khatib-etal-2018-modeling,
	title = {Modeling Deliberative Argumentation Strategies on Wikipedia},
	author = {Al-Khatib, Khalid  and
	Wachsmuth, Henning  and
	Lang, Kevin  and
	Herpel, Jakob  and
	Hagen, Matthias  and
	Stein, Benno},
	editor = {Gurevych, Iryna  and
	Miyao, Yusuke},
	booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	month = jul,
	year = {2018},
	address = {Melbourne, Australia},
	publisher = {Association for Computational Linguistics},
	url = {https://aclanthology.org/P18-1237},
	doi = {10.18653/v1/P18-1237},
	pages = {2545--2555},
}

@article{Avalle2024PersistentIP,
	title={Persistent interaction patterns across social media platforms and over time},
	author={Michele Avalle and Niccol{\`o} Di Marco and Gabriele Etta and Emanuele Sangiorgio and Shayan Alipour and Anita Bonetti and Lorenzo Alvisi and Antonio Scala and Andrea Baronchelli and Matteo Cinelli and Walter Quattrociocchi},
	journal={Nature},
	year={2024},
	volume={628},
	pages={582 - 589},
	url={https://api.semanticscholar.org/CorpusID:268550181}
}

@book{gamma1995design,
	title={Design patterns: elements of reusable object-oriented software},
	author={Gamma, Erich and Helm, Richard and Johnson, Ralph and Vlissides, John},
	year={1995},
	publisher={Pearson Deutschland GmbH}
}

@article{anjum2024hate,
	title={Hate speech, toxicity detection in online social media: a recent survey of state of the art and opportunities},
	author={Anjum and Katarya, Rahul},
	journal={International Journal of Information Security},
	volume={23},
	number={1},
	pages={577--608},
	year={2024},
	publisher={Springer}
}

@inproceedings{pavlopoulos-likas-2024-polarized,
	title = "Polarized Opinion Detection Improves the Detection of Toxic Language",
	author = "Pavlopoulos, John  and
	Likas, Aristidis",
	editor = "Graham, Yvette  and
	Purver, Matthew",
	booktitle = "Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers)",
	month = mar,
	year = "2024",
	address = "St. Julian{'}s, Malta",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.eacl-long.117",
	pages = "1946--1958",
	abstract = "Distance from unimodality (DFU) has been found to correlate well with human judgment for the assessment of polarized opinions. However, its un-normalized nature makes it less intuitive and somewhat difficult to exploit in machine learning (e.g., as a supervised signal). In this work a normalized version of this measure, called nDFU, is proposed that leads to better assessment of the degree of polarization. Then, we propose a methodology for K-class text classification, based on nDFU, that exploits polarized texts in the dataset. Such polarized instances are assigned to a separate K+1 class, so that a K+1-class classifier is trained. An empirical analysis on three datasets for abusive language detection, shows that nDFU can be used to model polarized annotations and prevent them from harming the classification performance. Finally, we further exploit nDFU to specify conditions that could explain polarization given a dimension and present text examples that polarized the annotators when the dimension was gender and race. Our code is available at https://github.com/ipavlopoulos/ndfu.",
}

@article{WrightDemocracy,
	author = {Scott Wright and John Street},
	title ={Democracy, deliberation and design: the case of online discussion forums},
	journal = {New Media \& Society},
	volume = {9},
	number = {5},
	pages = {849-869},
	year = {2007},
	doi = {10.1177/1461444807081230},
	URL = {https://doi.org/10.1177/1461444807081230},
	eprint = {https://doi.org/10.1177/1461444807081230},
	abstract = { Within democratic theory, the deliberative variant has assumed pre-eminence. It represents for many the ideal of democracy, and in pursuit of this ideal, online discussion forums have been proposed as solutions to the practical limits to mass deliberation. Critics have pointed to evidence which suggests that online discussion has tended to undermine deliberation. This article argues that this claim, which generates a stand-off between the two camps, misses a key issue: the role played by design in facilitating or thwarting deliberation. It argues that political choices are made both about the format and operation of the online discussion, and that this affects the possibility of deliberation. Evidence for the impact of design (and the choices behind it) is drawn from analysis of European Union and UK discussion forums. This evidence suggests that we should view deliberation as dependent on design and choice, rather than a predetermined product of the technology. }
}

@article{Janssen2005,
	author    = {Davy Janssen and Raphaël Kies},
	title     = {Online Forums and Deliberative Democracy},
	journal   = {Acta Politica},
	year      = {2005},
	volume    = {40},
	number    = {3},
	pages     = {317--335},
	doi       = {10.1057/palgrave.ap.5500115},
	url       = {https://doi.org/10.1057/palgrave.ap.5500115},
	issn      = {1741-1416},
	abstract  = {The Internet is rapidly becoming a part of the everyday lives of a majority of people in the Western world. People perform various activities on the Internet and one of them is discussing politics and society in so-called online forums. In this article, we present an overview of some of the empirical research that evaluates the quality of political conversations in online forums. In the first section, we distinguish research on Usenet groups, web-based political forums and e-consultation forums, and discuss some its findings. In the second section, we elaborate three original categories of variables that attempt to explain differences observed in the quality of deliberation. The third, more extensive section deals with methodological issues. It discusses the operationalisation of deliberative quality and the application of a set of criteria for the idealized public sphere to online conversations. In the conclusion, we present some objections to the previous research and offer some ideas for a more comprehensive approach to online forum analysis.}
}

@article{Papacharissi2004DemocracyOC,
	title={Democracy online: civility, politeness, and the democratic potential of online political discussion groups},
	author={Zizi Papacharissi},
	journal={New Media \& Society},
	year={2004},
	volume={6},
	pages={259 - 283},
	url={https://api.semanticscholar.org/CorpusID:18186437}
}

@article{XiaToxicity,
	author = {Xia, Yan and Zhu, Haiyi and Lu, Tun and Zhang, Peng and Gu, Ning},
	title = {Exploring Antecedents and Consequences of Toxicity in Online Discussions: A Case Study on Reddit},
	year = {2020},
	issue_date = {October 2020},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {4},
	number = {CSCW2},
	url = {https://doi.org/10.1145/3415179},
	doi = {10.1145/3415179},
	abstract = {Toxicity in online discussions has been an intriguing phenomenon and an important problem. In this paper, we seek to better understand toxicity dynamics in online discussions via a case study on Reddit that explores the antecedents and consequences of toxicity in text. We inspected two dimensions of toxicity: language toxicity, i.e. how toxic the text itself is; and toxicity elicitation, i.e. how much toxicity it elicits in its response. Through regression analyses on Reddit comments, we found that both author propensity and toxicity in discussion context were strong positive antecedents of language toxicity; meanwhile, language toxicity significantly increased the volume and user evaluation of the discussion in some sub-communities, while toxicity elicitation showed mixed effects. We then discuss how our results help understand and regulate toxicity in online discussions by interpreting the complicated triggers and outcomes of toxicity.},
	journal = {Proc. ACM Hum.-Comput. Interact.},
	month = {10},
	articleno = {108},
	numpages = {23},
	keywords = {toxicity, reddit, quantitative analysis, online discussions}
}

@online{Harvard2024,
	author    = {{Harvard Graduate School of Education}},
	title     = {Responding to Students},
	howpublished = {\url{https://instructionalmoves.gse.harvard.edu/responding-students}},
	note      = {Accessed: 2024-09-16},
	year      = {2024},
	url       = {https://instructionalmoves.gse.harvard.edu/responding-students}
}

@article{Wang2008StudentfacilitatorsRI,
	title={Student-facilitators' roles in moderating online discussions},
	author={Qiyun Wang},
	journal={Br. J. Educ. Technol.},
	year={2008},
	volume={39},
	pages={859-874},
	url={https://api.semanticscholar.org/CorpusID:46324927}
}


@article{ts2024,
	title = {Neural natural language processing for long texts: A survey on classification and summarization},
	journal = {Engineering Applications of Artificial Intelligence},
	volume = {133},
	pages = {108231},
	year = {2024},
	issn = {0952-1976},
	doi = {https://doi.org/10.1016/j.engappai.2024.108231},
	url = {https://www.sciencedirect.com/science/article/pii/S0952197624003890},
	author = {Dimitrios Tsirmpas and Ioannis Gkionis and Georgios Th. Papadopoulos and Ioannis Mademlis},
	keywords = {Natural language processing, Long document, Document classification, Document summarization, Sentiment analysis, Deep neural networks},
	abstract = {The adoption of Deep Neural Networks (DNNs) has greatly benefited Natural Language Processing (NLP) during the past decade. However, the demands of long document analysis are quite different from those of shorter texts, while the ever increasing size of documents uploaded online renders automated understanding of lengthy texts a critical issue. Relevant applications include automated Web mining, legal document review, medical records analysis, financial reports analysis, contract management, environmental impact assessment, news aggregation, etc. Despite the relatively recent development of efficient algorithms for analyzing long documents, practical tools in this field are currently flourishing. This article serves as an entry point into this dynamic domain and aims to achieve two objectives. First of all, it provides an introductory overview of the relevant neural building blocks, serving as a concise tutorial for the field. Secondly, it offers a brief examination of the current state-of-the-art in two key long document analysis tasks: document classification and document summarization. Sentiment analysis for long texts is also covered, since it is typically treated as a particular case of document classification. Consequently, this article presents an introductory exploration of document-level analysis, addressing the primary challenges, concerns, and existing solutions. Finally, it offers a concise definition of “long text/document”, presents an original overarching taxonomy of common deep neural methods for long document analysis and lists publicly available annotated datasets that can facilitate further research in this area.}
}

@article{tan2024largelanguagemodelsdata,
	title={Large Language Models for Data Annotation: A Survey},
	author={Zhen Tan and Dawei Li and Alimohammad Beigi and Song Wang and Ruocheng Guo and Amrita Bhattacharjee and Bohan Jiang and Mansooreh Karami and Jundong Li and Lu Cheng and Huan Liu},
	journal={ArXiv},
	year={2024},
	volume={abs/2402.13446},
	url={https://api.semanticscholar.org/CorpusID:267770019}
}

@misc{HadiASO,
	author = {Hadi, Muhammad Usman and Al-Tashi, Qasem and Qureshi, Rizwan and Shah, Abbas and Muneer, Amgad and Irfan, Muhammad and Zafar, Anas and Shaikh, Muhammad and Akhtar, Naveed and Wu, Jia and Mirjalili, Seyedali},
	year = {2023},
	month = {07},
	pages = {},
	title = {Large Language Models: A Comprehensive Survey of its Applications, Challenges, Limitations, and Future Prospects},
	doi = {10.36227/techrxiv.23589741.v1}
}

@article{Zhou2024LargeLM,
	title={Large Language Model (LLM) for Telecommunications: A Comprehensive Survey on Principles, Key Techniques, and Opportunities},
	author={Hao Zhou and Chengming Hu and Ye Yuan and Yufei Cui and Yili Jin and Can Chen and Haolun Wu and Dun Yuan and Li Jiang and Di Wu and Xue Liu and Charlie Zhang and Xianbin Wang and Jiangchuan Liu},
	journal={ArXiv},
	year={2024},
	volume={abs/2405.10825},
	url={https://api.semanticscholar.org/CorpusID:269899574}
}

@article{Hutchinson2024LLMAssistedVA,
	title={LLM-Assisted Visual Analytics: Opportunities and Challenges},
	author={Maeve E Hutchinson and Radu Jianu and Aidan Slingsby and Pranava Swaroop Madhyastha},
	journal={ArXiv},
	year={2024},
	volume={abs/2409.02691},
	url={https://api.semanticscholar.org/CorpusID:272397798}
}

@InProceedings{shi-2024-hatespeech,
	author="Shi, Xiaohou
	and Liu, Jiahao
	and Song, Yaqi",
	editor="Jin, Hai
	and Pan, Yi
	and Lu, Jianfeng",
	title="BERT and LLM-Based Multivariate Hate Speech Detection on Twitter: Comparative Analysis and Superior Performance",
	booktitle="Artificial Intelligence and Machine Learning",
	year="2024",
	publisher="Springer Nature Singapore",
	address="Singapore",
	pages="85--97",
	abstract="The detection of toxic and hate speech in online social media is becoming increasingly necessary due to its prevalence and the potentially harmful consequences it can cause. Previous research has demonstrated the vital role that machine learning and natural language processing models have in identifying inappropriate language. In this study, the aim is to assess the viability of BERT for accurately predicting multivariate classifications related to hate speech on Twitter. The analysis will be conducted using the Twitter hate speech dataset. BERT has demonstrated exceptional performance in numerous areas of NLP, making it a potentially superior alternative to traditional machine learning approaches. Experiments were performed on the same dataset using 1-layer BERT, 2-layers BERT, and logistic regression models for both training and prediction purposes. The results demonstrate that the 2-layer BERT produces an accuracy of 85{\%}. Additionally, we incorporated transfer learning techniques by leveraging a Large Language model GPT-3 and data augmentation strategies to further enhance model performance. This experiment reached a higher accuracy of 88{\%}. As this is a multivariate classification problem with an asymmetrical dataset, we anticipate BERT and GPT-3 will achieve greater accuracy for the binary classification problem of identifying hate speech. These findings enhance the comprehension of hate speech detection in online material and the implications of various modeling approaches.",
	isbn="978-981-97-1277-9"
}

@article{Nirmal2024TowardsIH,
	title={Towards Interpretable Hate Speech Detection using Large Language Model-extracted Rationales},
	author={Ayushi Nirmal and Amrita Bhattacharjee and Paras Sheth and Huan Liu},
	journal={ArXiv},
	year={2024},
	volume={abs/2403.12403},
	url={https://api.semanticscholar.org/CorpusID:268532294}
}

@misc{Xu2024ACS,
	title={A Comparative Study of Offline Models and Online LLMs in Fake News Detection}, 
	author={Ruoyu Xu and Gaoxiang Li},
	year={2024},
	eprint={2409.03067},
	archivePrefix={arXiv},
	primaryClass={cs.SI},
	url={https://arxiv.org/abs/2409.03067}, 
}

@inproceedings{Wang2024LLMGANCG,
	title={LLM-GAN: Construct Generative Adversarial Network Through Large Language Models For Explainable Fake News Detection},
	author={Yifeng Wang and Zhouhong Gu and Siwei Zhang and Suhang Zheng and Tao Wang and Tianyu Li and Hongwei Feng and Yanghua Xiao},
	year={2024},
	url={https://api.semanticscholar.org/CorpusID:272367748}
}

@article{Liu2024DetectIJ,
	title={Detect, Investigate, Judge and Determine: A Novel LLM-based Framework for Few-shot Fake News Detection},
	author={Ye Liu and Jiajun Zhu and Kai Zhang and Haoyu Tang and Yanghai Zhang and Xukai Liu and Qi Liu and Enhong Chen},
	journal={ArXiv},
	year={2024},
	volume={abs/2407.08952},
	url={https://api.semanticscholar.org/CorpusID:271161933}
}
@inproceedings{kang-qian-2024-implanting,
	title = "Implanting {LLM}{'}s Knowledge via Reading Comprehension Tree for Toxicity Detection",
	author = "Kang, Hankun  and
	Qian, Tieyun",
	editor = "Ku, Lun-Wei  and
	Martins, Andre  and
	Srikumar, Vivek",
	booktitle = "Findings of the Association for Computational Linguistics ACL 2024",
	month = aug,
	year = "2024",
	address = "Bangkok, Thailand and virtual meeting",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.findings-acl.56",
	doi = "10.18653/v1/2024.findings-acl.56",
	pages = "947--962",
	abstract = "Toxicity detection plays a crucial role in maintaining the peace of the society. Existing methods can be roughly categorized as small language model (SLM) based and large language model (LLM) based. However, due to the limitation of SLMs on general knowledge and the potential embedded bias in LLMs despite their large amount of knowledge, it is not a good idea to detect toxicity only with either SLM or LLM based method.In this work, we propose to implant LLM{'}s knowledge into SLM based methods such that we can stick to both types of models{'} strengths. To this end, we develop a reading comprehension (RC) tree to transfer knowledge between two models. Specifically, we first construct the RC tree, from an extensive to intensive reading perspective, to capture the local and global information in the text. We then model samples encoded by SLM and knowledge extracted from LLM as two distributions using the constructed RT tree. We finally transfer knowledge via optimal transportation between two distributions. Extensive experiments prove the effectiveness of our method on real-world and machine-generated datasets.",
}

@article{Wang2022ToxicityDW,
	title={Toxicity Detection with Generative Prompt-based Inference},
	author={Yau-Shian Wang and Ying Tai Chang},
	journal={ArXiv},
	year={2022},
	volume={abs/2205.12390},
	url={https://api.semanticscholar.org/CorpusID:249062985}
}

@article{Tsai2024Generative,
	author = {Tsai, Lily L. and Pentland, Alex and Braley, Alia and Chen, Nuole and Enr{\' i}quez, Jos{\' e} Ram{\' o}n and Reuel, Anka},
	journal = {An MIT Exploration of Generative AI},
	year = {2024},
	month = {3},
	note = {https://mit-genai.pubpub.org/pub/mn45hexw},
	publisher = {MIT},
	title = {Generative {AI} for {Pro}-{Democracy} {Platforms}},
}

@article{lockerbie2013race,
	title={Race and religion: Voting behavior and political attitudes},
	author={Lockerbie, Brad},
	journal={Social Science Quarterly},
	volume={94},
	number={4},
	pages={1145--1158},
	year={2013},
	publisher={Wiley Online Library}
}

@article{mckenzie2013shades,
	title={Shades of faith: Religious foundations of political attitudes among African Americans, Latinos, and Whites},
	author={McKenzie, Brian D and Rouse, Stella M},
	journal={American Journal of Political Science},
	volume={57},
	number={1},
	pages={218--235},
	year={2013},
	publisher={Wiley Online Library}
}

@Inbook{Rey2011,
	author="Rey, Denise
	and Neuh{\"a}user, Markus",
	editor="Lovric, Miodrag",
	title="Wilcoxon-Signed-Rank Test",
	bookTitle="International Encyclopedia of Statistical Science",
	year="2011",
	publisher="Springer Berlin Heidelberg",
	address="Berlin, Heidelberg",
	pages="1658--1659",
	isbn="978-3-642-04898-2",
	doi="10.1007/978-3-642-04898-2_616",
	url="https://doi.org/10.1007/978-3-642-04898-2_616"
}


@article{Taubenfeld2024SystematicBI,
	title={Systematic Biases in LLM Simulations of Debates},
	author={Amir Taubenfeld and Yaniv Dover and Roi Reichart and Ariel Goldstein},
	journal={ArXiv},
	year={2024},
	volume={abs/2402.04049},
	url={https://api.semanticscholar.org/CorpusID:267499945}
}

@inproceedings{Zhong2019ExploringTR,
	title={Exploring the Roles and Facilitation Strategies of Online Peer Moderators},
	author={Qunyan Maggie Zhong and Howard Norton},
	year={2019},
	url={https://api.semanticscholar.org/CorpusID:212423667}
}

@incollection{Carson2008,
	author    = {Carson, Lyn},
	title     = {E-Moderation in Public Discussion Forums},
	booktitle = {Electronic Government: Concepts, Methodologies, Tools, and Applications},
	editor    = {Anttiroiko, Ari-Veikko},
	pages     = {3517--3526},
	publisher = {IGI Global},
	year      = {2008},
	doi       = {10.4018/978-1-59904-947-2.ch256},
	url       = {https://doi.org/10.4018/978-1-59904-947-2.ch256}
}

@ONLINE {wef_moderation,
	author    = "World Economic Forum",
	title     = "Moderation and Facilitation",
	publisher = "World Economic Forum",
	month     = "1",
	year      = "2014",
	url       = "https://www3.weforum.org/docs/WEF_Moderation_Facilitation_Briefing_2014.pdf",
	urldate   = "2024-10-24"
}

@online{vincent_2019_ai_go,
	author    = {Vincent, James},
	title     = {Former Go champion beaten by DeepMind retires after declaring AI invincible},
	year      = {2019},
	month     = {11},
	url       = {https://www.theverge.com/2019/11/27/20985021/go-champion-retires-ai-invincible-deepmind-alphago-ai-artificial-intelligence},
	urldate   = {2024-11-22},
	journal   = {The Verge},
	note      = {Accessed: 2024-11-22},
}

@article{Park2023GenerativeAI,
	title={Generative Agents: Interactive Simulacra of Human Behavior},
	author={Joon Sung Park and Joseph C. O'Brien and Carrie J. Cai and Meredith Ringel Morris and Percy Liang and Michael S. Bernstein},
	journal={Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology},
	year={2023},
	url={https://api.semanticscholar.org/CorpusID:258040990}
}

@misc{park2024generativeagentsimulations1000,
	title={Generative Agent Simulations of 1,000 People}, 
	author={Joon Sung Park and Carolyn Q. Zou and Aaron Shaw and Benjamin Mako Hill and Carrie Cai and Meredith Ringel Morris and Robb Willer and Percy Liang and Michael S. Bernstein},
	year={2024},
	eprint={2411.10109},
	archivePrefix={arXiv},
	primaryClass={cs.AI},
	url={https://arxiv.org/abs/2411.10109}, 
}

@inproceedings{collective_constitution,
author = {Huang, Saffron and Siddarth, Divya and Lovitt, Liane and Liao, Thomas I. and Durmus, Esin and Tamkin, Alex and Ganguli, Deep},
title = {Collective Constitutional AI: Aligning a Language Model with Public Input},
year = {2024},
isbn = {9798400704505},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3630106.3658979},
doi = {10.1145/3630106.3658979},
abstract = {There is growing consensus that language model (LM) developers should not be the sole deciders of LM behavior, creating a need for methods that enable the broader public to collectively shape the behavior of LM systems that affect them. To address this need, we present Collective Constitutional AI (CCAI): a multi-stage process for sourcing and integrating public input into LMs—from identifying a target population to sourcing principles to training and evaluating a model. We demonstrate the real-world practicality of this approach by creating what is, to our knowledge, the first LM fine-tuned with collectively sourced public input and evaluating this model against a baseline model trained with established principles from a LM developer. Our quantitative evaluations demonstrate several benefits of our approach: the CCAI-trained model shows lower bias across nine social dimensions compared to the baseline model, while maintaining equivalent performance on language, math, and helpful-harmless evaluations. Qualitative comparisons of the models suggest that the models differ on the basis of their respective constitutions, e.g., when prompted with contentious topics, the CCAI-trained model tends to generate responses that reframe the matter positively instead of a refusal. These results demonstrate a promising, tractable pathway toward publicly informed development of language models.},
booktitle = {Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1395–1417},
numpages = {23},
keywords = {AI alignment, AI bias, AI ethics, collective alignment, generative AI, human-centered AI, participatory AI, reinforcement learning from human feedback, value alignment},
location = {Rio de Janeiro, Brazil},
series = {FAccT '24}
}

@mastersthesis{dtsirmpas_thesis,
    author = {Dimitris Tsirmpas},
    title = {Mitigating polarization in online discussions through adaptive moderation techniques},
    school = {Athens University of Economics and Business},
    year = {2024}
}

@article{make_reddit_great,
  title={Make Reddit Great Again: Assessing Community Effects of Moderation Interventions on r/The\_Donald},
  author={Amaury T. and Stefano C.},
  journal={Proceedings of the ACM on Human-Computer Interaction},
  year={2022},
  volume={6},
  pages={1 - 28},
  url={https://api.semanticscholar.org/CorpusID:246016404}
}

@inproceedings{schaffner_community_guidelines,
author = {Schaffner, Brennan and Bhagoji, Arjun Nitin and Cheng, Siyuan and Mei, Jacqueline and Shen, Jay L and Wang, Grace and Chetty, Marshini and Feamster, Nick and Lakier, Genevieve and Tan, Chenhao},
title = {"Community Guidelines Make this the Best Party on the Internet": An In-Depth Study of Online Platforms' Content Moderation Policies},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642333},
doi = {10.1145/3613904.3642333},
abstract = {Moderating user-generated content on online platforms is crucial for balancing user safety and freedom of speech. Particularly in the United States, platforms are not subject to legal constraints prescribing permissible content. Each platform has thus developed bespoke content moderation policies, but there is little work towards a comparative understanding of these policies across platforms and topics. This paper presents the first systematic study of these policies from the 43 largest online platforms hosting user-generated content, focusing on policies around copyright infringement, harmful speech, and misleading content. We build a custom web-scraper to obtain policy text and develop a unified annotation scheme to analyze the text for the presence of critical components. We find significant structural and compositional variation in policies across topics and platforms, with some variation attributable to disparate legal groundings. We lay the groundwork for future studies of ever-evolving content moderation policies and their impact on users.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {486},
numpages = {16},
keywords = {content moderation, dataset, qualitative analysis, quantitative analysis},
location = {Honolulu, HI, USA},
series = {CHI '24}
}


@inproceedings{falk-etal-2021-predicting,
    title = "Predicting Moderation of Deliberative Arguments: Is Argument Quality the Key?",
    author = "Falk, Neele  and
      Jundi, Iman  and
      Vecchi, Eva Maria  and
      Lapesa, Gabriella",
    editor = "Al-Khatib, Khalid  and
      Hou, Yufang  and
      Stede, Manfred",
    booktitle = "Proceedings of the 8th Workshop on Argument Mining",
    month = nov,
    year = "2021",
    address = "Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.argmining-1.13/",
    doi = "10.18653/v1/2021.argmining-1.13",
    pages = "133--141",
    abstract = "Human moderation is commonly employed in deliberative contexts (argumentation and discussion targeting a shared decision on an issue relevant to a group, e.g., citizens arguing on how to employ a shared budget). As the scale of discussion enlarges in online settings, the overall discussion quality risks to drop and moderation becomes more important to assist participants in having a cooperative and productive interaction. The scale also makes it more important to employ NLP methods for(semi-)automatic moderation, e.g. to prioritize when moderation is most needed. In this work, we make the first steps towards (semi-)automatic moderation by using state-of-the-art classification models to predict which posts require moderation, showing that while the task is undoubtedly difficult, performance is significantly above baseline. We further investigate whether argument quality is a key indicator of the need for moderation, showing that surprisingly, high quality arguments also trigger moderation. We make our code and data publicly available."
}

@article{proactive_moderation,
author = {Schluger, C. and Chang, J.P. and Danescu-Niculescu-Mizil, C. and Levy, K.},
title = {Proactive Moderation of Online Discussions: Existing Practices and the Potential for Algorithmic Support},
year = {2022},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {CSCW2},
url = {https://doi.org/10.1145/3555095},
doi = {10.1145/3555095},
abstract = {To address the widespread problem of uncivil behavior, many online discussion platforms employ human moderators to take action against objectionable content, such as removing it or placing sanctions on its authors. Thisreactive paradigm of taking action against already-posted antisocial content is currently the most common form of moderation, and has accordingly underpinned many recent efforts at introducing automation into the moderation process. Comparatively less work has been done to understand other moderation paradigms---such as proactively discouraging the emergence of antisocial behavior rather than reacting to it---and the role algorithmic support can play in these paradigms. In this work, we investigate such a proactive framework for moderation in a case study of a collaborative setting: Wikipedia Talk Pages. We employ a mixed methods approach, combining qualitative and design components for a holistic analysis. Through interviews with moderators, we find that despite a lack of technical and social support, moderators already engage in a number of proactive moderation behaviors, such as preemptively intervening in conversations to keep them on track. Further, we explore how automation could assist with this existing proactive moderation workflow by building a prototype tool, presenting it to moderators, and examining how the assistance it provides might fit into their workflow. The resulting feedback uncovers both strengths and drawbacks of the prototype tool and suggests concrete steps towards further developing such assisting technology so it can most effectively support moderators in their existing proactive moderation workflow.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = nov,
articleno = {370},
numpages = {27},
keywords = {algorithmic assistance, antisocial behavior, content moderation, hybrid systems}
}

@article{Matias2019TheCL,
  title={The Civic Labor of Volunteer Moderators Online},
  author={Jorge Nathan Matias},
  journal={Social Media + Society},
  year={2019},
  volume={5},
  url={https://api.semanticscholar.org/CorpusID:150507350}
}

@misc{stone1993vampires,
  author       = {Allucquère Rosanne Stone},
  title        = {What Vampires Know: Transsubjection and Transgender in Cyberspace},
  year         = {1993},
  note         = {Talk given at the “In Control: Mensch-Interface-Maschine” Conference in Graz, Austria}
}

@incollection{smith1998conflict,
  author       = {Anna DuVal Smith},
  title        = {Problems of Conflict Management in Virtual Communities},
  booktitle    = {Communities in Cyberspace},
  edition      = {1st},
  year         = {1998},
  publisher    = {Routledge},
  pages        = {30},
  isbn         = {9780203194959}
}

@article{seering_self_moderation,
author = {Seering, J.},
title = {Reconsidering Self-Moderation: the Role of Research in Supporting Community-Based Models for Online Content Moderation},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {CSCW2},
url = {https://doi.org/10.1145/3415178},
doi = {10.1145/3415178},
abstract = {Research in online content moderation has a long history of exploring different forms that moderation can take, including both user-driven moderation models on community-based platforms like Wikipedia, Facebook Groups, and Reddit, and centralized corporate moderation models on platforms like Twitter and Instagram. In this work I review different approaches to moderation research with the goal of providing a roadmap for researchers studying community self-moderation. I contrast community-based moderation research with platforms and policies-focused moderation research, and argue that the former has an important role to play in shaping discussions about the future of online moderation. I provide six guiding questions for future research that, if answered, can support the development of a form of user-driven moderation that is widely implementable across a variety of social spaces online, offering an alternative to the corporate moderation models that dominate public debate and discussion.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = oct,
articleno = {107},
numpages = {28},
keywords = {social networks, platforms, online communities, moderation, hate speech, harassment, governance}
}

@inproceedings{horta_automated_moderation,
author = {Horta Ribeiro, Manoel and Cheng, Justin and West, Robert},
title = {Automated Content Moderation Increases Adherence to Community Guidelines},
year = {2023},
isbn = {9781450394161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543507.3583275},
doi = {10.1145/3543507.3583275},
abstract = {Online social media platforms use automated moderation systems to remove or reduce the visibility of rule-breaking content. While previous work has documented the importance of manual content moderation, the effects of automated content moderation remain largely unknown. Here, in a large study of Facebook comments (n = 412M), we used a fuzzy regression discontinuity design to measure the impact of automated content moderation on subsequent rule-breaking behavior (number of comments hidden/deleted) and engagement (number of additional comments posted). We found that comment deletion decreased subsequent rule-breaking behavior in shorter threads (20 or fewer comments), even among other participants, suggesting that the intervention prevented conversations from derailing. Further, the effect of deletion on the affected user’s subsequent rule-breaking behavior was longer-lived than its effect on reducing commenting in general, suggesting that users were deterred from rule-breaking but not from commenting. In contrast, hiding (rather than deleting) content had small and statistically insignificant effects. Our results suggest that automated content moderation increases adherence to community guidelines.},
booktitle = {Proceedings of the ACM Web Conference 2023},
pages = {2666–2676},
numpages = {11},
keywords = {community guidelines, content moderation, online platforms},
location = {Austin, TX, USA},
series = {WWW '23}
}

@misc{concordia,
      title={Generative agent-based modeling with actions grounded in physical, social, or digital space using Concordia}, 
      author={Alexander Sasha Vezhnevets and John P. Agapiou and Avia Aharon and Ron Ziv and Jayd Matyas and Edgar A. Duéñez-Guzmán and William A. Cunningham and Simon Osindero and Danny Karmon and Joel Z. Leibo},
      year={2023},
      eprint={2312.03664},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2312.03664}, 
}

@online{langchain,
	author = {LangChain Contributors},
	title = {LangChain: Building applications with LLMs through composability},
	howpublished = {\url{https://github.com/langchain-ai/langchain}},
	year = {2023},
	note = {GitHub repository},
	url = {https://github.com/langchain-ai/langchain},
	urldate = {2025-02-13},
}

@online{tinytroupe,
    author={TinyTroupe contributors},
    title={TinyTroupe},
    year={2024},
    note = {GitHub repository},
    url={https://github.com/microsoft/TinyTroupe},
    urldate={2025-02-13}
}

@article{Madaan2023SelfRefineIR,
  title={Self-Refine: Iterative Refinement with Self-Feedback},
  author={Aman Madaan and Niket Tandon and Prakhar Gupta and Skyler Hallinan and Luyu Gao and Sarah Wiegreffe and Uri Alon and Nouha Dziri and Shrimai Prabhumoye and Yiming Yang and Sean Welleck and Bodhisattwa Prasad Majumder and Shashank Gupta and Amir Yazdanbakhsh and Peter Clark},
  journal={ArXiv},
  year={2023},
  volume={abs/2303.17651},
  url={https://api.semanticscholar.org/CorpusID:257900871}
}

@book{dimitra-book,
    author = {Kimbra White and Nicole Hunter and Keith Greaves} ,
    title = {facilitating deliberation - a practical guide},
    publisher = {Mosaic Lab},
    year = {2024}
}

@unpublished{dimitra-guide,
    author = {{MIT Center for Constructive Communication}},
    title = {Unpublished training materials developed by the MIT Center for Constructive Communication},
    note = {Guide given to human facilitators},
    year={2024}
}

@article{burton2024large,
  author    = {Burton, J. W. and Lopez-Lopez, E. and Hechtlinger, S. and others},
  title     = {How Large Language Models Can Reshape Collective Intelligence},
  journal   = {Nature Human Behaviour},
  year      = {2024},
  volume    = {8},
  pages     = {1643--1655},
}


@inproceedings{schroeder-etal-2024-fora,
    title = "Fora: A corpus and framework for the study of facilitated dialogue",
    author = "Schroeder, H. and
      Roy, D.  and
      Kabbara, J.",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics",
    year = "2024",
    address = "Bangkok, Thailand",
    pages = "13985--14001",
}

@inproceedings{cho-etal-2024-language,
    title = "Can Language Model Moderators Improve the Health of Online Discourse?",
    author = "Cho, H.  and
      Liu, S.  and
      Shi, T.  and
      Jain, D.  and
      Rizk, B.  and
      Huang, Y.  and
      Lu, Z.  and
      Wen, N.  and
      Gratch, J.  and
      Ferrara, E.  and
      May, J.",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    pages = "7478--7496",
}
@article{Kumar_AbuHashem_Durumeric_2024, title={Watch Your Language: Investigating Content Moderation with Large Language Models}, volume={18}, number={1}, journal={Proceedings of the International AAAI Conference on Web and Social Media}, author={Kumar, D. and AbuHashem, Y. A. and Durumeric, Z.}, year={2024}, pages={865-878} }

@inproceedings{bose-etal-2023-detoxifying,
    title = "Detoxifying Online Discourse: A Guided Response Generation Approach for Reducing Toxicity in User-Generated Text",
    author = "Bose, R.  and
      Perera, I.  and
      Dorr, B.",
    booktitle = "Proceedings of the First Workshop on Social Influence in Conversations",
    year = "2023",
    address = "Toronto, Canada",
    pages = "9--14",
}

@article{kim_et_al_chatbot,
author = {Kim, S. and Eun, J. and Seering, J. and Lee, J.},
title = {Moderator Chatbot for Deliberative Discussion: Effects of Discussion Structure and Discussant Facilitation},
year = {2021},
issue_date = {April 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {CSCW1},
url = {https://doi.org/10.1145/3449161},
doi = {10.1145/3449161},
abstract = {Online chat functions as a discussion channel for diverse social issues. However, deliberative discussion and consensus-reaching can be difficult in online chats in part because of the lack of structure. To explore the feasibility of a conversational agent that enables deliberative discussion, we designed and developed DebateBot, a chatbot that structures discussion and encourages reticent participants to contribute. We conducted a 2 (discussion structure: unstructured vs. structured) \texttimes{} 2 (discussant facilitation: unfacilitated vs. facilitated) between-subjects experiment (N = 64, 12 groups). Our findings are as follows: (1) Structured discussion positively affects discussion quality by generating diverse opinions within a group and resulting in a high level of perceived deliberative quality. (2) Facilitation drives a high level of opinion alignment between group consensus and independent individual opinions, resulting in authentic consensus reaching. Facilitation also drives more even contribution and a higher level of task cohesion and communication fairness. Our results suggest that a chatbot agent could partially substitute for a human moderator in deliberative discussions.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = apr,
articleno = {87},
numpages = {26},
keywords = {chatbot, consensus reaching, conversational agent, deliberative discussion}
}

@article{cheung-et-al-2011,
author = {Lim, S.C.R. and Cheung, W. and Hew, K.},
year = {2011},
month = {05},
pages = {52-65},
title = {Critical Thinking in Asynchronous Online Discussion: An Investigation of Student Facilitation Techniques},
volume = {59},
journal = {New Horizons in Education}
}

@inproceedings{park_et_al_2012_facilitation,
author = {Park, J. and Klingel, S. and Cardie, C. and Newhart, M. and Farina, C. and Vallb\'{e}, J.J.},
title = {Facilitative moderation for online participation in eRulemaking},
year = {2012},
isbn = {9781450314039},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2307729.2307757},
doi = {10.1145/2307729.2307757},
booktitle = {Proceedings of the 13th Annual International Conference on Digital Government Research},
pages = {173–182},
numpages = {10},
location = {College Park, Maryland, USA},
}

@unpublished{korre2025evaluation,
  title={Evaluation and Facilitation of Online Discussions in the LLM Era: A Survey},
  author={Korre, Katerina and Tsirmpas, Dimitris and Gkoumas, Nikos and Cabalé, Emma and Kontarinis, Dionysis and Myrtzani, Danai and Evgeniou, Theodoros and Androutsopoulos, Ion and Pavlopoulos, John},
  year={2025},
  month={02},
  note={ACL ARR 2025 February Submission},
  url={https://openreview.net/forum?id=T9c0zA8GTj}
}

@unpublished{hewitt2024predicting,
  title={Predicting Results of Social Science Experiments Using Large Language Models},
  author={Hewitt, Luke and Ashokkumar, Ashwini and Ghezae, Isaias and Willer, Robb},
  year={2024},
  month={08},
  note={Equal contribution, order randomized},
  institution={Stanford University, New York University}
}

@article{Gligoric2024CanUL,
  title={Can Unconfident LLM Annotations Be Used for Confident Conclusions?},
  author={Kristina Gligori'c and Tijana Zrnic and Cinoo Lee and Emmanuel J. Candes and Dan Jurafsky},
  journal={ArXiv},
  year={2024},
  volume={abs/2408.15204},
  url={https://api.semanticscholar.org/CorpusID:271962879}
}

@misc{openai2024gpt4technicalreport,
      title={GPT-4 Technical Report}, 
      author={OpenAI and Josh Achiam and Steven Adler and Sandhini Agarwal and Lama Ahmad and Ilge Akkaya and Florencia Leoni Aleman and Diogo Almeida and Janko Altenschmidt and Sam Altman and Shyamal Anadkat and Red Avila and Igor Babuschkin and Suchir Balaji and Valerie Balcom and Paul Baltescu and Haiming Bao and Mohammad Bavarian and Jeff Belgum and Irwan Bello and Jake Berdine and Gabriel Bernadett-Shapiro and Christopher Berner and Lenny Bogdonoff and Oleg Boiko and Madelaine Boyd and Anna-Luisa Brakman and Greg Brockman and Tim Brooks and Miles Brundage and Kevin Button and Trevor Cai and Rosie Campbell and Andrew Cann and Brittany Carey and Chelsea Carlson and Rory Carmichael and Brooke Chan and Che Chang and Fotis Chantzis and Derek Chen and Sully Chen and Ruby Chen and Jason Chen and Mark Chen and Ben Chess and Chester Cho and Casey Chu and Hyung Won Chung and Dave Cummings and Jeremiah Currier and Yunxing Dai and Cory Decareaux and Thomas Degry and Noah Deutsch and Damien Deville and Arka Dhar and David Dohan and Steve Dowling and Sheila Dunning and Adrien Ecoffet and Atty Eleti and Tyna Eloundou and David Farhi and Liam Fedus and Niko Felix and Simón Posada Fishman and Juston Forte and Isabella Fulford and Leo Gao and Elie Georges and Christian Gibson and Vik Goel and Tarun Gogineni and Gabriel Goh and Rapha Gontijo-Lopes and Jonathan Gordon and Morgan Grafstein and Scott Gray and Ryan Greene and Joshua Gross and Shixiang Shane Gu and Yufei Guo and Chris Hallacy and Jesse Han and Jeff Harris and Yuchen He and Mike Heaton and Johannes Heidecke and Chris Hesse and Alan Hickey and Wade Hickey and Peter Hoeschele and Brandon Houghton and Kenny Hsu and Shengli Hu and Xin Hu and Joost Huizinga and Shantanu Jain and Shawn Jain and Joanne Jang and Angela Jiang and Roger Jiang and Haozhun Jin and Denny Jin and Shino Jomoto and Billie Jonn and Heewoo Jun and Tomer Kaftan and Łukasz Kaiser and Ali Kamali and Ingmar Kanitscheider and Nitish Shirish Keskar and Tabarak Khan and Logan Kilpatrick and Jong Wook Kim and Christina Kim and Yongjik Kim and Jan Hendrik Kirchner and Jamie Kiros and Matt Knight and Daniel Kokotajlo and Łukasz Kondraciuk and Andrew Kondrich and Aris Konstantinidis and Kyle Kosic and Gretchen Krueger and Vishal Kuo and Michael Lampe and Ikai Lan and Teddy Lee and Jan Leike and Jade Leung and Daniel Levy and Chak Ming Li and Rachel Lim and Molly Lin and Stephanie Lin and Mateusz Litwin and Theresa Lopez and Ryan Lowe and Patricia Lue and Anna Makanju and Kim Malfacini and Sam Manning and Todor Markov and Yaniv Markovski and Bianca Martin and Katie Mayer and Andrew Mayne and Bob McGrew and Scott Mayer McKinney and Christine McLeavey and Paul McMillan and Jake McNeil and David Medina and Aalok Mehta and Jacob Menick and Luke Metz and Andrey Mishchenko and Pamela Mishkin and Vinnie Monaco and Evan Morikawa and Daniel Mossing and Tong Mu and Mira Murati and Oleg Murk and David Mély and Ashvin Nair and Reiichiro Nakano and Rajeev Nayak and Arvind Neelakantan and Richard Ngo and Hyeonwoo Noh and Long Ouyang and Cullen O'Keefe and Jakub Pachocki and Alex Paino and Joe Palermo and Ashley Pantuliano and Giambattista Parascandolo and Joel Parish and Emy Parparita and Alex Passos and Mikhail Pavlov and Andrew Peng and Adam Perelman and Filipe de Avila Belbute Peres and Michael Petrov and Henrique Ponde de Oliveira Pinto and Michael and Pokorny and Michelle Pokrass and Vitchyr H. Pong and Tolly Powell and Alethea Power and Boris Power and Elizabeth Proehl and Raul Puri and Alec Radford and Jack Rae and Aditya Ramesh and Cameron Raymond and Francis Real and Kendra Rimbach and Carl Ross and Bob Rotsted and Henri Roussez and Nick Ryder and Mario Saltarelli and Ted Sanders and Shibani Santurkar and Girish Sastry and Heather Schmidt and David Schnurr and John Schulman and Daniel Selsam and Kyla Sheppard and Toki Sherbakov and Jessica Shieh and Sarah Shoker and Pranav Shyam and Szymon Sidor and Eric Sigler and Maddie Simens and Jordan Sitkin and Katarina Slama and Ian Sohl and Benjamin Sokolowsky and Yang Song and Natalie Staudacher and Felipe Petroski Such and Natalie Summers and Ilya Sutskever and Jie Tang and Nikolas Tezak and Madeleine B. Thompson and Phil Tillet and Amin Tootoonchian and Elizabeth Tseng and Preston Tuggle and Nick Turley and Jerry Tworek and Juan Felipe Cerón Uribe and Andrea Vallone and Arun Vijayvergiya and Chelsea Voss and Carroll Wainwright and Justin Jay Wang and Alvin Wang and Ben Wang and Jonathan Ward and Jason Wei and CJ Weinmann and Akila Welihinda and Peter Welinder and Jiayi Weng and Lilian Weng and Matt Wiethoff and Dave Willner and Clemens Winter and Samuel Wolrich and Hannah Wong and Lauren Workman and Sherwin Wu and Jeff Wu and Michael Wu and Kai Xiao and Tao Xu and Sarah Yoo and Kevin Yu and Qiming Yuan and Wojciech Zaremba and Rowan Zellers and Chong Zhang and Marvin Zhang and Shengjia Zhao and Tianhao Zheng and Juntang Zhuang and William Zhuk and Barret Zoph},
      year={2024},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2303.08774}, 
}

@article{argyle2023,
author = {Lisa P Argyle and Christopher A Bail and Ethan C Busby and Joshua R Gubler and Thomas Howe and Christopher Rytting and Taylor Sorensen and David Wingate},
title = {Leveraging {AI} for democratic discourse: Chat interventions can improve online political conversations at scale},
journal = {Proceedings of the National Academy of Sciences},
volume = {120},
number = {41},
pages = {1-8},
year = {2023},
}

@article{Kruskal01121952,
author = {William H. Kruskal and W. Allen Wallis},
title = {Use of Ranks in One-Criterion Variance Analysis},
journal = {Journal of the American Statistical Association},
volume = {47},
number = {260},
pages = {583--621},
year = {1952},
publisher = {ASA Website},
doi = {10.1080/01621459.1952.10483441},
URL = { https://www.tandfonline.com/doi/abs/10.1080/01621459.1952.10483441},
eprint = { https://www.tandfonline.com/doi/pdf/10.1080/01621459.1952.10483441}
}

@article{dunn,
 ISSN = {00401706},
 URL = {http://www.jstor.org/stable/1266041},
 abstract = {This paper considers the use of rank sums from a combined ranking of k independent samples in order to decide which populations differ. Such a procedure is suggested as a convenient alternative to making separate rankings for each pair of samples, and the two methods are compared. Asymptotic use of the normal tables is given and the treatment of ties is discussed. A numerical example is given.},
 author = {Olive Jean Dunn},
 journal = {Technometrics},
 number = {3},
 pages = {241--252},
 publisher = {[Taylor & Francis, Ltd., American Statistical Association, American Society for Quality]},
 title = {Multiple Comparisons Using Rank Sums},
 urldate = {2025-03-03},
 volume = {6},
 year = {1964}
}

@inproceedings{koh-etal-2024-llms,
    title = "Can {LLM}s Recognize Toxicity? A Structured Investigation Framework and Toxicity Metric",
    author = "Koh, Hyukhun  and
      Kim, Dohyung  and
      Lee, Minwoo  and
      Jung, Kyomin",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-emnlp.353/",
    doi = "10.18653/v1/2024.findings-emnlp.353",
    pages = "6092--6114",
    abstract = "In the pursuit of developing Large Language Models (LLMs) that adhere to societal standards, it is imperative to detect the toxicity in the generated text. The majority of existing toxicity metrics rely on encoder models trained on specific toxicity datasets, which are susceptible to out-of-distribution (OOD) problems and depend on the dataset`s definition of toxicity. In this paper, we introduce a robust metric grounded on LLMs to flexibly measure toxicity according to the given definition. We first analyze the toxicity factors, followed by an examination of the intrinsic toxic attributes of LLMs to ascertain their suitability as evaluators. Finally, we evaluate the performance of our metric with detailed analysis. Our empirical results demonstrate outstanding performance in measuring toxicity within verified factors, improving on conventional metrics by 12 points in the F1 score. Our findings also indicate that upstream toxicity significantly influences downstream metrics, suggesting that LLMs are unsuitable for toxicity evaluations within unverified factors."
}

@inproceedings{cresci_pesonalized_interventions,
author = {Cresci, Stefano and Trujillo, Amaury and Fagni, Tiziano},
title = {Personalized Interventions for Online Moderation},
year = {2022},
isbn = {9781450392334},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511095.3536369},
doi = {10.1145/3511095.3536369},
abstract = {Current online moderation follows a one-size-fits-all approach, where each intervention is applied in the same way to all users. This na\"{\i}ve approach is challenged by established socio-behavioral theories and by recent empirical results that showed the limited effectiveness of such interventions. We propose a paradigm-shift in online moderation by moving towards a personalized and user-centered approach. Our multidisciplinary vision combines state-of-the-art theories and practices in diverse fields such as computer science, sociology and psychology, to design personalized moderation interventions (PMIs). In outlining the path leading to the next-generation of moderation interventions, we also discuss the most prominent challenges introduced by such a disruptive change.},
booktitle = {Proceedings of the 33rd ACM Conference on Hypertext and Social Media},
pages = {248–251},
numpages = {4},
keywords = {moderation interventions, online moderation, personalization, social media, user modeling},
location = {Barcelona, Spain},
series = {HT '22}
}

@inproceedings{ziegenbein-etal-2023-modeling,
    title = "Modeling Appropriate Language in Argumentation",
    author = "Ziegenbein, Timon  and
      Syed, Shahbaz  and
      Lange, Felix  and
      Potthast, Martin  and
      Wachsmuth, Henning",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.238/",
    doi = "10.18653/v1/2023.acl-long.238",
    pages = "4344--4363",
    abstract = "Online discussion moderators must make ad-hoc decisions about whether the contributions of discussion participants are appropriate or should be removed to maintain civility. Existing research on offensive language and the resulting tools cover only one aspect among many involved in such decisions. The question of what is considered appropriate in a controversial discussion has not yet been systematically addressed. In this paper, we operationalize appropriate language in argumentation for the first time. In particular, we model appropriateness through the absence of flaws, grounded in research on argument quality assessment, especially in aspects from rhetoric. From these, we derive a new taxonomy of 14 dimensions that determine inappropriate language in online discussions. Building on three argument quality corpora, we then create a corpus of 2191 arguments annotated for the 14 dimensions. Empirical analyses support that the taxonomy covers the concept of appropriateness comprehensively, showing several plausible correlations with argument quality dimensions. Moreover, results of baseline approaches to assessing appropriateness suggest that all dimensions can be modeled computationally on the corpus."
}

@inproceedings{falk-etal-2024-moderation,
    title = "Moderation in the Wild: Investigating User-Driven Moderation in Online Discussions",
    author = "Falk, Neele  and
      Vecchi, Eva  and
      Jundi, Iman  and
      Lapesa, Gabriella",
    editor = "Graham, Yvette  and
      Purver, Matthew",
    booktitle = "Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = mar,
    year = "2024",
    address = "St. Julian{'}s, Malta",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.eacl-long.60/",
    pages = "992--1013",
    abstract = "Effective content moderation is imperative for fostering healthy and productive discussions in online domains. Despite the substantial efforts of moderators, the overwhelming nature of discussion flow can limit their effectiveness. However, it is not only trained moderators who intervene in online discussions to improve their quality. {\textquotedblleft}Ordinary{\textquotedblright} users also act as moderators, actively intervening to correct information of other users' posts, enhance arguments, and steer discussions back on course.This paper introduces the phenomenon of user moderation, documenting and releasing UMOD, the first dataset of comments in whichusers act as moderators. UMOD contains 1000 comment-reply pairs from the subreddit r/changemyview with crowdsourced annotations from a large annotator pool and with a fine-grained annotation schema targeting the functions of moderation, stylistic properties(aggressiveness, subjectivity, sentiment), constructiveness, as well as the individual perspectives of the annotators on the task. The releaseof UMOD is complemented by two analyses which focus on the constitutive features of constructiveness in user moderation and on thesources of annotator disagreements, given the high subjectivity of the task."
}

@inproceedings{Steck_2024, series={WWW ’24},
   title={Is Cosine-Similarity of Embeddings Really About Similarity?},
   url={http://dx.doi.org/10.1145/3589335.3651526},
   DOI={10.1145/3589335.3651526},
   booktitle={Companion Proceedings of the ACM Web Conference 2024},
   publisher={ACM},
   author={Steck, Harald and Ekanadham, Chaitanya and Kallus, Nathan},
   year={2024},
   month=may, pages={887–890},
   collection={WWW ’24} }


@inproceedings{arditi_abliteration,
 author = {Arditi, Andy and Obeso, Oscar and Syed, Aaquib and Paleka, Daniel and Panickssery, Nina and Gurnee, Wes and Nanda, Neel},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {136037--136083},
 publisher = {Curran Associates, Inc.},
 title = {Refusal in Language Models Is Mediated by a Single Direction},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/f545448535dfde4f9786555403ab7c49-Paper-Conference.pdf},
 volume = {37},
 year = {2024}
}

@article{rossi_2024, title={The Problems of LLM-generated Data in Social Science Research}, volume={18}, url={https://sociologica.unibo.it/article/view/19576}, DOI={10.6092/issn.1971-8853/19576}, abstractNote={&amp;lt;p style=&amp;quot;font-weight: 400;&amp;quot;&amp;gt;Beyond being used as fast and cheap annotators for otherwise complex classification tasks, LLMs have seen a growing adoption for generating synthetic data for social science and design research. Researchers have used LLM-generated data for data augmentation and prototyping, as well as for direct analysis where LLMs acted as proxies for real human subjects. LLM-based synthetic data build on fundamentally different epistemological assumptions than previous synthetically generated data and are justified by a different set of considerations. In this essay, we explore the various ways in which LLMs have been used to generate research data and consider the underlying epistemological (and accompanying methodological) assumptions. We challenge some of the assumptions made about LLM-generated data, and we highlight the main challenges that social sciences and humanities need to address if they want to adopt LLMs as synthetic data generators.&amp;lt;/p&amp;gt;}, number={2}, journal={Sociologica}, author={Rossi, Luca and Harrison, Katherine and Shklovski, Irina}, year={2024}, month={Jan.}, pages={145–168} }

@article{grossman_2023,
author = {Grossmann, Igor and Feinberg, Matthew and Parker, Dawn and Christakis, Nicholas and Tetlock, Philip and Cunningham, William},
year = {2023},
month = {06},
pages = {1108-1109},
title = {AI and the transformation of social science research},
volume = {380},
journal = {Science (New York, N.Y.)},
doi = {10.1126/science.adi1778}
}

@article{jansen_2023,
title = {Employing large language models in survey research},
journal = {Natural Language Processing Journal},
volume = {4},
pages = {100020},
year = {2023},
issn = {2949-7191},
doi = {https://doi.org/10.1016/j.nlp.2023.100020},
url = {https://www.sciencedirect.com/science/article/pii/S2949719123000171},
author = {Bernard J. Jansen and Soon-gyo Jung and Joni Salminen},
keywords = {Survey research, Large language models, Survey data, Surveys, LLM survey respondents},
abstract = {This article discusses the promising potential of employing large language models (LLMs) for survey research, including generating responses to survey items. LLMs can address some of the challenges associated with survey research regarding question-wording and response bias. They can address issues relating to a lack of clarity and understanding but cannot yet correct for sampling or nonresponse bias challenges. While LLMs can assist with some of the challenges with survey research, at present, LLMs need to be used in conjunction with other methods and approaches. With thoughtful and nuanced approaches to development, LLMs can be used responsibly and beneficially while minimizing the associated risks.}
}

@article{bisbee_2023, title={Synthetic Replacements for Human Survey Data? The Perils of Large Language Models}, volume={32}, DOI={10.1017/pan.2024.5}, number={4}, journal={Political Analysis}, author={Bisbee, James and Clinton, Joshua D. and Dorff, Cassy and Kenkel, Brenton and Larson, Jennifer M.}, year={2024}, pages={401–416}} <div></div>

@misc{shapiro2019polling,
  author = {Walter Shapiro},
  title = {{The Polling Industry Is in Crisis}},
  howpublished = {From \textit{The New Republic}, accessed April 24 2025},
    year={2019},
  url = {https://newrepublic.com/article/154124/polling-industry-crisis},
}


@article{tornberg_2024,
author = {Petter Törnberg},
title ={Large Language Models Outperform Expert Coders and Supervised Classifiers at Annotating Political Social Media Messages},
journal = {Social Science Computer Review},
volume = {0},
number = {0},
pages = {08944393241286471},
year = {2024},
doi = {10.1177/08944393241286471},
URL = {https://doi.org/10.1177/08944393241286471},
eprint = {https://doi.org/10.1177/08944393241286471},
abstract = { Instruction-tuned Large Language Models (LLMs) have recently emerged as a powerful new tool for text analysis. As these models are capable of zero-shot annotation based on instructions written in natural language, they obviate the need of large sets of training data—and thus bring potential paradigm-shifting implications for using text as data. While the models show substantial promise, their relative performance compared to human coders and supervised models remains poorly understood and subject to significant academic debate. This paper assesses the strengths and weaknesses of popular fine-tuned AI models compared to both conventional supervised classifiers and manual annotation by experts and crowd workers. The task used is to identify the political affiliation of politicians based on a single X/Twitter message, focusing on data from 11 different countries. The paper finds that GPT-4 achieves higher accuracy than both supervised models and human coders across all languages and country contexts. In the US context, it achieves an accuracy of 0.934 and an inter-coder reliability of 0.982. Examining the cases where the models fail, the paper finds that the LLM—unlike the supervised models—correctly annotates messages that require interpretation of implicit or unspoken references, or reasoning on the basis of contextual knowledge—capacities that have traditionally been understood to be distinctly human. The paper thus contributes to our understanding of the revolutionary implications of LLMs for text analysis within the social sciences. }
}

@misc{anthis_2025,
      title={LLM Social Simulations Are a Promising Research Method}, 
      author={Jacy Reese Anthis and Ryan Liu and Sean M. Richardson and Austin C. Kozlowski and Bernard Koch and James Evans and Erik Brynjolfsson and Michael Bernstein},
      year={2025},
      eprint={2504.02234},
      archivePrefix={arXiv},
      primaryClass={cs.HC},
      url={https://arxiv.org/abs/2504.02234}, 
}

@misc{y_social,
      title={Y Social: an LLM-powered Social Media Digital Twin}, 
      author={Giulio Rossetti and Massimo Stella and Rémy Cazabet and Katherine Abramski and Erica Cau and Salvatore Citraro and Andrea Failla and Riccardo Improta and Virginia Morini and Valentina Pansanella},
      year={2024},
      eprint={2408.00818},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2408.00818}, 
}

@misc{mou_2024,
      title={Unveiling the Truth and Facilitating Change: Towards Agent-based Large-scale Social Movement Simulation}, 
      author={Xinyi Mou and Zhongyu Wei and Xuanjing Huang},
      year={2024},
      eprint={2402.16333},
      archivePrefix={arXiv},
      primaryClass={cs.CY},
      url={https://arxiv.org/abs/2402.16333}, 
}

@misc{tornberg_2023,
      title={Simulating Social Media Using Large Language Models to Evaluate Alternative News Feed Algorithms}, 
      author={Petter Törnberg and Diliara Valeeva and Justus Uitermark and Christopher Bail},
      year={2023},
      eprint={2310.05984},
      archivePrefix={arXiv},
      primaryClass={cs.SI},
      url={https://arxiv.org/abs/2310.05984}, 
}

@misc{demarzo_2023,
      title={Emergence of Scale-Free Networks in Social Interactions among Large Language Models}, 
      author={Giordano De Marzo and Luciano Pietronero and David Garcia},
      year={2023},
      eprint={2312.06619},
      archivePrefix={arXiv},
      primaryClass={physics.soc-ph},
      url={https://arxiv.org/abs/2312.06619}, 
}

@article{abramski_2023,
AUTHOR = {Abramski, Katherine and Citraro, Salvatore and Lombardi, Luigi and Rossetti, Giulio and Stella, Massimo},
TITLE = {Cognitive Network Science Reveals Bias in GPT-3, GPT-3.5 Turbo, and GPT-4 Mirroring Math Anxiety in High-School Students},
JOURNAL = {Big Data and Cognitive Computing},
VOLUME = {7},
YEAR = {2023},
NUMBER = {3},
ARTICLE-NUMBER = {124},
URL = {https://www.mdpi.com/2504-2289/7/3/124},
ISSN = {2504-2289},
ABSTRACT = {Large Language Models (LLMs) are becoming increasingly integrated into our lives. Hence, it is important to understand the biases present in their outputs in order to avoid perpetuating harmful stereotypes, which originate in our own flawed ways of thinking. This challenge requires developing new benchmarks and methods for quantifying affective and semantic bias, keeping in mind that LLMs act as psycho-social mirrors that reflect the views and tendencies that are prevalent in society. One such tendency that has harmful negative effects is the global phenomenon of anxiety toward math and STEM subjects. In this study, we introduce a novel application of network science and cognitive psychology to understand biases towards math and STEM fields in LLMs from ChatGPT, such as GPT-3, GPT-3.5, and GPT-4. Specifically, we use behavioral forma mentis networks (BFMNs) to understand how these LLMs frame math and STEM disciplines in relation to other concepts. We use data obtained by probing the three LLMs in a language generation task that has previously been applied to humans. Our findings indicate that LLMs have negative perceptions of math and STEM fields, associating math with negative concepts in 6 cases out of 10. We observe significant differences across OpenAI’s models: newer versions (i.e., GPT-4) produce 5× semantically richer, more emotionally polarized perceptions with fewer negative associations compared to older versions and N=159 high-school students. These findings suggest that advances in the architecture of LLMs may lead to increasingly less biased models that could even perhaps someday aid in reducing harmful stereotypes in society rather than perpetuating them.},
DOI = {10.3390/bdcc7030124}
}

@misc{leng_2024,
      title={Do LLM Agents Exhibit Social Behavior?}, 
      author={Yan Leng and Yuan Yuan},
      year={2024},
      eprint={2312.15198},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2312.15198}, 
}

@misc{neumann_2025,
      title={Should you use LLMs to simulate opinions? Quality checks for early-stage deliberation}, 
      author={Terrence Neumann and Maria De-Arteaga and Sina Fazelpour},
      year={2025},
      eprint={2504.08954},
      archivePrefix={arXiv},
      primaryClass={cs.CY},
      url={https://arxiv.org/abs/2504.08954}, 
}

@misc{balog_2024,
      title={Towards Realistic Synthetic User-Generated Content: A Scaffolding Approach to Generating Online Discussions}, 
      author={Krisztian Balog and John Palowitch and Barbara Ikica and Filip Radlinski and Hamidreza Alvari and Mehdi Manshadi},
      year={2024},
      eprint={2408.08379},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2408.08379}, 
}

@misc{atil_2025,
      title={Non-Determinism of "Deterministic" LLM Settings}, 
      author={Berk Atil and Sarp Aykent and Alexa Chittams and Lisheng Fu and Rebecca J. Passonneau and Evan Radcliffe and Guru Rajan Rajagopal and Adam Sloan and Tomasz Tudrej and Ferhan Ture and Zhe Wu and Lixinyu Xu and Breck Baldwin},
      year={2025},
      eprint={2408.04667},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2408.04667}, 
}

@book{rosenberg2015nonviolent,
  title={Nonviolent communication: A language of life: Life-changing tools for healthy relationships},
  author={Rosenberg, Marshall B and Chopra, Deepak},
  year={2015},
  publisher={PuddleDancer Press}
}

@inproceedings{
zhou_2024_sotopia,
title={{SOTOPIA}: Interactive Evaluation for Social Intelligence in Language Agents},
author={Xuhui Zhou and Hao Zhu and Leena Mathur and Ruohong Zhang and Haofei Yu and Zhengyang Qi and Louis-Philippe Morency and Yonatan Bisk and Daniel Fried and Graham Neubig and Maarten Sap},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=mM7VurbA4r}
}

@inproceedings{zhou-etal-2024-real,
    title = "Is this the real life? Is this just fantasy? The Misleading Success of Simulating Social Interactions With {LLM}s",
    author = "Zhou, Xuhui  and
      Su, Zhe  and
      Eisape, Tiwalayo  and
      Kim, Hyunwoo  and
      Sap, Maarten",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.1208/",
    doi = "10.18653/v1/2024.emnlp-main.1208",
    pages = "21692--21714",
    abstract = "Recent advances in large language models (LLM) have enabled richer social simulations, allowing for the study of various social phenomena. However, most recent work has used a more omniscient perspective on these simulations (e.g., single LLM to generate all interlocutors), which is fundamentally at odds with the non-omniscient, information asymmetric interactions that involve humans and AI agents in the real world. To examine these differences, we develop an evaluation framework to simulate social interactions with LLMs in various settings (omniscient, non-omniscient). Our experiments show that LLMs perform better in unrealistic, omniscient simulation settings but struggle in ones that more accurately reflect real-world conditions with information asymmetry. Moreover, we illustrate the limitations inherent in learning from omniscient simulations. Our findings indicate that addressing information asymmetry remains a fundamental challenge for LLM-based agents."
}

@misc{retraction_watch,
  title        = "Experiment using AI-generated posts on Reddit draws fire for ethics concerns",
  author       = "Retraction-Watch",
  howpublished = "\url{https://retractionwatch.com/2025/04/28/experiment-using-ai-generated-posts-on-reddit-draws-fire-for-ethics-concerns/}",
  year         = 2025,
  note         = "Accessed: 2025-04-29"
}

@misc{eu_ai_act_2024,
  title        = {Regulation (EU) 2024/1689 of the European Parliament and of the Council of 13 June 2024 laying down harmonised rules on artificial intelligence and amending certain Union legislative acts (Artificial Intelligence Act)},
  author       = {{European Parliament and Council}},
  howpublished = {\url{https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX:32024R1689}},
  year         = {2024},
  month        = jun,
  note         = {OJ L 2024/1689, 12.7.2024}
}


@inproceedings{potter-etal-2024-hidden,
    title = "Hidden Persuaders: {LLM}s' Political Leaning and Their Influence on Voters",
    author = "Potter, Yujin  and
      Lai, Shiyang  and
      Kim, Junsol  and
      Evans, James  and
      Song, Dawn",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.244/",
    doi = "10.18653/v1/2024.emnlp-main.244",
    pages = "4244--4275",
    abstract = "Do LLMs have political leanings and are LLMs able to shift our political views? This paper explores these questions in the context of the 2024 U.S. presidential election. Through a voting simulation, we demonstrate 18 open-weight and closed-source LLMs' political preference for Biden over Trump. We show how Biden-leaning becomes more pronounced in instruction-tuned and reinforced models compared to their base versions by analyzing their responses to political questions related to the two nominees. We further explore the potential impact of LLMs on voter choice by recruiting 935 U.S. registered voters. Participants interacted with LLMs (Claude-3, Llama-3, and GPT-4) over five exchanges. Intriguingly, although LLMs were not asked to persuade users to support Biden, about 20{\%} of Trump supporters reduced their support for Trump after LLM interaction. This result is noteworthy given that many studies on the persuasiveness of political campaigns have shown minimal effects in presidential elections. Many users also expressed a desire for further interaction with LLMs on political subjects. Further research on how LLMs affect users' political views is required, as their use becomes more widespread."
}

@article{political_2024,
    doi = {10.1371/journal.pone.0306621},
    author = {Rozado, David},
    journal = {PLOS ONE},
    publisher = {Public Library of Science},
    title = {The political preferences of LLMs},
    year = {2024},
    month = {07},
    volume = {19},
    url = {https://doi.org/10.1371/journal.pone.0306621},
    pages = {1-15},
    abstract = {I report here a comprehensive analysis about the political preferences embedded in Large Language Models (LLMs). Namely, I administer 11 political orientation tests, designed to identify the political preferences of the test taker, to 24 state-of-the-art conversational LLMs, both closed and open source. When probed with questions/statements with political connotations, most conversational LLMs tend to generate responses that are diagnosed by most political test instruments as manifesting preferences for left-of-center viewpoints. This does not appear to be the case for five additional base (i.e. foundation) models upon which LLMs optimized for conversation with humans are built. However, the weak performance of the base models at coherently answering the tests’ questions makes this subset of results inconclusive. Finally, I demonstrate that LLMs can be steered towards specific locations in the political spectrum through Supervised Fine-Tuning (SFT) with only modest amounts of politically aligned data, suggesting SFT’s potential to embed political orientation in LLMs. With LLMs beginning to partially displace traditional information sources like search engines and Wikipedia, the societal implications of political biases embedded in LLMs are substantial.},
    number = {7},
}

@misc{pit2024oninvestigatingpoliticalstance,
      title={Whose Side Are You On? Investigating the Political Stance of Large Language Models}, 
      author={Pagnarasmey Pit and Xingjun Ma and Mike Conway and Qingyu Chen and James Bailey and Henry Pit and Putrasmey Keo and Watey Diep and Yu-Gang Jiang},
      year={2024},
      eprint={2403.13840},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2403.13840}, 
}